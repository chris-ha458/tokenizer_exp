{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coded by Chris Ha (hac541309@gmail.com) of EleutherAI, DuckAI\n",
    "# for polyglot(EleutherAI) and polylingual(DuckAI) projects\n",
    "# Licensed as MIT or Apache 2.0 or later versions of these licenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equal : averages nlp\n",
    "# min : takes the lowest (unlikeliest) of two options\n",
    "# max : takes the highest (likeliest) of two options\n",
    "nlp_mode = \"max\"\n",
    "# this epsilon value is later used to separate byte level tokens \n",
    "# They will have the lowest negative log probs, the most unlikeliest.\n",
    "# This will put them on the bottom, and other characters or tokens will be tokenized above them.\n",
    "epsilon = 1E-7\n",
    "\n",
    "# sort by negative logprob, first, then length then vocab\n",
    "sort_key = lambda vocab_logprob_pair: [-vocab_logprob_pair[1],vocab_logprob_pair[0]]\n",
    "\n",
    "# if not None : trim to vocab_size.\n",
    "target_vocab_size = 256000\n",
    "# the number we set the internal tokens to achieve the final intended vocab size\n",
    "target_intermediate_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section initializes byte vocabulary and unicode exemplars\n",
    "# Korean is handled differently from the other exemplars due to quantity\n",
    "\n",
    "# Since the ByteLevel works as its name suggests, at the byte level, it encodes each byte value to a unique visible character. \n",
    "# This means that there is a total of 256 different characters composing this alphabet.\n",
    "byte_level = tokenizers.pre_tokenizers.ByteLevel(add_prefix_space = False)\n",
    "byte_level.add_prefix_space = False\n",
    "byte_level_alphabet = sorted(tokenizers.pre_tokenizers.ByteLevel.alphabet())\n",
    "assert len(byte_level_alphabet) == 256\n",
    "whitespaces = [\"  \",\"    \",\"        \",\"                \"]\n",
    "\n",
    "with open(\"/home/karyo/corpus/data/unicode/non_korean_exemplars.json\") as f:\n",
    "    non_korean_exemplars = json.load(f)\n",
    "    non_korean_exemplars += whitespaces\n",
    "    non_korean_exemplar_byte = []\n",
    "    for exemplar in non_korean_exemplars:\n",
    "        non_korean_exemplar_byte.append(byte_level.pre_tokenize_str(exemplar)[0][0])\n",
    "\n",
    "with open(\"/home/karyo/corpus/data/unicode/ksx-1001.txt\") as f:\n",
    "    korean_exemplars = f.read().split()\n",
    "    korean_exemplar_byte = []\n",
    "    for exemplar in korean_exemplars:\n",
    "        korean_exemplar_byte.append(byte_level.pre_tokenize_str(exemplar)[0][0])\n",
    "\n",
    "essential_byte_tokens = set(byte_level_alphabet + non_korean_exemplar_byte + korean_exemplar_byte)\n",
    "essential_tokens = set(non_korean_exemplars + korean_exemplars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tokenizer_path = \"/home/karyo/corpus/models/hplt/rc/base_v2.json\"\n",
    "assert os.path.isfile(base_tokenizer_path)\n",
    "new_tokenizer_path = \"/home/karyo/corpus/models/hplt/rc/v2_exemplars.json\"\n",
    "base_tokenizer = tokenizers.Tokenizer.from_file(base_tokenizer_path)\n",
    "essential_plus = list(essential_byte_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size = 270177, Unadded size = 270177\n",
      "256000\n"
     ]
    }
   ],
   "source": [
    "base_total_size = base_tokenizer.get_vocab_size(True)\n",
    "base_unadded_size = base_tokenizer.get_vocab_size(False)\n",
    "assert base_total_size == len(base_tokenizer.get_vocab(True))\n",
    "assert base_unadded_size == len(base_tokenizer.get_vocab(False))\n",
    "# special tokens might not considered added\n",
    "print(f\"Total Size = {base_total_size}, Unadded size = {base_unadded_size}\")\n",
    "added_offset = base_total_size - base_unadded_size\n",
    "if target_vocab_size:\n",
    "    target_intermediate_size = target_vocab_size - added_offset\n",
    "    print(target_intermediate_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_tokenizer_path) as base_file:\n",
    "    base_tokenizer_json = json.load(base_file)\n",
    "    assert base_tokenizer_json['model']['type'].lower() == \"unigram\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tokenizer_dict = dict(base_tokenizer_json['model']['vocab'])\n",
    "sorted_base = sorted(base_tokenizer_dict.items(), key=sort_key,reverse=True)\n",
    "min_log_prob = min(sorted_base, key = lambda pair: pair[1])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276493"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# artificially add all essential tokens\n",
    "for token in essential_plus:\n",
    "    if token not in base_tokenizer_dict:\n",
    "        sorted_base.append((token,min_log_prob))\n",
    "sorted_base.sort(key=sort_key)\n",
    "len(sorted_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove until size matches target\n",
    "# do not remove essential tokens\n",
    "for pair in sorted_base:\n",
    "    if len(sorted_base) <= target_vocab_size:\n",
    "        break\n",
    "    if pair[0] in essential_plus or pair[0].startswith(\"<|\"):\n",
    "        continue\n",
    "    else:\n",
    "        sorted_base.remove(pair)\n",
    "\n",
    "# sort vocab\n",
    "sorted_base.sort(key=sort_key)\n",
    "len(sorted_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the foundational tokens to have smallest log probs\n",
    "new_tokenizer_list = []\n",
    "for pair in sorted_base:\n",
    "    vocab = pair[0]\n",
    "    if vocab in essential_byte_tokens:\n",
    "        nlp = min_log_prob - epsilon * (len(vocab) - 1)\n",
    "    else:\n",
    "        nlp = pair[1]\n",
    "    new_tokenizer_list.append((vocab,nlp))\n",
    "len(new_tokenizer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer_dict = dict(\n",
    "    sorted(new_tokenizer_list, key=lambda pair: (-pair[1], pair[0]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tokenizer_json['model']['vocab'] = list(new_tokenizer_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write new tokenizer\n",
    "with open(new_tokenizer_path, \"w\") as new_file:\n",
    "    json.dump(base_tokenizer_json, new_file,indent=2,ensure_ascii=False)\n",
    "\n",
    "# check if new tokenizer can be loaded\n",
    "new_tokenizer = tokenizers.Tokenizer.from_file(new_tokenizer_path)\n",
    "if target_vocab_size:\n",
    "    assert target_vocab_size == new_tokenizer.get_vocab_size(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
