{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coded by Chris Ha (hac541309@gmail.com) of EleutherAI, DuckAI\n",
    "# for polyglot(EleutherAI) and polylingual(DuckAI) projects\n",
    "# Licensed as MIT or Apache 2.0 or later versions of these licenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstitute into dicts\n",
    "# this might be just done with dict(tokenizer_json['model']['vocab'])\n",
    "def recover_vocab_dict(tokenizer_json: dict) -> dict:\n",
    "    tokenizer_dict = {}\n",
    "    tokenizer_list = tokenizer_json['model']['vocab']\n",
    "    for nlp_pair in tokenizer_list:\n",
    "        tokenizer_dict[nlp_pair[0]] = nlp_pair[1]\n",
    "    return tokenizer_dict\n",
    "\n",
    "def check_duplicates(lst):\n",
    "    count_dict = {}\n",
    "    for item in lst:\n",
    "        if item[0] in count_dict:\n",
    "            count_dict[item[0]] += 1\n",
    "        else:\n",
    "            count_dict[item[0]] = 1\n",
    "    duplicates = {key: value for key, value in count_dict.items() if value > 1}\n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equal : averages nlp\n",
    "# min : takes the lowest (unlikeliest) of two options\n",
    "# max : takes the highest (likeliest) of two options\n",
    "nlp_mode = \"max\"\n",
    "# this epsilon value is later used to separate byte level tokens \n",
    "# They will have the lowest negative log probs, the most unlikeliest.\n",
    "# This will put them on the bottom, and other characters or tokens will be tokenized above them.\n",
    "\n",
    "epsilon = 1E-7\n",
    "# whether to bring in added_tokens of additional vocabs\n",
    "with_added_tokens = False\n",
    "\n",
    "# if not None : trim to vocab_size.\n",
    "target_vocab_size = None\n",
    "# the number we set the internal tokens to achieve the final intended vocab size\n",
    "target_intermediate_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the ByteLevel works as its name suggests, at the byte level, it encodes each byte value to a unique visible character. \n",
    "# This means that there is a total of 256 different characters composing this alphabet.\n",
    "byte_level = tokenizers.pre_tokenizers.ByteLevel(add_prefix_space = False)\n",
    "byte_level.add_prefix_space = False\n",
    "byte_level_alphabet = sorted(tokenizers.pre_tokenizers.ByteLevel.alphabet())\n",
    "assert len(byte_level_alphabet) == 256\n",
    "\n",
    "with open(\"/home/karyo/corpus/data/unicode/non_korean_exemplars.json\") as f:\n",
    "    non_korean_exemplars = json.load(f)\n",
    "    non_korean_exemplar_byte = []\n",
    "    for exemplar in non_korean_exemplars:\n",
    "        non_korean_exemplar_byte.append(byte_level.pre_tokenize_str(exemplar)[0][0])\n",
    "\n",
    "with open(\"/home/karyo/corpus/data/unicode/ksx-1001.txt\") as f:\n",
    "    korean_exemplars = f.read().split()\n",
    "    korean_exemplar_byte = []\n",
    "    for exemplar in korean_exemplars:\n",
    "        korean_exemplar_byte.append(byte_level.pre_tokenize_str(exemplar)[0][0])\n",
    "\n",
    "essential_byte_tokens = set(byte_level_alphabet + non_korean_exemplar_byte + korean_exemplar_byte)\n",
    "essential_tokens = set(non_korean_exemplars + korean_exemplars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# everything except the vocab is inherited from base\n",
    "# special tokens, add tokens, preprocessing etc\n",
    "base_tokenizer_path = \"/home/karyo/corpus/models/hplt/merging/intermediate_c8.json\"\n",
    "add_tokenizer_path = \"/home/karyo/corpus/models/hplt/mt_2k.json\"\n",
    "assert os.path.isfile(base_tokenizer_path)\n",
    "assert os.path.isfile(add_tokenizer_path)\n",
    "new_tokenizer_path = \"/home/karyo/corpus/models/hplt/merging/final.json\"\n",
    "base_tokenizer = tokenizers.Tokenizer.from_file(base_tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size = 269203, Unadded size = 269203\n",
      "Internal Total Size = 269203, Unadded size = 269203\n"
     ]
    }
   ],
   "source": [
    "base_total_size = base_tokenizer.get_vocab_size(True)\n",
    "base_unadded_size = base_tokenizer.get_vocab_size(False)\n",
    "print(f\"Total Size = {base_total_size}, Unadded size = {base_unadded_size}\")\n",
    "\n",
    "#internal representation might be different due to <bos> <eos>\n",
    "base_internal_total = len(base_tokenizer.get_vocab(True))\n",
    "base_internal_unadded = len(base_tokenizer.get_vocab(False))\n",
    "print(f\"Internal Total Size = {base_internal_total}, Unadded size = {base_internal_unadded}\")\n",
    "assert (base_total_size - base_unadded_size) == (base_internal_total - base_internal_unadded)\n",
    "internal_offset = base_total_size - base_internal_total\n",
    "added_offset = base_total_size - base_unadded_size\n",
    "\n",
    "# we are setting the intermediate_size according to offsets\n",
    "# the difference arises from how huggingface tokenizers treat added tokens and special tokens\n",
    "if target_vocab_size:\n",
    "    target_intermediate_size = target_vocab_size - (internal_offset + added_offset)\n",
    "    print(target_intermediate_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_tokenizer_path) as base_file,open(add_tokenizer_path) as add_file:\n",
    "    base_tokenizer_json = json.load(base_file)\n",
    "    add_tokenizer_json = json.load(add_file)\n",
    "    # this only works for unigram\n",
    "    assert base_tokenizer_json['model']['type'].lower() == \"unigram\"\n",
    "    assert add_tokenizer_json['model']['type'].lower() == \"unigram\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269203 2000\n"
     ]
    }
   ],
   "source": [
    "# base_tokenizer_dict = recover_vocab_dict(base_tokenizer_json)\n",
    "# add_tokenizer_dict = recover_vocab_dict(add_tokenizer_json)\n",
    "base_tokenizer_dict = dict(base_tokenizer_json['model']['vocab'])\n",
    "add_tokenizer_dict = dict(add_tokenizer_json['model']['vocab'])\n",
    "print(len(base_tokenizer_dict), len(add_tokenizer_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through new vocab and add them\n",
    "# we take the max because averaging actually reduces the negative logprob compared to other tokens\n",
    "# these tokens are those that are shared! there is no reason for them to be penalized\n",
    "# we take the maximum of each and bump them up a small(epsilon amount) \n",
    "# but we don't want them to exceed 0\n",
    "for token in add_tokenizer_dict:\n",
    "    if token in base_tokenizer_dict:\n",
    "        base_tokenizer_dict[token] = min(\n",
    "            0, max(base_tokenizer_dict[token], add_tokenizer_dict[token]) + epsilon\n",
    "        )\n",
    "    else:\n",
    "        base_tokenizer_dict[token] = add_tokenizer_dict[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270177\n"
     ]
    }
   ],
   "source": [
    "# sort by negative logprob, first, then length then vocab\n",
    "sort_key = lambda x: [-x[1],x[0]]\n",
    "# huggingface tokenizers match added/special tokens directly with the ids\n",
    "# so this sorting process can cause a lot of pain\n",
    "# this can be helped by adding sorted(special_tokens) before using this tool.\n",
    "# another alternative is to manually realign the ids\n",
    "sorted_base = sorted(base_tokenizer_dict.items(), key=sort_key,reverse=True)\n",
    "print(len(sorted_base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_tokens = list(filter(lambda token:len(token[0])==1,sorted_base))\n",
    "# assert len(single_tokens) == 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstitute vocabs. only take {target_vocab_size} number of items\n",
    "# if target_vocab_size is None. the slice does nothing\n",
    "\n",
    "\n",
    "# remove until size matches target\n",
    "# do not remove essential tokens\n",
    "if target_vocab_size:\n",
    "    for pair in sorted_base:\n",
    "        if len(sorted_base) <= target_vocab_size: break\n",
    "        if pair[0] not in essential_byte_tokens:\n",
    "            sorted_base.remove(pair)\n",
    "    merged_vocab = sorted(sorted_base, key=sort_key)\n",
    "\n",
    "# sort vocab\n",
    "merged_vocab = list(map(list,sorted_base))[:target_intermediate_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270177 -18.986092964742106\n"
     ]
    }
   ],
   "source": [
    "# ensure byte tokens are the unlikeliest\n",
    "min_log_prob = min(merged_vocab,key=lambda item:item[1])[1] - epsilon\n",
    "print(len(merged_vocab), min_log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to forcibly remove bytelevel token items\n",
    "\n",
    "# merged_vocab = [item for item in merged_vocab if item[0] not in byte_level_alphabet]\n",
    "# for item in merged_vocab:\n",
    "#    if item[0] in byte_level_alphabet:\n",
    "#        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270177 0\n"
     ]
    }
   ],
   "source": [
    "# here we do 2 things\n",
    "# 1. push any existing byte_level_alphabets into a low negative log prob\n",
    "# 2. add any byte alphabets back into it, but not append but replace (to keep vocab_size)\n",
    "not_included_count = 0\n",
    "for byte in byte_level_alphabet:\n",
    "    included=False\n",
    "    # if byte\n",
    "    for item in merged_vocab:\n",
    "        if byte == item[0]:\n",
    "            item[1] = min_log_prob\n",
    "            included=True\n",
    "            break\n",
    "    # if byte token isnt included (was trimmed)\n",
    "    # add it inplace of an unlikely token that is not a byte token.\n",
    "    if not included:\n",
    "        not_included_count +=1\n",
    "        for item in reversed(merged_vocab):\n",
    "            if item[0] in byte_level_alphabet:\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"vocab item {item[0]}, replaced with {byte}\")\n",
    "                item[0] = byte\n",
    "                item[1] = min_log_prob\n",
    "                break\n",
    "merged_vocab = sorted(merged_vocab, key=sort_key)\n",
    "assert len(merged_vocab)\n",
    "print(len(merged_vocab), not_included_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269203 270177\n"
     ]
    }
   ],
   "source": [
    "print(len(base_tokenizer_json['model']['vocab']), len(merged_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tokenizer_json['model']['vocab'] = merged_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write new tokenizer\n",
    "with open(new_tokenizer_path, \"w\") as new_file:\n",
    "    json.dump(base_tokenizer_json, new_file,indent=2,ensure_ascii=False)\n",
    "\n",
    "# check if new tokenizer can be loaded\n",
    "new_tokenizer = tokenizers.Tokenizer.from_file(new_tokenizer_path)\n",
    "if target_vocab_size:\n",
    "    assert target_vocab_size == new_tokenizer.get_vocab_size(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4152\n",
      "1084\n"
     ]
    }
   ],
   "source": [
    "exemplar_count = 0\n",
    "vocab = new_tokenizer.get_vocab()\n",
    "for exemplar in non_korean_exemplar_byte:\n",
    "    if exemplar in vocab:\n",
    "        exemplar_count += 1\n",
    "\n",
    "korean_exemplar_count = 0\n",
    "for exemplar in korean_exemplar_byte:\n",
    "    if exemplar in vocab:\n",
    "        korean_exemplar_count += 1\n",
    "\n",
    "print(exemplar_count)\n",
    "print(korean_exemplar_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
