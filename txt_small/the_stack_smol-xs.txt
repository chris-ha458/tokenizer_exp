# -*- coding: utf-8 -*-

from setuptools import setup, find_packages


with open('README.md') as f:
    readme = f.read()

with open('LICENSE') as f:
    license = f.read()

setup(
    name='xcodeproject',
    version='0.9',
    description='Xcode project file inspection utilities',
    long_description=readme,
    author='Marc Liyanage',
    author_email='reg.python-xcodeproject@entropy.ch',
    url='https://github.com/liyanage/python-xcodeproject',
    license=license,
    packages=find_packages(exclude=('tests', 'docs')),
    entry_points = {
        "console_scripts": [
            "xcodeproject-util=xcodeproject.tool:XcodeprojectTool.main",
        ],
    }
)

from django.urls import path,include
from . import views
from django.contrib.auth import views as auth_views
# from rest_framework import routers
from django.conf import settings
from django.conf.urls.static import static

urlpatterns = [
    path('index/',views.index, name='index'),
    path('register', views.register, name='register'),
    path('',auth_views.LoginView.as_view(), name='login'),
    path('account/', include('django.contrib.auth.urls')),
    path('all-hoods/', views.hoods, name='hood'),
    path('new-hood/', views.create_hood, name='new-hood'),
    path('profile/<username>', views.profile, name='profile'),
    path('profile/<username>/edit/', views.edit_profile, name='edit-profile'),
    path('join_hood/<id>', views.join_hood, name='join-hood'),
    path('leave_hood/<id>', views.leave_hood, name='leave-hood'),
    path('single_hood/<hood_id>', views.single_hood, name='single-hood'),
    path('<hood_id>/new-post', views.create_post, name='post'),
    path('<hood_id>/members', views.hood_members, name='members'),
    path('search/', views.search_business, name='search'),
]
if settings.DEBUG:
    urlpatterns+= static(settings.MEDIA_URL, document_root = settings.MEDIA_ROOT)
#!/usr/bin/python3
# -----------------------------------------------------------
# Define project related exceptions
# -----------------------------------------------------------

class OverlappingColumns(Exception):
    """
    Exception for overlapping columns of two dataframes
    """
    def __init__(self, common_columns):
        message = f"Columns of dataframes to cross join overlap: {str(common_columns)}"
        super().__init__(message)


class InconsistentDates(Exception):
    """
    Exception for dates incoherency
    """
    def __init__(self, error_details):
        message = f"Dates are inconsistent: {str(error_details)}"
        super().__init__(message)


class EmptyTrainingSet(Exception):
    """
    Exception empty training set
    """
    def __init__(self, error_details):
        message = f"Training set is empty, \
                    please check your training dates regarding to your data files {str(error_details)}"
        super().__init__(message)


class MissingDataForPrediction(Exception):
    """
    Exception for missing data when trying to build up features for prediction
    """
    def __init__(self, error_details):
        msg = f"Prediction set is empty, \
                please check your prediction dates regarding to your data files {str(error_details)}"
        super().__init__(msg)

import time
import logging
import httplib as http_client
import requests
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.poolmanager import PoolManager
from requests.packages.urllib3.util.retry import Retry
import ssl
import base64
try:
    from cStringIO import StringIO
except BaseException:
    from StringIO import StringIO
import zipfile


class MyAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        self.poolmanager = PoolManager(num_pools=connections,
                                       maxsize=maxsize,
                                       block=block,
                                       ssl_version=getattr(ssl, 'PROTOCOL_TLSv1_2', ssl.PROTOCOL_TLSv1))

# Retry logic if the API fails to responde


def requests_retry_session(
        retries=5,
        backoff_factor=0.5,
        status_forcelist=(500, 502, 504, 495, 496, 525, 526),
        session=None,
):
    session = session or requests.Session()
    retry = Retry(
        total=retries,
        read=retries,
        connect=retries,
        backoff_factor=backoff_factor,
        status_forcelist=status_forcelist,
    )
    adapter = MyAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session

# 1and1 Object Classes


class OneAndOneService(object):

    # Init Function
    def __init__(
            self,
            api_token,
            api_url='https://cloudpanel-api.1and1.com/v1',
            enable_logs=False):
        if api_url == '' or api_url == 'default':
            api_url = 'https://cloudpanel-api.1and1.com/v1'
        self.api_token = api_token
        self.base_url = api_url
        self.header = {'X-TOKEN': self.api_token}
        self.success_codes = (200, 201, 202)
        if enable_logs:
            http_client.HTTPConnection.debuglevel = 1
            logging.basicConfig()
            logging.getLogger().setLevel(logging.DEBUG)
            requests_log = logging.getLogger("requests.packages.urllib3")
            requests_log.setLevel(logging.ERROR)
            requests_log.propagate = True

    def __repr__(self):
        return 'OneAndOneService: api_token=%s, base_url=%s' % (self.api_token,
                                                                self.base_url)

    # Server Functions

    # 'GET' methods

    def list_servers(self, page=None, per_page=None, sort=None, q=None,
                     fields=None):

        # Perform Request
        parameters = {
            'page': page,
            'per_page': per_page,
            'sort': sort,
            'q': q,
            'fields': fields
        }

        url = '%s/servers' % self.base_url

        try:
            r = requests_retry_session().get(url, headers=self.header, params=parameters)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def fixed_server_flavors(self):

        # Perform Request
        url = '%s/servers/fixed_instance_sizes' % self.base_url

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_fixed_server(self, fixed_server_id=None):

        # Error Handling
        if(fixed_server_id is None):
            raise ValueError('fixed_server_id is a required parameter')

        # Perform Request
        url = ('%s/servers/fixed_instance_sizes/%s' %
               (self.base_url, fixed_server_id))

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_server(self, server_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')

        # Perform Request
        url = '%s/servers/%s' % (self.base_url, server_id)
        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_server_hardware(self, server_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')

        # Perform Request
        url = '%s/servers/%s/hardware' % (self.base_url, server_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def list_server_hdds(self, server_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')

        # Perform Request
        url = '%s/servers/%s/hardware/hdds' % (self.base_url, server_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_server_hdd(self, server_id=None, hdd_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')
        if(hdd_id is None):
            raise ValueError('hdd_id is a required parameter')

        # Perform Request
        url = ('%s/servers/%s/hardware/hdds/%s' %
               (self.base_url, server_id, hdd_id))

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_server_image(self, server_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')

        # Perform Request
        url = '%s/servers/%s/image' % (self.base_url, server_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def list_server_ips(self, server_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')

        # Perform Request
        url = '%s/servers/%s/ips' % (self.base_url, server_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_server_ip(self, server_id=None, ip_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')
        if(ip_id is None):
            raise ValueError('ip_id is a required parameter')

        # Perform Request
        url = '%s/servers/%s/ips/%s' % (self.base_url, server_id, ip_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def list_ip_firewall_policy(self, server_id=None, ip_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')
        if(ip_id is None):
            raise ValueError('ip_id is a required parameter')

        # Perform Request
        url = ('%s/servers/%s/ips/%s/firewall_policy' %
               (self.base_url, server_id, ip_id))

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def list_ip_load_balancers(self, server_id=None, ip_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')
        if(ip_id is None):
            raise ValueError('ip_id is a required parameter')

        # Perform Request
        url = ('%s/servers/%s/ips/%s/load_balancers' %
               (self.base_url, server_id, ip_id))

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_server_status(self, server_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')

        # Perform Request
        url = '%s/servers/%s/status' % (self.base_url, server_id)
        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()

        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_server_dvd(self, server_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')

        # Perform Request
        url = '%s/servers/%s/dvd' % (self.base_url, server_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def list_server_private_networks(self, server_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')

        # Perform Request
        url = '%s/servers/%s/private_networks' % (self.base_url, server_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def private_network_info(self, server_id=None, private_network_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')
        if(private_network_id is None):
            raise ValueError('private_network_id is a required parameter')

        # Perform Request
        url = ('%s/servers/%s/private_networks/%s' %
               (self.base_url, server_id, private_network_id))

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def list_server_snapshots(self, server_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')

        # Perform Request
        url = '%s/servers/%s/snapshots' % (self.base_url, server_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def list_baremetal_models(
            self,
            page=None,
            per_page=None,
            sort=None,
            q=None,
            fields=None):

        # Perform Request
        parameters = {
            'page': page,
            'per_page': per_page,
            'sort': sort,
            'q': q,
            'fields': fields
        }

        url = '%s/servers/baremetal_models' % self.base_url

        r = requests.get(url, headers=self.header, params=parameters)

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def get_baremetal_model(self, model_id=None):

        # Error Handling
        if (model_id is None):
            raise ValueError('model_id is a required parameter')

        # Perform Request
        url = '%s/servers/baremetal_models/%s' % (self.base_url, model_id)

        r = requests.get(url, headers=self.header)

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    # 'PUT' methods

    def modify_server(self, server_id=None, name=None, description=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')
        if(name is None):
            raise ValueError('name is a required parameter')

        # Perform Request
        data = {
            'name': name,
            'description': description
        }

        url = '%s/servers/%s' % (self.base_url, server_id)

        try:
            r = requests_retry_session().put(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def modify_server_hardware(
            self,
            server_id=None,
            fixed_instance_size_id=None,
            vcore=None,
            cores_per_processor=None,
            ram=None,
            test=False):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')

        # Use 'test' flag to skip this block when running unit test
        if(test == False):

            # Prevent hot decreasing of server hardware, allow cold decreasing.
            server_specs = self.get_server_hardware(server_id=server_id)

            server_status = self.get_server_status(server_id=server_id)

            if(server_status['state'] == 'POWERED_ON'):
                if(vcore is not None):
                    if(server_specs['vcore'] > vcore):
                        raise ValueError(('Cannot perform a hot decrease of '
                                          'server CPU.  The new value must be '
                                          'greater than current value.'))
                if(ram is not None):
                    if(server_specs['ram'] > ram):
                        raise ValueError(('Cannot perform a hot decrease of '
                                          'server RAM.  The new value must be '
                                          'greater than current value.'))

        # Perform Request
        data = {
            'fixed_instance_size_id': fixed_instance_size_id,
            'vcore': vcore,
            'cores_per_processor': cores_per_processor,
            'ram': ram
        }

        url = '%s/servers/%s/hardware' % (self.base_url, server_id)

        try:
            r = requests_retry_session().put(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def modify_hdd(self, server_id=None, hdd_id=None, size=None, test=False):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')
        if(hdd_id is None):
            raise ValueError('hdd_id is a required parameter')

        # Use 'test' flag to skip this block when running unit test
        if(test == False):

            # Make sure size argument is valid.  HDD size can't be decreased.
            old_hdd = self.get_server_hdd(server_id=server_id, hdd_id=hdd_id)

            if(size is not None):
                if(old_hdd['size'] > size):
                    raise ValueError('HDD size can never be decreased. '
                                     'Must be greater than or equal to the '
                                     'current HDD size.')

        # Perform Request
        data = {'size': size}

        url = ('%s/servers/%s/hardware/hdds/%s' %
               (self.base_url, server_id, hdd_id))

        try:
            r = requests_retry_session().put(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def add_firewall_policy(
            self,
            server_id=None,
            ip_id=None,
            firewall_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')
        if(ip_id is None):
            raise ValueError('ip_id is a required parameter')
        if(firewall_id is None):
            raise ValueError('firewall_id is a required parameter')

        # Perform Request
        data = {'id': firewall_id}

        url = ('%s/servers/%s/ips/%s/firewall_policy' %
               (self.base_url, server_id, ip_id))

        try:
            r = requests_retry_session().put(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def modify_server_status(
            self,
            server_id=None,
            action=None,
            method='SOFTWARE',
            recovery_mode=False,
            recovery_image_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')
        if(action is None):
            raise ValueError('action is a required parameter')

        # Make sure user is passing in correct arguments
        if(action != 'POWER_ON' and action != 'POWER_OFF' and
                action != 'REBOOT'):
            raise ValueError(('action must be set to "POWER_ON",'
                              '"POWER_OFF", or "REBOOT".'))

        if method != 'HARDWARE' and method != 'SOFTWARE':
            raise ValueError(('method must be set to either '
                              '"HARDWARE" or "SOFTWARE".'))
        if recovery_mode and recovery_image_id is None:
            raise ValueError(
                ('If you want to reboot in recovery mode you must specify an image id recovery_image_id'))

        # Perform Request
        if recovery_mode:
            data = {
                'action': action,
                'method': method,
                'recovery_mode': True,
                'recovery_image_id': recovery_image_id
            }
        else:
            data = {
                'action': action,
                'method': method,
            }

        url = '%s/servers/%s/status/action' % (self.base_url, server_id)

        try:
            r = requests_retry_session().put(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def stop_server(self, server_id=None, method='SOFTWARE'):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')

        # Make sure user is passing in correct arguments
        if(method != 'HARDWARE' and method != 'SOFTWARE'):
            raise ValueError(('method must be set to either '
                              '"HARDWARE" or "SOFTWARE".'))

        # Perform Request
        data = {
            'action': 'POWER_OFF',
            'method': method
        }

        url = '%s/servers/%s/status/action' % (self.base_url, server_id)

        try:
            r = requests_retry_session().put(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def start_server(self, server_id=None, method='SOFTWARE'):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')

        # Make sure user is passing in correct arguments
        if(method != 'HARDWARE' and method != 'SOFTWARE'):
            raise ValueError(('method must be set to either '
                              '"HARDWARE" or "SOFTWARE".'))

        # Perform Request
        data = {
            'action': 'POWER_ON',
            'method': method
        }

        url = '%s/servers/%s/status/action' % (self.base_url, server_id)

        try:
            r = requests_retry_session().put(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def load_dvd(self, server_id=None, dvd_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')
        if(dvd_id is None):
            raise ValueError('dvd_id is a required parameter')

        # Perform Request
        data = {'id': dvd_id}

        url = '%s/servers/%s/dvd' % (self.base_url, server_id)

        try:
            r = requests_retry_session().put(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def restore_snapshot(self, server_id=None, snapshot_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')
        if(snapshot_id is None):
            raise ValueError('snapshot_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = ('%s/servers/%s/snapshots/%s' %
               (self.base_url, server_id, snapshot_id))

        try:
            r = requests_retry_session().put(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def reinstall_image(self, server_id=None, image_id=None, password=None,
                        firewall_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')
        if(image_id is None):
            raise ValueError('image_id is a required parameter')

        # Create firewall object, if necessary
        firewall_policy = {'id': firewall_id}

        # Perform Request
        data = {
            'id': image_id,
            'password': password,
            'firewall_policy': firewall_policy
        }

        url = '%s/servers/%s/image' % (self.base_url, server_id)

        try:
            r = requests_retry_session().put(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'DELETE' methods

    def delete_server(self, server_id=None, keep_ips=None, keep_hdds=True):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'
        parameters = {'keep_ips': keep_ips, 'keep_hdds': keep_hdds}

        url = '%s/servers/%s' % (self.base_url, server_id)

        try:
            r = requests_retry_session().delete(
                url, headers=self.header, params=parameters)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def remove_hdd(self, server_id=None, hdd_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')
        if(hdd_id is None):
            raise ValueError('hdd_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = ('%s/servers/%s/hardware/hdds/%s' %
               (self.base_url, server_id, hdd_id))

        try:
            r = requests_retry_session().delete(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def remove_ip(self, server_id=None, ip_id=None, keep_ip=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')
        if(ip_id is None):
            raise ValueError('ip_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'
        parameters = {'keep_ip': keep_ip}

        url = '%s/servers/%s/ips/%s' % (self.base_url, server_id, ip_id)

        try:
            r = requests_retry_session().delete(
                url, headers=self.header, params=parameters)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def remove_load_balancer(self, server_id=None, ip_id=None,
                             load_balancer_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')
        if(ip_id is None):
            raise ValueError('ip_id is a required parameter')
        if(load_balancer_id is None):
            raise ValueError('load_balancer_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = ('%s/servers/%s/ips/%s/load_balancers/%s' %
               (self.base_url, server_id, ip_id, load_balancer_id))

        try:
            r = requests_retry_session().delete(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def remove_private_network(self, server_id=None, private_network_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')
        if(private_network_id is None):
            raise ValueError('private_network_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = ('%s/servers/%s/private_networks/%s' %
               (self.base_url, server_id, private_network_id))

        try:
            r = requests_retry_session().delete(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def eject_dvd(self, server_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = '%s/servers/%s/dvd' % (self.base_url, server_id)

        try:
            r = requests_retry_session().delete(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def delete_snapshot(self, server_id=None, snapshot_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')
        if(snapshot_id is None):
            raise ValueError('snapshot_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = ('%s/servers/%s/snapshots/%s' %
               (self.base_url, server_id, snapshot_id))

        try:
            r = requests_retry_session().delete(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'POST' methods

    def add_new_ip(self, server_id=None, ip_type=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')
        if(ip_type is not None) and (ip_type != 'IPV4'):
            raise ValueError(("ip_type.  Only type 'IPV4' is currently "
                              "supported."))

        # Perform Request
        data = {'type': ip_type}

        url = '%s/servers/%s/ips' % (self.base_url, server_id)

        try:
            r = requests_retry_session().post(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def add_load_balancer(self, server_id=None, ip_id=None,
                          load_balancer_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')
        if(ip_id is None):
            raise ValueError('ip_id is a required parameter')
        if(load_balancer_id is None):
            raise ValueError('load_balancer_id is a required parameter')

        # Perform Request
        data = {'load_balancer_id': load_balancer_id}

        url = ('%s/servers/%s/ips/%s/load_balancers' %
               (self.base_url, server_id, ip_id))
        try:
            r = requests_retry_session().post(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def assign_private_network(self, server_id=None, private_network_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')
        if(private_network_id is None):
            raise ValueError('private_network_id is a required parameter')

        # Perform Request
        data = {'id': private_network_id}

        url = '%s/servers/%s/private_networks' % (self.base_url, server_id)

        try:
            r = requests_retry_session().post(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def create_snapshot(self, server_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = '%s/servers/%s/snapshots' % (self.base_url, server_id)

        try:
            r = requests_retry_session().post(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def clone_server(self, server_id=None, name=None, datacenter_id=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')
        if(name is None):
            raise ValueError('name is a required parameter')

        # Perform Request
        data = {
            'name': name,
            'datacenter_id': datacenter_id
        }

        url = '%s/servers/%s/clone' % (self.base_url, server_id)

        try:
            r = requests_retry_session().post(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def create_server(self, server=None, hdds=None):

        # Error Handling
        if(server is None):
            raise ValueError(('server is a required parameter. Make '
                              'sure you pass a Server object.'))

        # Unpack hdds
        if hdds:
            hdd = []

            for value in hdds:
                hdd.append(value.specs)

            # Add hdds to server object
            server.specs['hardware']['hdds'] = hdd

        # Clean dictionary
        keys = [k for k, v in server.specs['hardware'].items() if
                v is None]
        for x in keys:
            del server.specs['hardware'][x]

        # Build URL and perform request
        url = '%s/servers' % self.base_url

        try:
            r = requests_retry_session().post(url, headers=self.header, json=server.specs)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            # Assign new server_id back to calling Server object
            response = r.json()

            server.specs.update(server_id=response['id'])
            server.specs.update(api_token=self.header)
            server.first_password = response['first_password']

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def add_hdd(self, server_id=None, hdds=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')
        if(hdds is None):
            raise ValueError(('hdds is a required parameter.  Make '
                              'sure you pass a list with at least '
                              'one Hdd object.'))

        # Unpack hdds
        hdd = []

        for value in hdds:
            hdd.append(value.specs)

        # Perform Request
        data = {'hdds': hdd}

        url = '%s/servers/%s/hardware/hdds' % (self.base_url, server_id)

        try:
            r = requests_retry_session().post(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # Image Functions

    # 'GET' Methods

    def list_images(self, page=None, per_page=None, sort=None, q=None,
                    fields=None):

        # Perform Request
        parameters = {
            'page': page,
            'per_page': per_page,
            'sort': sort,
            'q': q,
            'fields': fields
        }

        url = '%s/images' % self.base_url

        try:
            r = requests_retry_session().get(url, headers=self.header, params=parameters)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_image(self, image_id=None):

        # Error Handling
        if(image_id is None):
            raise ValueError('image_id is a required parameter')

        # Perform Request

        url = '%s/images/%s' % (self.base_url, image_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'POST' Methods

    def create_image(self, image=None):

        # Error Handling
        if(image.server_id is None):
            raise ValueError('server_id is a required parameter')
        if(image.name is None):
            raise ValueError('name is a required parameter')
        if(image.frequency is None):
            raise ValueError('frequency is a required parameter')
        if(image.num_images is None):
            raise ValueError('num_images is a required parameter')

        # Perform Request
        data = {
            'server_id': image.server_id,
            'name': image.name,
            'frequency': image.frequency,
            'num_images': image.num_images,
            'description': image.description,
            'source': image.source,
            'url': image.url,
            'os_id': image.os_id,
            'type': image.type
        }

        url = '%s/images' % self.base_url

        try:
            r = requests_retry_session().post(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            # Assign new image_id back to calling Image object
            response = r.json()

            image.specs.update(image_id=response['id'])
            image.specs.update(api_token=self.header)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'DELETE' Methods

    def delete_image(self, image_id=None):

        # Error Handling
        if(image_id is None):
            raise ValueError('image_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = '%s/images/%s' % (self.base_url, image_id)

        try:
            r = requests_retry_session().delete(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'PUT' Methods

    def modify_image(self, image_id=None, name=None, description=None,
                     frequency=None):

        # Error Handling
        if(image_id is None):
            raise ValueError('image_id is a required parameter')

        # Perform Request
        data = {
            'name': name,
            'frequency': frequency,
            'description': description
        }

        url = '%s/images/%s' % (self.base_url, image_id)

        try:
            r = requests_retry_session().put(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # Shared Storage Functions

    # 'GET' Methods

    def list_shared_storages(self, page=None, per_page=None, sort=None,
                             q=None, fields=None):

        # Perform Request
        parameters = {
            'page': page,
            'per_page': per_page,
            'sort': sort,
            'q': q,
            'fields': fields
        }

        url = '%s/shared_storages' % self.base_url

        try:
            r = requests_retry_session().get(url, headers=self.header, params=parameters)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_shared_storage(self, shared_storage_id=None):

        # Error Handling
        if(shared_storage_id is None):
            raise ValueError('shared_storage_id is a required parameter')

        # Perform Request
        url = '%s/shared_storages/%s' % (self.base_url, shared_storage_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def list_servers_attached_storage(self, shared_storage_id=None):

        # Error Handling
        if(shared_storage_id is None):
            raise ValueError('shared_storage_id is a required parameter')

        # Perform Request
        url = ('%s/shared_storages/%s/servers' %
               (self.base_url, shared_storage_id))

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_shared_storage_server(
            self,
            shared_storage_id=None,
            server_id=None):

        # Error Handling
        if(shared_storage_id is None):
            raise ValueError('shared_storage_id parameter is required')
        if(server_id is None):
            raise ValueError('server_id parameter is required')

        # Perform Request
        url = ('%s/shared_storages/%s/servers/%s' %
               (self.base_url, shared_storage_id, server_id))

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_credentials(self):

        # Perform Request
        url = '%s/shared_storages/access' % self.base_url

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'POST' Methods

    def create_shared_storage(self, shared_storage=None):

        # Error Handling
        if(shared_storage.name is None):
            raise ValueError('name is a required parameter')
        if(shared_storage.size is None):
            raise ValueError('size is a required parameter')

        # Perform Request
        data = {
            'name': shared_storage.name,
            'description': shared_storage.description,
            'size': shared_storage.size,
            'datacenter_id': shared_storage.datacenter_id
        }

        url = '%s/shared_storages' % self.base_url

        try:
            r = requests_retry_session().post(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            # Assign new shared_storage_id back to calling SharedStorage object
            response = r.json()

            shared_storage.specs.update(shared_storage_id=response['id'])
            shared_storage.specs.update(api_token=self.header)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def attach_server_shared_storage(self, shared_storage_id=None,
                                     server_ids=None):

        # Error Handling
        if(shared_storage_id is None):
            raise ValueError('shared_storage_id is a required parameter')
        if(server_ids is None):
            raise ValueError(('server_ids is a required parameter.  '
                              'Must attach at least one server'))

        # Unpack servers
        servers = []

        for value in server_ids:
            servers.append({'id': value.server_id, 'rights': value.rights})

        # Perform Request
        data = {'servers': servers}

        url = ('%s/shared_storages/%s/servers' %
               (self.base_url, shared_storage_id))

        try:
            r = requests_retry_session().post(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'PUT' Methods

    def modify_shared_storage(self, shared_storage_id=None, name=None,
                              description=None, size=None):

        # Error Handling
        if(shared_storage_id is None):
            raise ValueError('shared_storage_id is a required parameter')

        # Perform Request
        data = {
            'name': name,
            'description': description,
            'size': size
        }

        url = '%s/shared_storages/%s' % (self.base_url, shared_storage_id)

        try:
            r = requests_retry_session().put(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def change_password(self, password=None):

        # Error Handlong
        if(password is None):
            raise ValueError(('password is a required parameter. '
                              'password must contain at least 8 characters.'))

        # Perform Request
        data = {'password': password}

        url = '%s/shared_storages/access' % self.base_url

        try:
            r = requests_retry_session().put(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'DELETE' Methods

    def delete_shared_storage(self, shared_storage_id=None):

        # Error Handling
        if(shared_storage_id is None):
            raise ValueError('shared_storage_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = '%s/shared_storages/%s' % (self.base_url, shared_storage_id)

        try:
            r = requests_retry_session().delete(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def detach_server_shared_storage(self, shared_storage_id=None,
                                     server_id=None):

        # Error Handling
        if(shared_storage_id is None):
            raise ValueError('shared_storage_id is a required parameter')
        if(server_id is None):
            raise ValueError('server_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = ('%s/shared_storages/%s/servers/%s' %
               (self.base_url, shared_storage_id, server_id))

        try:
            r = requests_retry_session().delete(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # Firewall Policy Functions

    # 'GET' Methods

    def list_firewall_policies(self, page=None, per_page=None, sort=None,
                               q=None, fields=None):

        # Perform Request
        parameters = {
            'page': page,
            'per_page': per_page,
            'sort': sort,
            'q': q,
            'fields': fields
        }

        url = '%s/firewall_policies' % self.base_url
        try:
            r = requests_retry_session().get(url, headers=self.header, params=parameters)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_firewall(self, firewall_id=None):

        # Error Handling
        if(firewall_id is None):
            raise ValueError('firewall_id is a required parameter')

        # Perform Request
        url = '%s/firewall_policies/%s' % (self.base_url, firewall_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def list_firewall_servers(self, firewall_id=None):

        # Error Handling
        if(firewall_id is None):
            raise ValueError('firewall_id is a required parameter')

        # Perform Request
        url = ('%s/firewall_policies/%s/server_ips' %
               (self.base_url, firewall_id))

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_firewall_server(self, firewall_id=None, server_ip_id=None):

        # Error Handling
        if(firewall_id is None):
            raise ValueError('firewall_id is a required parameter')
        if(server_ip_id is None):
            raise ValueError('server_ip_id is a required parameter')

        # Perform Request
        url = ('%s/firewall_policies/%s/server_ips/%s' %
               (self.base_url, firewall_id, server_ip_id))

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def list_firewall_policy_rules(self, firewall_id=None):

        # Error Handling
        if(firewall_id is None):
            raise ValueError('firewall_id is a required parameter')

        # Perform Request
        url = '%s/firewall_policies/%s/rules' % (self.base_url, firewall_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_firewall_policy_rule(self, firewall_id=None, rule_id=None):

        # Error Handling
        if(firewall_id is None):
            raise ValueError('firewall_id is a required parameter')
        if(rule_id is None):
            raise ValueError('rule_id is a required parameter')

        # Perform Request
        url = ('%s/firewall_policies/%s/rules/%s' %
               (self.base_url, firewall_id, rule_id))

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'PUT' Methods

    def modify_firewall(self, firewall_id=None, name=None, description=None):

        # Error Handling
        if(firewall_id is None):
            raise ValueError('firewall_id is a required parameter')

        # Perform Request
        data = {
            'name': name,
            'description': description
        }

        url = '%s/firewall_policies/%s' % (self.base_url, firewall_id)

        try:
            r = requests_retry_session().put(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'POST' Methods

    def create_firewall_policy(self, firewall_policy=None,
                               firewall_policy_rules=None):

        # Error Handling
        if(firewall_policy.specs['name'] is None):
            raise ValueError(('Policy name is required.  Make sure your '
                              'FirewallPolicy object was initialized with '
                              'a name parameter'))
        if(firewall_policy_rules is None):
            raise ValueError(('firewall_policy_rules is required.  Make sure '
                              'you pass a list with at least one '
                              'FirewallPolicyRule object.'))

        # Unpack Rules
        rules = []

        for value in firewall_policy_rules:
            rules.append(value.rule_set)

        # Attach rules and Perform Request
        firewall_policy.specs['rules'] = rules

        url = '%s/firewall_policies' % self.base_url

        try:
            r = requests_retry_session().post(
                url, headers=self.header, json=firewall_policy.specs)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            # Assign new firewall_id back to calling FirewallPolicy object
            response = r.json()

            firewall_policy.specs.update(firewall_id=response['id'])
            firewall_policy.specs.update(api_token=self.header)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def add_firewall_policy_rule(self, firewall_id=None,
                                 firewall_policy_rules=None):

        # Error Handling
        if(firewall_id is None):
            raise ValueError('firewall_id is a required parameter')
        if(firewall_policy_rules is None):
            raise ValueError(('firewall_policy_rules is required.  Make '
                              'sure you pass a list with at least one '
                              'FirewallPolicyRule object'))

        # Unpack rules
        rules = []

        for value in firewall_policy_rules:
            rules.append(value.rule_set)

        # Perform Request
        data = {'rules': rules}

        url = '%s/firewall_policies/%s/rules' % (self.base_url, firewall_id)

        try:
            r = requests_retry_session().post(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def attach_server_firewall_policy(self, firewall_id=None, server_ips=None):

        # Error Handling
        if(firewall_id is None):
            raise ValueError('firewall_id is a required parameter')
        if(server_ips is None):
            raise ValueError(('server_ips is required. Make sure you pass '
                              'a list with at least one AttachServer object'))

        # Unpack servers
        servers = []

        for value in server_ips:
            servers.append(value.server_ip_id)

        # Perform Request
        data = {'server_ips': servers}

        url = ('%s/firewall_policies/%s/server_ips' %
               (self.base_url, firewall_id))

        try:
            r = requests_retry_session().post(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'DELETE' Methods

    def delete_firewall(self, firewall_id=None):

        # Error Handling
        if(firewall_id is None):
            raise ValueError('firewall_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = '%s/firewall_policies/%s' % (self.base_url, firewall_id)

        try:
            r = requests_retry_session().delete(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def remove_firewall_rule(self, firewall_id=None, rule_id=None):

        # Error Handling
        if(firewall_id is None):
            raise ValueError('firewall_id is a required parameter')
        if(rule_id is None):
            raise ValueError('rule_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = ('%s/firewall_policies/%s/rules/%s' %
               (self.base_url, firewall_id, rule_id))

        try:
            r = requests_retry_session().delete(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # Load Balancer Functions

    # 'GET' Methods

    def list_load_balancers(self, page=None, per_page=None, sort=None, q=None,
                            fields=None):

        # Perform Request
        parameters = {
            'page': page,
            'per_page': per_page,
            'sort': sort,
            'q': q,
            'fields': fields
        }

        url = '%s/load_balancers' % self.base_url

        try:
            r = requests_retry_session().get(url, headers=self.header, params=parameters)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_load_balancer(self, load_balancer_id=None):

        # Error Handling
        if(load_balancer_id is None):
            raise ValueError('load_balancer_id is a required parameter')

        # Perform Request
        url = '%s/load_balancers/%s' % (self.base_url, load_balancer_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def list_load_balancer_servers(self, load_balancer_id=None):

        # Error Handling
        if(load_balancer_id is None):
            raise ValueError('load_balancer_id is a required parameter')

        # Perform Request
        url = ('%s/load_balancers/%s/server_ips' %
               (self.base_url, load_balancer_id))

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_load_balancer_server(self, load_balancer_id=None,
                                 server_ip_id=None):

        # Error Handling
        if(load_balancer_id is None):
            raise ValueError('load_balancer_id is a required parameter')
        if(server_ip_id is None):
            raise ValueError('server_ip_id is a required parameter')

        # Perform Request
        url = ('%s/load_balancers/%s/server_ips/%s' %
               (self.base_url, load_balancer_id, server_ip_id))

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def load_balancer_rules(self, load_balancer_id=None):

        # Error Handling
        if(load_balancer_id is None):
            raise ValueError('load_balancer_id is a required parameter')

        # Perform Request
        url = '%s/load_balancers/%s/rules' % (self.base_url, load_balancer_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_load_balancer_rule(self, load_balancer_id=None, rule_id=None):

        # Error Handling
        if(load_balancer_id is None):
            raise ValueError('load_balancer_id is a required parameter')
        if(rule_id is None):
            raise ValueError('rule_id is a required parameter')

        # Perform Request
        url = ('%s/load_balancers/%s/rules/%s' %
               (self.base_url, load_balancer_id, rule_id))

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'PUT' Methods

    def modify_load_balancer(
            self,
            load_balancer_id=None,
            name=None,
            description=None,
            health_check_test=None,
            health_check_interval=None,
            health_check_path=None,
            health_check_parse=None,
            persistence=None,
            persistence_time=None,
            method=None):

        # Error Handling
        if(load_balancer_id is None):
            raise ValueError('load_balancer_id is a required parameter')

        if(method is not None and method != 'ROUND_ROBIN' and
           method != 'LEAST_CONNECTIONS'):
            raise ValueError(('method must be set to either "ROUND_ROBIN" '
                              'or "LEAST_CONNECTIONS".'))

        if(health_check_test is not None and health_check_test != 'TCP'):
            raise ValueError(('health_check_test must be set to "TCP". '
                              '"HTTP" is not currently supported.'))

        if(health_check_interval is not None and health_check_interval < 5 and health_check_interval > 300):
            raise ValueError(('health_check_interval must be an integer '
                              'between 5 and 300.'))

        if(persistence_time is not None and persistence_time < 30 and persistence_time > 1200):
            raise ValueError(('persistence_time must be an integer '
                              'between 30 and 1200.'))

        # Perform Request
        data = {
            'name': name,
            'description': description,
            'health_check_test': health_check_test,
            'health_check_interval': health_check_interval,
            'health_check_path': health_check_path,
            'health_check_parse': health_check_parse,
            'persistence': persistence,
            'persistence_time': persistence_time,
            'method': method
        }

        url = '%s/load_balancers/%s' % (self.base_url, load_balancer_id)

        try:
            r = requests_retry_session().put(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'POST' Methods

    def create_load_balancer(self, load_balancer=None,
                             load_balancer_rules=None):

        # Error Handling
        if(load_balancer is None):
            raise ValueError(('load_balancer parameter is required.  Must '
                              'pass a LoadBalancer object.'))
        if(load_balancer_rules is None):
            raise ValueError(('load_balancer_rules parameter is required. '
                              'Must pass a list with at least one '
                              'LoadBalancerRule object.'))
        if(load_balancer.specs['method'] is not None and
                load_balancer.specs['method'] != 'ROUND_ROBIN' and
                load_balancer.specs['method'] != 'LEAST_CONNECTIONS'):
            raise ValueError(('method must be set to either "ROUND_ROBIN" '
                              'or "LEAST_CONNECTIONS".'))

        # Unpack rules
        rules = []

        for value in load_balancer_rules:
            rules.append(value.rule_set)

        # Perform Request
        load_balancer.specs['rules'] = rules

        url = '%s/load_balancers' % self.base_url

        try:
            r = requests_retry_session().post(
                url, headers=self.header, json=load_balancer.specs)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            # Assign new load_balancer_id back to calling LoadBalancer object
            response = r.json()

            load_balancer.specs.update(load_balancer_id=response['id'])
            load_balancer.specs.update(api_token=self.header)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def attach_load_balancer_server(self, load_balancer_id=None,
                                    server_ips=None):

        # Error Handling
        if(load_balancer_id is None):
            raise ValueError('load_balancer_id is a required parameter.')
        if(server_ips is None):
            raise ValueError(('server_ips is a required parameter. Must '
                              'pass a list with at least one AttachServer '
                              'object'))

        # Unpack servers
        servers = []

        for value in server_ips:
            servers.append(value.server_ip_id)

        # Perform Request
        data = {'server_ips': servers}

        url = ('%s/load_balancers/%s/server_ips' %
               (self.base_url, load_balancer_id))

        try:
            r = requests_retry_session().post(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def add_load_balancer_rule(self, load_balancer_id=None,
                               load_balancer_rules=None):

        # Error Handling
        if(load_balancer_id is None):
            raise ValueError('load_balancer_id is a required parameter.')
        if(load_balancer_rules is None):
            raise ValueError(('load_balancer_rules is a required '
                              'parameter. Must pass a list with at least one '
                              'LoadBalancerRule object'))

        # Unpack rules
        rules = []

        for value in load_balancer_rules:
            rules.append(value.rule_set)

        # Perform Request
        data = {'rules': rules}

        url = ('%s/load_balancers/%s/rules' %
               (self.base_url, load_balancer_id))

        try:
            r = requests_retry_session().post(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'DELETE' Methods

    def delete_load_balancer(self, load_balancer_id=None):

        # Error Handling
        if(load_balancer_id is None):
            raise ValueError('load_balancer_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = '%s/load_balancers/%s' % (self.base_url, load_balancer_id)

        try:
            r = requests_retry_session().delete(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def remove_load_balancer_server(self, load_balancer_id=None,
                                    server_ip_id=None):

        # Error Handling
        if(load_balancer_id is None):
            raise ValueError('load_balancer_id is a required parameter.')
        if(server_ip_id is None):
            raise ValueError('server_ip_id is a required parameter.')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = ('%s/load_balancers/%s/server_ips/%s' %
               (self.base_url, load_balancer_id, server_ip_id))

        try:
            r = requests_retry_session().delete(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def remove_load_balancer_rule(self, load_balancer_id=None, rule_id=None):

        # Error Handling
        if(load_balancer_id is None):
            raise ValueError('load_balancer_id is a required parameter.')
        if(rule_id is None):
            raise ValueError('rule_id is a required parameter.')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = ('%s/load_balancers/%s/rules/%s' %
               (self.base_url, load_balancer_id, rule_id))

        try:
            r = requests_retry_session().delete(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # Public IP Functions

    # 'GET' Methods

    def list_public_ips(self, page=None, per_page=None, sort=None, q=None,
                        fields=None):

        # Perform Request
        parameters = {
            'page': page,
            'per_page': per_page,
            'sort': sort,
            'q': q,
            'fields': fields
        }

        url = '%s/public_ips' % self.base_url

        try:
            r = requests_retry_session().get(url, headers=self.header, params=parameters)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_public_ip(self, ip_id=None):

        # Error Handling
        if(ip_id is None):
            raise ValueError('ip_id is a required parameter')

        # Perform Request
        url = '%s/public_ips/%s' % (self.base_url, ip_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'POST' Methods

    def create_public_ip(self, reverse_dns=None, ip_type=None,
                         datacenter_id=None):

        # Error Handling
        if(ip_type != 'IPV4' and ip_type is not None):
            raise ValueError('ip_type must be set to "IPV4".')

        # Perform Request
        data = {
            'reverse_dns': reverse_dns,
            'type': ip_type,
            'datacenter_id': datacenter_id
        }

        url = '%s/public_ips' % self.base_url

        try:
            r = requests_retry_session().post(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'PUT' Methods

    def modify_public_ip(self, ip_id=None, reverse_dns=None):

        # Error Handling
        if(ip_id is None):
            raise ValueError('ip_id is a required parameter')

        # Perform Request
        data = {'reverse_dns': reverse_dns}

        url = '%s/public_ips/%s' % (self.base_url, ip_id)

        try:
            r = requests_retry_session().put(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'DELETE' Methods

    def delete_public_ip(self, ip_id=None):

        # Error Handling
        if(ip_id is None):
            raise ValueError('ip_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = '%s/public_ips/%s' % (self.base_url, ip_id)

        try:
            r = requests_retry_session().delete(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # Private Network Functions

    # 'GET' Methods

    def list_private_networks(self, page=None, per_page=None, sort=None,
                              q=None, fields=None):

        # Perform Request
        parameters = {
            'page': page,
            'per_page': per_page,
            'sort': sort,
            'q': q,
            'fields': fields
        }

        url = '%s/private_networks' % self.base_url

        try:
            r = requests_retry_session().get(url, headers=self.header, params=parameters)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_private_network(self, private_network_id):

        # Error Handling
        if(private_network_id is None):
            raise ValueError('private_network_id is a required parameter')

        # Perform Request
        url = '%s/private_networks/%s' % (self.base_url, private_network_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def list_private_network_servers(self, private_network_id=None):

        # Error Handling
        if(private_network_id is None):
            raise ValueError('private_network_id is a required parameter')

        # Perform Request
        url = ('%s/private_networks/%s/servers' %
               (self.base_url, private_network_id))

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_private_network_server(self, private_network_id=None,
                                   server_id=None):

        # Error Handling
        if(private_network_id is None):
            raise ValueError('private_network_id is a required parameter')
        if(server_id is None):
            raise ValueError('server_id is a required parameter')

        # Perform Request
        url = ('%s/private_networks/%s/servers/%s' %
               (self.base_url, private_network_id, server_id))

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'POST' Methods

    def create_private_network(self, private_network=None):

        # Error Handling
        if(private_network.name is None):
            raise ValueError('name is a required parameter')

        # Perform Request
        data = {
            'name': private_network.name,
            'description': private_network.description,
            'network_address': private_network.network_address,
            'subnet_mask': private_network.subnet_mask,
            'datacenter_id': private_network.datacenter_id
        }

        url = '%s/private_networks' % self.base_url

        try:
            r = requests_retry_session().post(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            # Assign new private_network_id back to calling PrivateNetwork
            # object
            response = r.json()

            private_network.specs.update(private_network_id=response['id'])
            private_network.specs.update(api_token=self.header)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def attach_private_network_servers(self, private_network_id=None,
                                       server_ids=None):

        # Error Handling
        if(private_network_id is None):
            raise ValueError('private_network_id is a required parameter')
        if(server_ids is None):
            raise ValueError(('server_ids is a required parameter.  Make '
                              'sure you pass a list with at least one '
                              'server_id string'))

        # Unpack servers
        servers = []

        for value in server_ids:
            servers.append(value.server_id)

        # Perform Request
        data = {'servers': servers}

        url = ('%s/private_networks/%s/servers' %
               (self.base_url, private_network_id))

        try:
            r = requests_retry_session().post(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'PUT' Methods

    def modify_private_network(
            self,
            private_network_id=None,
            name=None,
            description=None,
            network_address=None,
            subnet_mask=None):

        # Perform Request
        data = {
            'name': name,
            'description': description,
            'network_address': network_address,
            'subnet_mask': subnet_mask
        }

        url = '%s/private_networks/%s' % (self.base_url, private_network_id)

        try:
            r = requests_retry_session().put(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'DELETE' Methods

    def delete_private_network(self, private_network_id=None):

        # Error Handling
        if(private_network_id is None):
            raise ValueError('private_network_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = '%s/private_networks/%s' % (self.base_url, private_network_id)

        try:
            r = requests_retry_session().delete(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def remove_private_network_server(self, private_network_id=None,
                                      server_id=None):

        # Error Handling
        if(private_network_id is None):
            raise ValueError('private_network_id is a required parameter')
        if(server_id is None):
            raise ValueError('server_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = ('%s/private_networks/%s/servers/%s' %
               (self.base_url, private_network_id, server_id))

        try:
            r = requests_retry_session().delete(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # Monitoring Center Functions

    # 'GET' Methods

    def list_server_usages(self, page=None, per_page=None, sort=None,
                           q=None, fields=None):

        # Perform Request
        parameters = {
            'page': page,
            'per_page': per_page,
            'sort': sort,
            'q': q,
            'fields': fields
        }

        url = '%s/monitoring_center' % self.base_url

        try:
            r = requests_retry_session().get(url, headers=self.header, params=parameters)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_usage(self, server_id=None, period='LAST_24H',
                  start_date=None, end_date=None):

        # Error Handling
        if(server_id is None):
            raise ValueError('server_id is a required parameter')
        if(period == 'CUSTOM'):
            if(start_date is None):
                raise ValueError(('start_date parameter is required when '
                                  'using CUSTOM period'))
            if(end_date is None):
                raise ValueError(('end_date parameter is required when '
                                  'using CUSTOM period'))

        # Perform Request
        parameters = {
            'period': period,
            'start_date': start_date,
            'end_date': end_date
        }

        url = '%s/monitoring_center/%s' % (self.base_url, server_id)

        try:
            r = requests_retry_session().get(url, headers=self.header, params=parameters)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # Monitoring Policy Functions

    # 'GET' Methods

    def list_monitoring_policies(self, page=None, per_page=None,
                                 sort=None, q=None, fields=None):

        # Perform Request
        parameters = {
            'page': page,
            'per_page': per_page,
            'sort': sort,
            'q': q,
            'fields': fields
        }

        url = '%s/monitoring_policies' % self.base_url

        try:
            r = requests_retry_session().get(url, headers=self.header, params=parameters)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_monitoring_policy(self, monitoring_policy_id=None):

        # Error Handling
        if(monitoring_policy_id is None):
            raise ValueError('monitoring_policy_id is a required parameter')

        # Perform Request
        url = ('%s/monitoring_policies/%s' %
               (self.base_url, monitoring_policy_id))

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def list_monitoring_policy_ports(self, monitoring_policy_id=None):

        # Error Handling
        if(monitoring_policy_id is None):
            raise ValueError('monitoring_policy_id is a required parameter')

        # Perform Request
        url = ('%s/monitoring_policies/%s/ports' %
               (self.base_url, monitoring_policy_id))

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_monitoring_policy_port(self, monitoring_policy_id=None,
                                   port_id=None):

        # Error Handling
        if(monitoring_policy_id is None):
            raise ValueError('monitoring_policy_id is a required parameter')
        if(port_id is None):
            raise ValueError('port_id is a required parameter')

        # Perform Request
        url = ('%s/monitoring_policies/%s/ports/%s' %
               (self.base_url, monitoring_policy_id, port_id))

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def list_monitoring_policy_processes(self, monitoring_policy_id=None):

        # Error Handling
        if(monitoring_policy_id is None):
            raise ValueError('monitoring_policy_id is a required parameter')

        # Perform Request
        url = ('%s/monitoring_policies/%s/processes' %
               (self.base_url, monitoring_policy_id))

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_monitoring_policy_process(self, monitoring_policy_id=None,
                                      process_id=None):

        # Error Handling
        if(monitoring_policy_id is None):
            raise ValueError('monitoring_policy_id is a required parameter')
        if(process_id is None):
            raise ValueError('process_id is a required parameter')

        # Perform Request
        url = ('%s/monitoring_policies/%s/processes/%s' %
               (self.base_url, monitoring_policy_id, process_id))

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def list_monitoring_policy_servers(self, monitoring_policy_id=None):

        # Error Handling
        if(monitoring_policy_id is None):
            raise ValueError('monitoring_policy_id is a required parameter')

        # Perform Request
        url = ('%s/monitoring_policies/%s/servers' %
               (self.base_url, monitoring_policy_id))

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_monitoring_policy_server(self, monitoring_policy_id=None,
                                     server_id=None):

        # Error Handling
        if(monitoring_policy_id is None):
            raise ValueError('monitoring_policy_id is a required parameter')
        if(server_id is None):
            raise ValueError('server_id is a required parameter')

        # Perform Request
        url = ('%s/monitoring_policies/%s/servers/%s' %
               (self.base_url, monitoring_policy_id, server_id))

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'DELETE' Methods

    def delete_monitoring_policy(self, monitoring_policy_id=None):

        # Error Handling
        if(monitoring_policy_id is None):
            raise ValueError('monitoring_policy_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = ('%s/monitoring_policies/%s' %
               (self.base_url, monitoring_policy_id))

        try:
            r = requests_retry_session().delete(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def delete_monitoring_policy_port(self, monitoring_policy_id=None,
                                      port_id=None):

        # Error Handling
        if(monitoring_policy_id is None):
            raise ValueError('monitoring_policy_id is a required parameter')
        if(port_id is None):
            raise ValueError('port_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = ('%s/monitoring_policies/%s/ports/%s' %
               (self.base_url, monitoring_policy_id, port_id))

        try:
            r = requests_retry_session().delete(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def delete_monitoring_policy_process(self, monitoring_policy_id=None,
                                         process_id=None):

        # Error Handling
        if(monitoring_policy_id is None):
            raise ValueError('monitoring_policy_id is a required parameter')
        if(process_id is None):
            raise ValueError('process_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = ('%s/monitoring_policies/%s/processes/%s' %
               (self.base_url, monitoring_policy_id, process_id))

        try:
            r = requests_retry_session().delete(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def detach_monitoring_policy_server(self, monitoring_policy_id=None,
                                        server_id=None):

        # Error Handling
        if(monitoring_policy_id is None):
            raise ValueError('monitoring_policy_id is a required parameter')
        if(server_id is None):
            raise ValueError('server_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = ('%s/monitoring_policies/%s/servers/%s' %
               (self.base_url, monitoring_policy_id, server_id))

        try:
            r = requests_retry_session().delete(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'POST' Methods

    def create_monitoring_policy(self, monitoring_policy=None,
                                 thresholds=None, ports=None, processes=None):

        # Error Handling
        if(monitoring_policy is None):
            raise ValueError(('monitoring_policy is a required parameter. '
                              'Make sure you pass a MonitoringPolicy object.'))
        if(thresholds is None):
            raise ValueError(('thresholds is a required parameter.  Make '
                              'sure you pass a list with all 5 Threshold '
                              'objects(cpu, ram, disk, transfer, '
                              'internal_ping).'))
        if(ports is None):
            raise ValueError(
                ('ports is a required parameter.  Make sure '
                 'you pass a list with at least one Port object.'))
        if(processes is None):
            raise ValueError(('processes is a required parameter.  Make '
                              'sure you pass a list with at least one '
                              'Process object.'))

        # Unpack Thresholds
        new_thresholds = {}

        for value in thresholds:
            new_thresholds[value.entity] = {
                'warning': {
                    'value': value.warning_value,
                    'alert': value.warning_alert
                },
                'critical': {
                    'value': value.critical_value,
                    'alert': value.critical_alert
                }
            }

        # Unpack Ports
        new_ports = []

        for value in ports:
            new_ports.append(value.specs)

        # Unpack Processes
        new_processes = []

        for value in processes:
            new_processes.append(value.process_set)

        # Add Ports, Processes, and Thresholds to Monitoring Policy object
        monitoring_policy.specs['thresholds'] = new_thresholds
        monitoring_policy.specs['ports'] = new_ports
        monitoring_policy.specs['processes'] = new_processes

        # Perform Request
        url = '%s/monitoring_policies' % self.base_url

        try:
            r = requests_retry_session().post(url, headers=self.header,
                                              json=monitoring_policy.specs)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            # Assign new monitoring_policy_id back to calling MonitoringPolicy
            # object
            response = r.json()

            monitoring_policy.specs.update(monitoring_policy_id=response['id'])
            monitoring_policy.specs.update(api_token=self.header)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def add_port(self, monitoring_policy_id=None, ports=None):

        # Error Handling
        if(monitoring_policy_id is None):
            raise ValueError('monitoring_policy_id is a required parameter')
        if(ports is None):
            raise ValueError(('ports is a required parameter. Make sure you '
                              'send in a list with at least one Port object'))

        # Unpack ports
        new_ports = []

        for value in ports:
            new_ports.append(value.specs)

        # Perform Request
        data = {'ports': new_ports}

        url = ('%s/monitoring_policies/%s/ports' %
               (self.base_url, monitoring_policy_id))

        try:
            r = requests_retry_session().post(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def add_process(self, monitoring_policy_id=None, processes=None):

        # Error Handling
        if(monitoring_policy_id is None):
            raise ValueError('monitoring_policy_id is a required parameter')
        if(processes is None):
            raise ValueError(('processes is a required parameter. Make '
                              'sure you send in a list with at least one '
                              'Process object'))

        # Unpack processes
        new_processes = []

        for value in processes:
            new_processes.append(value.process_set)

        # Perform Request
        data = {'processes': new_processes}

        url = ('%s/monitoring_policies/%s/processes' %
               (self.base_url, monitoring_policy_id))

        try:
            r = requests_retry_session().post(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def attach_monitoring_policy_server(self, monitoring_policy_id=None,
                                        servers=None):

        # Error Handling
        if(monitoring_policy_id is None):
            raise ValueError('monitoring_policy_id is a required parameter')
        if(servers is None):
            raise ValueError(('servers is a required parameter. Make sure '
                              'you send in a list with at least one '
                              'AttachServer object'))

        # Unpack servers
        add_servers = []

        for value in servers:
            add_servers.append(value.server_id)

        # Perform Request
        data = {'servers': add_servers}

        url = ('%s/monitoring_policies/%s/servers' %
               (self.base_url, monitoring_policy_id))

        try:
            r = requests_retry_session().post(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'PUT' Methods

    def modify_monitoring_policy(
            self,
            monitoring_policy_id=None,
            monitoring_policy=None,
            thresholds=None,
            test=False):

        try:
            # Error Handling
            if(monitoring_policy_id is None):
                raise ValueError(
                    'monitoring_policy_id is a required parameter')

            # Use flag to skip this live API call when running unit test
            if(test == False):
                # Make request for existing monitoring policy object
                json = self.get_monitoring_policy(
                    monitoring_policy_id=monitoring_policy_id)

                # Update policy specs with new values, if necessary.
                if(monitoring_policy):
                    if(json['name'] != monitoring_policy.specs['name']):
                        if(monitoring_policy.specs['name'] is not None):
                            json['name'] = monitoring_policy.specs['name']

                    if(json['description'] !=
                            monitoring_policy.specs['description']):
                        if(monitoring_policy.specs['description'] is not None):
                            json['description'] = monitoring_policy.specs['description']

                    if(json['email'] != monitoring_policy.specs['email']):
                        if(monitoring_policy.specs['email'] is not None):
                            json['email'] = monitoring_policy.specs['email']

                # Unpack thresholds
                if(thresholds):
                    new_thresholds = {}

                    for value in thresholds:
                        new_thresholds[value.entity] = {
                            'warning': {
                                'value': value.warning_value,
                                'alert': value.warning_alert
                            },
                            'critical': {
                                'value': value.critical_value,
                                'alert': value.critical_alert
                            }
                        }

                    # Compare all threshold values and update, if necessary.
                    threshold_entities = ['cpu', 'ram', 'disk', 'transfer',
                                          'internal_ping']

                    for value in threshold_entities:

                        if(value in new_thresholds.keys()):
                            if(json['thresholds'][value]['warning']['value'] !=
                                    new_thresholds[value]['warning']['value']):
                                json['thresholds'][value]['warning']['value'] = new_thresholds[value]['warning']['value']

                            if(json['thresholds'][value]['warning']['alert'] !=
                                    new_thresholds[value]['warning']['alert']):
                                json['thresholds'][value]['warning']['alert'] = new_thresholds[value]['warning']['alert']

                            if(json['thresholds'][value]['critical']['value'] !=
                                    new_thresholds[value]['critical']['value']):
                                json['thresholds'][value]['critical']['value'] = new_thresholds[value]['critical']['value']

                            if(json['thresholds'][value]['critical']['alert'] !=
                                    new_thresholds[value]['critical']['alert']):
                                json['thresholds'][value]['critical']['alert'] = new_thresholds[value]['critical']['alert']

                # Perform Request
                data = {
                    'name': json['name'],
                    'description': json['description'],
                    'email': json['email'],
                    'thresholds': json['thresholds']
                }

                url = ('%s/monitoring_policies/%s' %
                       (self.base_url, monitoring_policy_id))

                r = requests_retry_session().put(url, headers=self.header, json=data)

            else:
                # Mock Request for Unit Testing
                r = requests_retry_session().put(
                    self.base_url + '/monitoring_policies/%s' %
                    (monitoring_policy_id), headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def modify_port(self, monitoring_policy_id=None, port_id=None, port=None,
                    test=False):

        try:
            # Error Handling
            if(monitoring_policy_id is None):
                raise ValueError(
                    'monitoring_policy_id is a required parameter')
            if(port_id is None):
                raise ValueError('port_id is a required parameter')

            # Use flag to skip this live API call when running unit test
            if(test == False):
                # Make request for existing port object
                json = self.get_monitoring_policy_port(
                    monitoring_policy_id=monitoring_policy_id, port_id=port_id)
                del json['id']

                # Update port object with new values, if necessary.
                if(json['alert_if'] != port.specs['alert_if']):
                    if(port.specs['alert_if'] is not None):
                        json['alert_if'] = port.specs['alert_if']

                if(json['email_notification'] != port.specs['email_notification']):
                    if(port.specs['email_notification'] is not None):
                        json['email_notification'] = port.specs['email_notification']

                # Perform Request
                data = {'ports': json}

                url = ('%s/monitoring_policies/%s/ports/%s' %
                       (self.base_url, monitoring_policy_id, port_id))

                r = requests_retry_session().put(url, headers=self.header, json=data)

            else:
                # Mock Request for Unit Testing
                r = requests_retry_session().put(
                    self.base_url + '/monitoring_policies/%s/ports/%s' %
                    (monitoring_policy_id, port_id), headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def modify_process(self, monitoring_policy_id=None, process_id=None,
                       process=None, test=False):

        try:
            # Error Handling
            if(monitoring_policy_id is None):
                raise ValueError(
                    'monitoring_policy_id is a required parameter')
            if(process_id is None):
                raise ValueError('process_id is a required parameter')

            # Use flag to skip this live API call when running unit test
            if(test == False):
                # Make request for existing process object
                json = self.get_monitoring_policy_process(
                    monitoring_policy_id=monitoring_policy_id,
                    process_id=process_id)
                del json['id']

                # Update process object with new values, if necessary.
                if(json['alert_if'] != process.process_set['alert_if']):
                    if(process.process_set['alert_if'] is not None):
                        json['alert_if'] = process.process_set['alert_if']

                if(json['email_notification'] !=
                        process.process_set['email_notification']):
                    if(process.process_set['email_notification'] is not None):
                        json['email_notification'] = process.process_set['email_notification']

                # Perform Request
                data = {'processes': json}

                url = ('%s/monitoring_policies/%s/processes/%s' %
                       (self.base_url, monitoring_policy_id, process_id))

                r = requests_retry_session().put(url, headers=self.header, json=data)

            else:
                # Mock Request for Unit Testing
                r = requests_retry_session().put(
                    self.base_url + '/monitoring_policies/%s/processes/%s' %
                    (monitoring_policy_id, process_id), headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # Log Functions

    # 'GET' Methods

    def list_logs(
            self,
            page=None,
            per_page=None,
            sort=None,
            q=None,
            fields=None,
            period='LAST_24H',
            start_date=None,
            end_date=None):

        # Error Handling
        if(period == 'CUSTOM'):
            if(start_date is None):
                raise ValueError(('start_date parameter is required when '
                                  'using CUSTOM period'))
            if(end_date is None):
                raise ValueError(('end_date parameter is required when '
                                  'using CUSTOM period'))

        # Perform Request
        parameters = {
            'page': page,
            'per_page': per_page,
            'sort': sort,
            'q': q,
            'fields': fields,
            'period': period,
            'start_date': start_date,
            'end_date': end_date
        }

        url = '%s/logs' % self.base_url

        try:
            r = requests_retry_session().get(url, headers=self.header, params=parameters)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_log(self, log_id=None):

        # Error Handling
        if(log_id is None):
            raise ValueError('log_id parameter is required')

        # Perform Request
        url = '%s/logs/%s' % (self.base_url, log_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # User Functions

    # 'GET' Methods

    def list_users(self, page=None, per_page=None, sort=None, q=None,
                   fields=None):

        # Perform Request
        parameters = {
            'page': page,
            'per_page': per_page,
            'sort': sort,
            'q': q,
            'fields': fields
        }

        url = '%s/users' % self.base_url

        try:
            r = requests_retry_session().get(url, headers=self.header, params=parameters)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_user(self, user_id=None):

        # Error Handling
        if(user_id is None):
            raise ValueError('user_id is a required parameter')

        # Perform Request
        url = '%s/users/%s' % (self.base_url, user_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def api_info(self, user_id=None):

        # Error Handling
        if(user_id is None):
            raise ValueError('user_id is a required parameter')

        # Perform Request
        url = '%s/users/%s/api' % (self.base_url, user_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def show_api_key(self, user_id=None):

        # Error Handling
        if(user_id is None):
            raise ValueError('user_id is a required parameter')

        # Perform Request
        url = '%s/users/%s/api/key' % (self.base_url, user_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def show_user_permissions(self):

        # Perform Request
        url = '%s/users/current_user_permissions' % (self.base_url)

        r = requests.get(url, headers=self.header)

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def ips_api_access_allowed(self, user_id=None):

        # Error Handling
        if(user_id is None):
            raise ValueError('user_id is a required parameter')

        # Perform Request
        url = '%s/users/%s/api/ips' % (self.base_url, user_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'POST' Methods

    def create_user(self, name=None, password=None, email=None,
                    description=None):

        # Error Handling
        if(name is None):
            raise ValueError('name is a required parameter')
        if(password is None):
            raise ValueError('password is a required parameter')

        # Perform Request
        data = {
            'name': name,
            'password': password,
            'email': email,
            'description': description
        }

        url = '%s/users' % self.base_url

        try:
            r = requests_retry_session().post(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def add_user_ip(self, user_id=None, user_ips=None):

        # Error Handling
        if(user_id is None):
            raise ValueError('user_id is a required parameter')
        if(user_ips is None):
            raise ValueError(('user_ips is a required parameter. Make '
                              'sure you pass a list with at least '
                              'one IP string.'))

        # Unpack IPs
        ips = []

        for value in user_ips:
            ips.append(value)

        # Perform Request
        data = {'ips': ips}

        url = '%s/users/%s/api/ips' % (self.base_url, user_id)

        try:
            r = requests_retry_session().post(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'PUT' Methods

    def modify_user(self, user_id=None, description=None, email=None,
                    password=None, state=None):

        # Error Handling
        if(user_id is None):
            raise ValueError('user_id is a required parameter')
        if(password is not None) and (len(password) < 8):
            raise ValueError('password must be at least 8 characters long')
        if(state is not None) and (state != 'ACTIVE') and (state != 'DISABLE'):
            raise ValueError('state should be set to "ACTIVE" or "DISABLE".')

        # Perform Request
        data = {
            'description': description,
            'email': email,
            'password': password,
            'state': state
        }

        url = '%s/users/%s' % (self.base_url, user_id)

        try:
            r = requests_retry_session().put(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def modify_user_api(self, user_id=None, active=None):

        # Error Handling
        if(user_id is None):
            raise ValueError('user_id is a required parameter')
        if(active is not None) and (active != True) and (active != False):
            raise ValueError('active parameter only accepts a boolean value')

        # Perform Request
        data = {'active': active}

        url = '%s/users/%s/api' % (self.base_url, user_id)

        try:
            r = requests_retry_session().put(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def change_api_key(self, user_id=None):

        # Error Handling
        if(user_id is None):
            raise ValueError('user_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = '%s/users/%s/api/key' % (self.base_url, user_id)

        try:
            r = requests_retry_session().put(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'DELETE' Methods

    def delete_user(self, user_id=None):

        # Error Handling
        if(user_id is None):
            raise ValueError('user_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = '%s/users/%s' % (self.base_url, user_id)

        try:
            r = requests_retry_session().delete(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def remove_user_ip(self, user_id=None, ip=None):

        # Error Handling
        if(user_id is None):
            raise ValueError('user_id is a required parameter')
        if(ip is None):
            raise ValueError('ip is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = '%s/users/%s/api/ips/%s' % (self.base_url, user_id, ip)

        try:
            r = requests_retry_session().delete(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # Usage Functions

    # 'GET' Methods

    def list_usages(
            self,
            page=None,
            per_page=None,
            sort=None,
            q=None,
            fields=None,
            period='LAST_24H',
            start_date=None,
            end_date=None):

        # Error Handling
        if(period == 'CUSTOM'):
            if(start_date is None):
                raise ValueError(('start_date parameter is required when '
                                  'using CUSTOM period'))
            if(end_date is None):
                raise ValueError(('end_date parameter is required when '
                                  'using CUSTOM period'))

        # Perform Request
        parameters = {
            'page': page,
            'per_page': per_page,
            'sort': sort,
            'q': q,
            'fields': fields,
            'period': period,
            'start_date': start_date,
            'end_date': end_date
        }

        url = '%s/usages' % self.base_url

        try:
            r = requests_retry_session().get(url, headers=self.header, params=parameters)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_pricing(self):

        # Perform Request
        url = '%s/pricing' % (self.base_url)

        r = requests.get(url, headers=self.header)

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    # Recovery images

    # 'GET' Methods

    def list_recovery_images(self, page=None, per_page=None, sort=None,
                             q=None, fields=None):

        # Perform Request
        parameters = {
            'page': page,
            'per_page': per_page,
            'sort': sort,
            'q': q,
            'fields': fields
        }

        url = '%s/recovery_appliances' % self.base_url

        r = requests.get(url, headers=self.header, params=parameters)

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def get_recovery_image(self, image_id=None):

        # Error Handling
        if(image_id is None):
            raise ValueError('appliance_id is a required parameter')

        # Perform Request
        url = '%s/recovery_appliances/%s' % (self.base_url, image_id)

        r = requests.get(url, headers=self.header)

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    # Server Appliance Functions

    # 'GET' Methods

    def list_appliances(self, page=None, per_page=None, sort=None,
                        q=None, fields=None):

        # Perform Request
        parameters = {
            'page': page,
            'per_page': per_page,
            'sort': sort,
            'q': q,
            'fields': fields
        }

        url = '%s/server_appliances' % self.base_url

        try:
            r = requests_retry_session().get(url, headers=self.header, params=parameters)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_appliance(self, appliance_id=None):

        # Error Handling
        if(appliance_id is None):
            raise ValueError('appliance_id is a required parameter')

        # Perform Request
        url = '%s/server_appliances/%s' % (self.base_url, appliance_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # DVD Functions

    # 'GET' Methods

    def list_dvds(self, page=None, per_page=None, sort=None,
                  q=None, fields=None):

        # Perform Request
        parameters = {
            'page': page,
            'per_page': per_page,
            'sort': sort,
            'q': q,
            'fields': fields
        }

        url = '%s/dvd_isos' % self.base_url

        try:
            r = requests_retry_session().get(url, headers=self.header, params=parameters)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_dvd(self, iso_id=None):

        # Error Handling
        if(iso_id is None):
            raise ValueError('iso_id parameter is required')

        # Perform Request
        url = '%s/dvd_isos/%s' % (self.base_url, iso_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # Datacenter Functions

    # 'GET' Methods

    def list_datacenters(self, page=None, per_page=None, sort=None,
                         q=None, fields=None):

        # Perform Request
        parameters = {
            'page': page,
            'per_page': per_page,
            'sort': sort,
            'q': q,
            'fields': fields
        }

        url = '%s/datacenters' % self.base_url

        try:
            r = requests_retry_session().get(url, headers=self.header, params=parameters)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_datacenter(self, datacenter_id=None):

        # Error Handling
        if(datacenter_id is None):
            raise ValueError('datacenter_id parameter is required')

        # Perform Request
        url = '%s/datacenters/%s' % (self.base_url, datacenter_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # Pricing Functions

    # 'GET' Methods

    def pricing(self):

        # Perform Request
        url = '%s/pricing' % self.base_url

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # Ping Functions

    # 'GET' Methods

    def ping(self):

        # Perform Request
        url = '%s/ping' % self.base_url

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # Ping Auth Functions

    # 'GET' Methods

    def ping_auth(self):

        # Perform Request
        url = '%s/ping_auth' % self.base_url

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # VPN Functions

    # 'GET' Methods

    def list_vpns(self, page=None, per_page=None, sort=None, q=None,
                  fields=None):

        # Perform Request
        parameters = {
            'page': page,
            'per_page': per_page,
            'sort': sort,
            'q': q,
            'fields': fields
        }

        url = '%s/vpns' % self.base_url

        try:
            r = requests_retry_session().get(url, headers=self.header, params=parameters)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_vpn(self, vpn_id=None):

        # Error Handling
        if(vpn_id is None):
            raise ValueError('vpn_id is a required parameter')

        # Perform Request

        url = '%s/vpns/%s' % (self.base_url, vpn_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()

        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def download_config(self, vpn_id=None, file_path=None):
        # Error Handling
        if(vpn_id is None):
            raise ValueError('vpn_id is a required parameter')

        # Perform Request

        url = '%s/vpns/%s/configuration_file' % (self.base_url, vpn_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)
                body = r.json()
                filestring = base64.b64decode(body["config_zip_file"])
                zipPath = file_path + '.zip'
                with open(zipPath, 'wb') as zipFile:
                    zipFile.write(filestring)

                    return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'POST' Methods

    def create_vpn(self, vpn=None):

        # Perform Request
        data = {
            'name': vpn.name,
            'description': vpn.description,
            'datacenter_id': vpn.datacenter_id
        }

        url = '%s/vpns' % self.base_url

        try:
            r = requests_retry_session().post(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            # Assign new image_id back to calling Image object
            response = r.json()

            vpn.specs.update(vpn_id=response['id'])
            vpn.specs.update(api_token=self.header)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'DELETE' Methods

    def delete_vpn(self, vpn_id=None):

        # Error Handling
        if(vpn_id is None):
            raise ValueError('vpn_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = '%s/vpns/%s' % (self.base_url, vpn_id)

        try:
            r = requests_retry_session().delete(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'PUT' Methods

    def modify_vpn(self, vpn_id=None, name=None, description=None):

        # Error Handling
        if(vpn_id is None):
            raise ValueError('vpn_id is a required parameter')

        # Perform Request
        data = {
            'name': name,
            'description': description
        }

        url = '%s/vpns/%s' % (self.base_url, vpn_id)

        try:
            r = requests_retry_session().put(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # Role Functions

    # 'GET' Methods

    def list_roles(self, page=None, per_page=None, sort=None, q=None,
                   fields=None):

        # Perform Request
        parameters = {
            'page': page,
            'per_page': per_page,
            'sort': sort,
            'q': q,
            'fields': fields
        }

        url = '%s/roles' % self.base_url

        try:
            r = requests_retry_session().get(url, headers=self.header, params=parameters)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_role(self, role_id=None):

        # Error Handling
        if(role_id is None):
            raise ValueError('role_id is a required parameter')

        # Perform Request

        url = '%s/roles/%s' % (self.base_url, role_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def permissions(self, role_id=None):

        # Error Handling
        if(role_id is None):
            raise ValueError('role_id is a required parameter')

        # Perform Request

        url = '%s/roles/%s/permissions' % (self.base_url, role_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def current_user_permissions(self):

        # Perform Request

        url = '%s/users/current_user_permissions' % (self.base_url)

        r = requests_retry_session().get(url, headers=self.header)

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def role_users(self, role_id=None):

        # Error Handling
        if(role_id is None):
            raise ValueError('role_id is a required parameter')

        # Perform Request

        url = '%s/roles/%s/users' % (self.base_url, role_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def get_role_user(self, role_id=None, user_id=None):

        # Error Handling
        if(role_id is None):
            raise ValueError('role_id is a required parameter')

        # Perform Request
        url = '%s/roles/%s/users/%s' % (self.base_url, role_id, user_id)

        try:
            r = requests_retry_session().get(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'POST' Methods

    def create_role(self, name=None):

        # Perform Request
        data = {
            'name': name
        }

        url = '%s/roles' % self.base_url

        try:
            r = requests_retry_session().post(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def add_users(self, role_id=None, users=None):

        # Error Handling
        if(role_id is None):
            raise ValueError('role_id is a required parameter')

        # Perform Request
        data = {
            'users': users
        }

        url = '%s/roles/%s/users' % (self.base_url, role_id)

        try:
            r = requests_retry_session().post(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def clone_role(self, role_id=None, name=None):

        # Error Handling
        if(role_id is None):
            raise ValueError('role_id is a required parameter')

        # Perform Request
        data = {
            'name': name
        }

        url = '%s/roles/%s/clone' % (self.base_url, role_id)

        try:
            r = requests_retry_session().post(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'DELETE' Methods

    def delete_role(self, role_id=None):

        # Error Handling
        if(role_id is None):
            raise ValueError('role_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = '%s/roles/%s' % (self.base_url, role_id)

        try:
            r = requests_retry_session().delete(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def remove_user(self, role_id=None, user_id=None):

        # Error Handling
        if(role_id is None):
            raise ValueError('role_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = '%s/roles/%s/users/%s' % (self.base_url, role_id, user_id)

        try:
            r = requests_retry_session().delete(url, headers=self.header)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # 'PUT' Methods

    def modify_role(self, role_id=None, name=None, description=None,
                    state=None):

        # Error Handling
        if(role_id is None):
            raise ValueError('role_id is a required parameter')

        # Perform Request
        data = {
            'name': name,
            'description': description,
            'state': state
        }

        url = '%s/roles/%s' % (self.base_url, role_id)

        try:
            r = requests_retry_session().put(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    def modify_permissions(
            self,
            role_id=None,
            servers=None,
            images=None,
            shared_storages=None,
            firewalls=None,
            load_balancers=None,
            ips=None,
            private_networks=None,
            vpns=None,
            monitoring_centers=None,
            monitoring_policies=None,
            backups=None,
            logs=None,
            users=None,
            roles=None,
            usages=None,
            interactive_invoices=None):

        # Error Handling
        if(role_id is None):
            raise ValueError('role_id is a required parameter')

        # Perform Request
        data = {
            'servers': servers,
            'images': images,
            'sharedstorages': shared_storages,
            'firewalls': firewalls,
            'loadbalancers': load_balancers,
            'ips': ips,
            'privatenetwork': private_networks,
            'vpn': vpns,
            'monitoringcenter': monitoring_centers,
            'monitoringpolicies': monitoring_policies,
            'backups': backups,
            'logs': logs,
            'users': users,
            'roles': roles,
            'usages': usages,
            'interactiveinvoice': interactive_invoices
        }

        url = '%s/roles/%s/permissions' % (self.base_url, role_id)

        try:
            r = requests_retry_session().put(url, headers=self.header, json=data)

            # Handle Potential Response Errors
            if r.status_code not in self.success_codes:
                error_message = ('Error Code: %s. Error Message: %s.' %
                                 (r.status_code, r.text))
                raise Exception(error_message)

            return r.json()
        except http_client.HTTPException:
            if r is not None:
                error_message = (
                    'Error Code: %s. Error Message: %s. Response Headers :%s' %
                    (r.status_code, r.text, r.headers))
                raise Exception(error_message)
            else:
                raise

    # Block Storage Functions

    # 'GET' Methods

    def list_block_storages(self, page=None, per_page=None, sort=None,
                            q=None, fields=None):

        # Perform Request
        parameters = {
            'page': page,
            'per_page': per_page,
            'sort': sort,
            'q': q,
            'fields': fields
        }

        url = '%s/block_storages' % self.base_url

        r = requests_retry_session().get(url, headers=self.header, params=parameters)

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def get_block_storage(self, block_storage_id=None):

        # Error Handling
        if(block_storage_id is None):
            raise ValueError('block_storage_id is a required parameter')

        # Perform Request
        url = '%s/block_storages/%s' % (self.base_url, block_storage_id)

        r = requests_retry_session().get(url, headers=self.header)

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    # 'POST' Methods

    def create_block_storage(self, block_storage=None):

        # Perform Request
        data = {
            'name': block_storage.name,
            'description': block_storage.description,
            'size': block_storage.size,
            'server': block_storage.server_id,
            'datacenter_id': block_storage.datacenter_id,
            'execution_group': block_storage.execution_group
        }

        url = '%s/block_storages' % self.base_url

        r = requests_retry_session().post(url, headers=self.header, json=data)

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        # Assign new block_storage_id back to calling BlockStorage object
        response = r.json()

        block_storage.specs.update(block_storage_id=response['id'])
        block_storage.specs.update(api_token=self.header)

        return r.json()

    def attach_block_storage(self, block_storage_id=None,
                             server_id=None):

        # Error Handling
        if(block_storage_id is None):
            raise ValueError('block_storage_id is a required parameter')
        if(server_id is None):
            raise ValueError('server_id is a required parameter.')

        # Perform Request
        data = {'server': server_id}

        url = ('%s/block_storages/%s/server' %
               (self.base_url, block_storage_id))

        r = requests_retry_session().post(url, headers=self.header, json=data)

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    # 'PUT' Methods

    def modify_block_storage(self, block_storage_id=None, name=None,
                             description=None, size=None):

        # Error Handling
        if(block_storage_id is None):
            raise ValueError('block_storage_id is a required parameter')

        # Perform Request
        data = {
            'name': name,
            'description': description,
            'size': size
        }

        url = '%s/block_storages/%s' % (self.base_url, block_storage_id)

        r = requests_retry_session().put(url, headers=self.header, json=data)

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    # 'DELETE' Methods

    def delete_block_storage(self, block_storage_id=None):

        # Error Handling
        if(block_storage_id is None):
            raise ValueError('block_storage_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = '%s/block_storages/%s' % (self.base_url, block_storage_id)

        r = requests_retry_session().delete(url, headers=self.header)

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def detach_block_storage(self, block_storage_id=None):

        # Error Handling
        if(block_storage_id is None):
            raise ValueError('block_storage_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = ('%s/block_storages/%s/server' %
               (self.base_url, block_storage_id))

        r = requests_retry_session().delete(url, headers=self.header)

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    # Ssh Key Functions

    # 'GET' Methods

    def list_ssh_keys(self, page=None, per_page=None, sort=None, q=None,
                      fields=None):

        # Perform Request
        parameters = {
            'page': page,
            'per_page': per_page,
            'sort': sort,
            'q': q,
            'fields': fields
        }

        url = '%s/ssh_keys' % self.base_url

        r = requests_retry_session().get(url, headers=self.header, params=parameters)

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def get_ssh_key(self, ssh_key_id=None):

        # Error Handling
        if(ssh_key_id is None):
            raise ValueError('ssh_key_id is a required parameter')

        # Perform Request

        url = '%s/ssh_keys/%s' % (self.base_url, ssh_key_id)

        r = requests_retry_session().get(url, headers=self.header)

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    # 'POST' Methods

    def create_ssh_key(self, ssh_key=None):

        # Perform Request
        url = '%s/ssh_keys' % self.base_url

        r = requests_retry_session().post(url, headers=self.header, json=ssh_key.specs)

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        # Assign new ssh_key_id back to calling SshKey object
        response = r.json()

        ssh_key.specs.update(ssh_key_id=response['id'])
        ssh_key.specs.update(api_token=self.header)

        return r.json()

    # 'DELETE' Methods

    def delete_ssh_key(self, ssh_key_id=None):

        # Error Handling
        if(ssh_key_id is None):
            raise ValueError('ssh_key_id is a required parameter')

        # Perform Request
        self.header['content-type'] = 'application/json'

        url = '%s/ssh_keys/%s' % (self.base_url, ssh_key_id)

        r = requests_retry_session().delete(url, headers=self.header)

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    # 'PUT' Methods

    def modify_ssh_key(self, ssh_key_id=None, name=None, description=None):

        # Error Handling
        if(ssh_key_id is None):
            raise ValueError('ssh_key_id is a required parameter')

        # Perform Request
        data = {
            'name': name,
            'description': description
        }

        url = '%s/ssh_keys/%s' % (self.base_url, ssh_key_id)

        r = requests_retry_session().put(url, headers=self.header, json=data)

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()


# Utility Classes

class Server(object):

    # Init Function
    def __init__(
            self,
            name=None,
            description=None,
            fixed_instance_size_id=None,
            vcore=None,
            cores_per_processor=None,
            ram=None,
            appliance_id=None,
            password=None,
            power_on=None,
            firewall_policy_id=None,
            ip_id=None,
            load_balancer_id=None,
            monitoring_policy_id=None,
            datacenter_id=None,
            rsa_key=None,
            private_network_id=None,
            server_type=None,
            public_key=None,
            baremetal_model_id=None,
            ipv6_range=None,
            hostname=None,
            execution_group=None):

        self.first_password = None
        self.first_ip = None

        self.specs = {
            'name': name,
            'description': description,
            'hardware': {
                'fixed_instance_size_id': fixed_instance_size_id,
                'vcore': vcore,
                'cores_per_processor': cores_per_processor,
                'ram': ram,
                'baremetal_model_id': baremetal_model_id
            },
            'appliance_id': appliance_id,
            'password': password,
            'power_on': power_on,
            'firewall_policy_id': firewall_policy_id,
            'ip_id': ip_id,
            'load_balancer_id': load_balancer_id,
            'monitoring_policy_id': monitoring_policy_id,
            'datacenter_id': datacenter_id,
            'rsa_key': rsa_key,
            'private_network_id': private_network_id,
            'server_type': server_type,
            'public_key': public_key,
            'ipv6_range': ipv6_range,
            'hostname': hostname,
            'execution_group': execution_group
        }

        self.base_url = 'https://cloudpanel-api.1and1.com/v1'
        self.success_codes = (200, 201, 202)
        self.good_states = (
            'ACTIVE',
            'ENABLED',
            'POWERED_ON',
            'POWERED_OFF',
            'ON RECOVERY')

    def __repr__(self):
        return (
            'Server: name=%s, description=%s, fixed_instance_size_id=%s, '
            'vcore=%s, cores_per_processor=%s, ram=%s, baremetal_model_id=%s, appliance_id=%s, '
            'password=%s, power_on=%s, firewall_policy_id=%s, ip_id=%s, '
            'load_balancer_id=%s, monitoring_policy_id=%s, '
            'rsa_key=%s, datacenter_id=%s, first_password=%s, '
            'first_ip=%s, public_key=%s, server_type=%s, ipv6_range=%s, execution_group=%s, hostname=%s' %
            (self.specs['name'],
             self.specs['description'],
             self.specs['hardware']['fixed_instance_size_id'],
             self.specs['hardware']['vcore'],
             self.specs['hardware']['cores_per_processor'],
             self.specs['hardware']['ram'],
             self.specs['hardware']['baremetal_model_id'],
             self.specs['appliance_id'],
             self.specs['password'],
             self.specs['power_on'],
             self.specs['firewall_policy_id'],
             self.specs['ip_id'],
             self.specs['load_balancer_id'],
             self.specs['monitoring_policy_id'],
             self.specs['rsa_key'],
             self.specs['datacenter_id'],
             self.first_password,
             self.first_ip,
             self.specs['server_type'],
             self.specs['ipv6_range'],
             self.specs['execution_group'],
             self.specs['hostname'],
             ))

    def get(self):

        # Perform Request
        url = ('%s/servers/%s' %
               (self.base_url, self.specs['server_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def hardware(self):

        # Perform Request
        url = ('%s/servers/%s/hardware' %
               (self.base_url, self.specs['server_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def hdds(self):

        # Perform Request
        url = ('%s/servers/%s/hardware/hdds' %
               (self.base_url, self.specs['server_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def image(self):

        # Perform Request
        url = ('%s/servers/%s/image' %
               (self.base_url, self.specs['server_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def ips(self):

        # Perform Request
        url = ('%s/servers/%s/ips' %
               (self.base_url, self.specs['server_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def status(self):

        # Perform Request
        url = ('%s/servers/%s/status' %
               (self.base_url, self.specs['server_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def dvd(self):

        # Perform Request
        url = ('%s/servers/%s/dvd' %
               (self.base_url, self.specs['server_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def private_networks(self):

        # Perform Request
        url = ('%s/servers/%s/private_networks' %
               (self.base_url, self.specs['server_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def snapshots(self):

        # Perform Request
        url = ('%s/servers/%s/snapshots' %
               (self.base_url, self.specs['server_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def wait_for(self, timeout=25, interval=15):

        # Capture start time
        start = time.time()
        duration = 0

        # Check initial server status
        url = '%s/servers/%s' % (self.base_url, self.specs['server_id'])

        r = requests_retry_session().get(url, headers=self.specs['api_token'])
        response = r.json()

        # Store initial server state and percent values
        server_state = response['status']['state']
        percent = response['status']['percent']

        # Keep polling the server's status until good
        while (server_state not in self.good_states) or (percent is not None):

            # Wait 15 seconds before polling again
            time.sleep(interval)

            # Check server status again
            r = requests_retry_session().get(
                url, headers=self.specs['api_token'])
            response = r.json()

            # Update server state and percent values
            server_state = response['status']['state']
            percent = response['status']['percent']

            # Check for timeout
            seconds = (time.time() - start)
            duration = seconds / 60
            if duration > timeout:
                print('The operation timed out after %s minutes.' % timeout)
                return

            # Parse for first IP address
            if len(response['ips']) == 1:
                self.first_ip = response['ips'][0]

        return {'duration': duration}

    def wait_deleted(self, timeout=25, interval=15):

        # Capture start time
        start = time.time()
        duration = 0

        # Check initial server status
        url = '%s/servers/%s' % (self.base_url, self.specs['server_id'])

        r = requests_retry_session().get(url, headers=self.specs['api_token'])
        response = r.json()

        # Keep polling the server's status until got 404
        while r.status_code != 404 :

            # Wait 15 seconds before polling again
            time.sleep(interval)

            # Check server status again
            r = requests_retry_session().get(
                url, headers=self.specs['api_token'])

            # Check for timeout
            seconds = (time.time() - start)
            duration = seconds / 60
            if duration > timeout:
                print('The operation timed out after %s minutes.' % timeout)
                return
        return {'duration': duration}


class Hdd(object):

    # Init Function
    def __init__(self, size=None, is_main=None):
        self.specs = {
            'size': size,
            'is_main': is_main
        }

    def __repr__(self):
        return ('HDD: size=%s, is_main=%s' %
                (self.specs['size'], self.specs['is_main']))


class AttachServer(object):

    # Init Function
    def __init__(self, server_id=None, rights=None, server_ip_id=None):
        self.server_id = server_id
        self.rights = rights
        self.server_ip_id = server_ip_id

    def __repr__(self):
        return ('AttachServer: server_id=%s, rights=%s, server_ip_id=%s' %
                (self.server_id, self.rights, self.server_ip_id))


class Image(object):

    # Init Function
    def __init__(
            self,
            server_id=None,
            name=None,
            description=None,
            frequency=None,
            num_images=None,
            source='server',
            url=None,
            os_id=None,
            isotype=None,
            type=None):

        self.server_id = server_id
        self.name = name
        self.description = description
        self.frequency = frequency
        self.num_images = num_images
        self.source = source
        self.url = url
        self.os_id = os_id
        self.type = isotype

        self.specs = {}

        self.base_url = 'https://cloudpanel-api.1and1.com/v1'
        self.success_codes = (200, 201, 202)
        self.good_states = ('ACTIVE', 'ENABLED', 'POWERED_ON', 'POWERED_OFF')

    def __repr__(self):
        return (
            'Image: server_id=%s, name=%s, description=%s, '
            'frequency=%s, num_images=%s, source=%s, url=%s'
            'os_id=%s, type=%s' %
            (self.server_id,
             self.name,
             self.description,
             self.frequency,
             self.num_images,
             self.source,
             self.url,
             self.os_id,
             self.type))

    def get(self):

        # Perform Request
        url = ('%s/images/%s' %
               (self.base_url, self.specs['image_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def wait_for(self, timeout=25, interval=15):

        # Capture start time
        start = time.time()
        duration = 0

        # Check initial image status
        url = '%s/images/%s' % (self.base_url, self.specs['image_id'])

        r = requests_retry_session().get(url, headers=self.specs['api_token'])
        response = r.json()

        # Store initial server state and percent values
        image_state = response['state']

        # Keep polling the server's status until good
        while image_state not in self.good_states:

            # Wait 15 seconds before polling again
            time.sleep(interval)

            # Check server status again
            r = requests_retry_session().get(
                url, headers=self.specs['api_token'])
            response = r.json()

            # Update server state and percent values
            image_state = response['state']

            # Check for timeout
            seconds = (time.time() - start)
            duration = seconds / 60
            if duration > timeout:
                print('The operation timed out after %s minutes.' % timeout)
                return

        return {'duration': duration}


class SharedStorage(object):

    # Init Function
    def __init__(self, name=None, description=None, size=None,
                 datacenter_id=None):

        self.name = name
        self.description = description
        self.size = size
        self.datacenter_id = datacenter_id

        self.specs = {}

        self.base_url = 'https://cloudpanel-api.1and1.com/v1'
        self.success_codes = (200, 201, 202)
        self.good_states = ('ACTIVE', 'ENABLED', 'POWERED_ON', 'POWERED_OFF')

    def __repr__(self):
        return ('Shared Storage: name=%s, description=%s, size=%s' %
                (self.name, self.description, self.size, self.datacenter_id))

    def get(self):

        # Perform Request
        url = ('%s/shared_storages/%s' %
               (self.base_url, self.specs['shared_storage_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def servers(self):

        # Perform Request
        url = ('%s/shared_storages/%s/servers' %
               (self.base_url, self.specs['shared_storage_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def wait_for(self, timeout=25, interval=5):

        # Capture start time
        start = time.time()
        duration = 0

        # Check initial image status
        url = '%s/shared_storages/%s' % (self.base_url,
                                         self.specs['shared_storage_id'])

        r = requests_retry_session().get(url, headers=self.specs['api_token'])
        response = r.json()

        # Store initial server state and percent values
        shared_storage_state = response['state']

        # Keep polling the server's status until good
        while shared_storage_state not in self.good_states:

            # Wait 15 seconds before polling again
            time.sleep(interval)

            # Check server status again
            r = requests_retry_session().get(
                url, headers=self.specs['api_token'])
            response = r.json()

            # Update server state and percent values
            shared_storage_state = response['state']

            # Check for timeout
            seconds = (time.time() - start)
            duration = seconds / 60
            if duration > timeout:
                print('The operation timed out after %s minutes.' % timeout)
                return

        return {'duration': duration}


class FirewallPolicyRule(object):

    # Init Function
    def __init__(self, protocol=None, port_from=None, port_to=None,
                 source=None, action=None, description=None, port=None):

        self.rule_set = {
            'protocol': protocol,
            'port_from': port_from,
            'port_to': port_to,
            'source': source,
            'action': action,
            'description': description,
            'port': port
        }

    def __repr__(self):
        return ('FirewallPolicyRule: protocol=%s, port_from=%s, '
                'port_to=%s, source=%s, action=%s, description=%s, port=%s' %
                (self.rule_set['protocol'], self.rule_set['port_from'],
                    self.rule_set['port_to'], self.rule_set['source'], self.rule_set['action'], self.rule_set['description'], self.rule_set['port']))


class FirewallPolicy(object):

    # Init Function
    def __init__(self, name=None, description=None):
        self.specs = {
            'name': name,
            'description': description
        }

        self.base_url = 'https://cloudpanel-api.1and1.com/v1'
        self.success_codes = (200, 201, 202)
        self.good_states = ('ACTIVE', 'ENABLED', 'POWERED_ON', 'POWERED_OFF')

    def __repr__(self):
        return ('FirewallPolicy: name=%s, description=%s' %
                (self.specs['name'], self.specs['description']))

    def get(self):

        # Perform Request
        url = ('%s/firewall_policies/%s' %
               (self.base_url, self.specs['firewall_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def ips(self):

        # Perform Request
        url = ('%s/firewall_policies/%s/server_ips' %
               (self.base_url, self.specs['firewall_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def rules(self):

        # Perform Request
        url = ('%s/firewall_policies/%s/rules' %
               (self.base_url, self.specs['firewall_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def wait_for(self, timeout=25, interval=5):

        # Capture start time
        start = time.time()
        duration = 0

        # Check initial image status
        url = '%s/firewall_policies/%s' % (self.base_url,
                                           self.specs['firewall_id'])

        r = requests_retry_session().get(url, headers=self.specs['api_token'])
        response = r.json()

        # Store initial server state and percent values
        firewall_state = response['state']

        # Keep polling the server's status until good
        while firewall_state not in self.good_states:

            # Wait 15 seconds before polling again
            time.sleep(interval)

            # Check server status again
            r = requests_retry_session().get(
                url, headers=self.specs['api_token'])
            response = r.json()

            # Update server state and percent values
            firewall_state = response['state']

            # Check for timeout
            seconds = (time.time() - start)
            duration = seconds / 60
            if duration > timeout:
                print('The operation timed out after %s minutes.' % timeout)
                return

        return {'duration': duration}


class LoadBalancerRule(object):

    # Init Function
    def __init__(self, protocol=None, port_balancer=None, port_server=None,
                 source=None):

        self.rule_set = {
            'protocol': protocol,
            'port_balancer': port_balancer,
            'port_server': port_server,
            'source': source
        }

    def __repr__(self):
        return (
            'LoadBalancerRule: protocol=%s, port_balancer=%s, '
            'port_server=%s, source=%s' %
            (self.rule_set['protocol'],
             self.rule_set['port_balancer'],
             self.rule_set['port_server'],
             self.rule_set['source']))


class LoadBalancer(object):

    # Init Function
    def __init__(self, health_check_path=None, health_check_parse=None,
                 name=None, description=None, health_check_test=None,
                 health_check_interval=None, persistence=None,
                 persistence_time=None, method=None, datacenter_id=None):

        self.specs = {
            'health_check_path': health_check_path,
            'health_check_parse': health_check_parse,
            'name': name,
            'description': description,
            'health_check_test': health_check_test,
            'health_check_interval': health_check_interval,
            'persistence': persistence,
            'persistence_time': persistence_time,
            'method': method,
            'datacenter_id': datacenter_id
        }

        self.base_url = 'https://cloudpanel-api.1and1.com/v1'
        self.success_codes = (200, 201, 202)
        self.good_states = ('ACTIVE', 'ENABLED', 'POWERED_ON', 'POWERED_OFF')

    def __repr__(self):
        return ('LoadBalancer: health_check_path=%s, health_check_parse=%s, '
                'name=%s, description=%s, health_check_test=%s, '
                'health_check_interval=%s, persistence=%s, '
                'persistence_time=%s, method=%s, datacenter_id=%s' %
                (self.specs['health_check_path'],
                    self.specs['health_check_parse'], self.specs['name'],
                    self.specs['description'], self.specs['health_check_test'],
                    self.specs['health_check_interval'],
                    self.specs['persistence'], self.specs['persistence_time'],
                    self.specs['method'], self.datacenter_id))

    def get(self):

        # Perform Request
        url = ('%s/load_balancers/%s' %
               (self.base_url, self.specs['load_balancer_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def ips(self):

        # Perform Request
        url = ('%s/load_balancers/%s/server_ips' %
               (self.base_url, self.specs['load_balancer_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def rules(self):

        # Perform Request
        url = ('%s/load_balancers/%s/rules' %
               (self.base_url, self.specs['load_balancer_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def wait_for(self, timeout=25, interval=5):

        # Capture start time
        start = time.time()
        duration = 0

        # Check initial image status
        url = '%s/load_balancers/%s' % (self.base_url,
                                        self.specs['load_balancer_id'])

        r = requests_retry_session().get(url, headers=self.specs['api_token'])
        response = r.json()

        # Store initial server state and percent values
        load_balancer_state = response['state']

        # Keep polling the server's status until good
        while load_balancer_state not in self.good_states:

            # Wait 15 seconds before polling again
            time.sleep(interval)

            # Check server status again
            r = requests_retry_session().get(
                url, headers=self.specs['api_token'])
            response = r.json()

            # Update server state and percent values
            load_balancer_state = response['state']

            # Check for timeout
            seconds = (time.time() - start)
            duration = seconds / 60
            if duration > timeout:
                print('The operation timed out after %s minutes.' % timeout)
                return

        return {'duration': duration}


class PrivateNetwork(object):

    # Init Function
    def __init__(self, name=None, description=None, network_address=None,
                 subnet_mask=None, datacenter_id=None):

        self.name = name
        self.description = description
        self.network_address = network_address
        self.subnet_mask = subnet_mask
        self.datacenter_id = datacenter_id

        self.specs = {}

        self.base_url = 'https://cloudpanel-api.1and1.com/v1'
        self.success_codes = (200, 201, 202)
        self.good_states = ('ACTIVE', 'ENABLED', 'POWERED_ON', 'POWERED_OFF')

    def __repr__(self):
        return (
            'Private Network: name=%s, description=%s, network_address=%s, '
            'subnet_mask=%s' %
            (self.name, self.description, self.network_address, self.subnet_mask))

    def get(self):

        # Perform Request
        url = ('%s/private_networks/%s' %
               (self.base_url, self.specs['private_network_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def servers(self):

        # Perform Request
        url = ('%s/private_networks/%s/servers' %
               (self.base_url, self.specs['private_network_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def wait_for(self, timeout=25, interval=5):

        # Capture start time
        start = time.time()
        duration = 0

        # Check initial image status
        url = '%s/private_networks/%s' % (self.base_url,
                                          self.specs['private_network_id'])

        r = requests_retry_session().get(url, headers=self.specs['api_token'])
        response = r.json()

        # Store initial server state and percent values
        private_network_state = response['state']

        # Keep polling the server's status until good
        while private_network_state not in self.good_states:

            # Wait 15 seconds before polling again
            time.sleep(interval)

            # Check server status again
            r = requests_retry_session().get(
                url, headers=self.specs['api_token'])
            response = r.json()

            # Update server state and percent values
            private_network_state = response['state']

            # Check for timeout
            seconds = (time.time() - start)
            duration = seconds / 60
            if duration > timeout:
                print('The operation timed out after %s minutes.' % timeout)
                return

        return {'duration': duration}


class MonitoringPolicy(object):

    # Init Function
    def __init__(self, name=None, description=None, email=None, agent=None):
        self.specs = {
            'name': name,
            'description': description,
            'email': email,
            'agent': agent
        }

        self.base_url = 'https://cloudpanel-api.1and1.com/v1'
        self.success_codes = (200, 201, 202)
        self.good_states = ('ACTIVE', 'ENABLED', 'POWERED_ON', 'POWERED_OFF')

    def __repr__(self):
        return ('MonitoringPolicy: name=%s, description=%s, email=%s, '
                'agent=%s' %
                (self.specs['name'], self.specs['description'],
                    self.specs['email'], self.specs['agent']))

    def get(self):

        # Perform Request
        url = ('%s/monitoring_policies/%s' %
               (self.base_url, self.specs['monitoring_policy_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def ports(self):

        # Perform Request
        url = ('%s/monitoring_policies/%s/ports' %
               (self.base_url, self.specs['monitoring_policy_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def processes(self):

        # Perform Request
        url = ('%s/monitoring_policies/%s/processes' %
               (self.base_url, self.specs['monitoring_policy_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def servers(self):

        # Perform Request
        url = ('%s/monitoring_policies/%s/servers' %
               (self.base_url, self.specs['monitoring_policy_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def wait_for(self, timeout=25, interval=5):

        # Capture start time
        start = time.time()
        duration = 0

        # Check initial image status
        url = '%s/monitoring_policies/%s' % (self.base_url,
                                             self.specs['monitoring_policy_id'])

        r = requests_retry_session().get(url, headers=self.specs['api_token'])
        response = r.json()

        # Store initial server state and percent values
        mp_state = response['state']

        # Keep polling the server's status until good
        while mp_state not in self.good_states:

            # Wait 15 seconds before polling again
            time.sleep(interval)

            # Check server status again
            r = requests_retry_session().get(
                url, headers=self.specs['api_token'])
            response = r.json()

            # Update server state and percent values
            mp_state = response['state']

            # Check for timeout
            seconds = (time.time() - start)
            duration = seconds / 60
            if duration > timeout:
                print('The operation timed out after %s minutes.' % timeout)
                return

        return {'duration': duration}


class Threshold(object):

    # Init Function
    def __init__(self, entity=None, warning_value=None, warning_alert=None,
                 critical_value=None, critical_alert=None):

        self.entity = entity
        self.warning_value = warning_value
        self.warning_alert = warning_alert
        self.critical_value = critical_value
        self.critical_alert = critical_alert

    def __repr__(self):
        return (
            'Threshold: entity=%s, warning_value=%s, warning_alert=%s, '
            'critical_value=%s, critical_alert=%s' %
            (self.entity,
             self.warning_value,
             self.warning_alert,
             self.critical_value,
             self.critical_alert))


class Port(object):

    # Init Function
    def __init__(self, protocol=None, port=None, alert_if=None,
                 email_notification=None):

        self.specs = {
            'protocol': protocol,
            'port': port,
            'alert_if': alert_if,
            'email_notification': email_notification
        }

    def __repr__(self):
        return (
            'Port: protocol=%s, port=%s, alert_if=%s, '
            'email_notification=%s' %
            (self.specs['protocol'],
             self.specs['port'],
             self.specs['alert_if'],
             self.specs['email_notification']))


class Process(object):

    # Init Function
    def __init__(self, process=None, alert_if=None, email_notification=None):
        self.process_set = {
            'process': process,
            'alert_if': alert_if,
            'email_notification': email_notification
        }

    def __repr__(self):
        return ('Process: process=%s, alert_if=%s, email_notification=%s' %
                (self.process_set['process'], self.process_set['alert_if'],
                    self.process_set['email_notification']))


class Vpn(object):

    # Init Function
    def __init__(self, name=None, description=None, datacenter_id=None):

        self.name = name
        self.description = description
        self.datacenter_id = datacenter_id

        self.specs = {}

        self.base_url = 'https://cloudpanel-api.1and1.com/v1'
        self.success_codes = (200, 201, 202)
        self.good_states = ('ACTIVE', 'ENABLED', 'POWERED_ON', 'POWERED_OFF')

    def __repr__(self):
        return ('Vpn: name=%s, description=%s, datacenter_id=%s' %
                (self.name, self.description, self.datacenter_id))

    def get(self):

        # Perform Request
        url = ('%s/vpns/%s' %
               (self.base_url, self.specs['vpn_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def wait_for(self, timeout=25, interval=15):

        # Capture start time
        start = time.time()
        duration = 0

        # Check initial image status
        url = '%s/vpns/%s' % (self.base_url, self.specs['vpn_id'])

        r = requests_retry_session().get(url, headers=self.specs['api_token'])
        response = r.json()

        # Store initial server state and percent values
        vpn_state = response['state']

        # Keep polling the server's status until good
        while vpn_state not in self.good_states:

            # Wait 15 seconds before polling again
            time.sleep(interval)

            # Check server status again
            r = requests_retry_session().get(
                url, headers=self.specs['api_token'])
            response = r.json()

            # Update server state and percent values
            vpn_state = response['state']

            # Check for timeout
            seconds = (time.time() - start)
            duration = seconds / 60
            if duration > timeout:
                print('The operation timed out after %s minutes.' % timeout)
                return

        return {'duration': duration}


class BlockStorage(object):

    # Init Function
    def __init__(self, name=None, description=None, size=None,
                 datacenter_id=None, server_id=None, execution_group=None):

        self.name = name
        self.description = description
        self.size = size
        self.datacenter_id = datacenter_id
        self.server_id = server_id
        self.execution_group = execution_group

        self.specs = {}

        self.base_url = 'https://cloudpanel-api.1and1.com/v1'
        self.success_codes = (200, 201, 202)
        self.good_states = ('ACTIVE', 'ENABLED', 'POWERED_ON', 'POWERED_OFF')

    def __repr__(self):
        return ('Block Storage: name=%s, description=%s, size=%s, execution_group=%s, server_id=%s' % (
            self.name, self.description, self.size, self.datacenter_id, self.execution_group, self.server_id))

    def get(self):

        # Perform Request
        url = ('%s/block_storages/%s' %
               (self.base_url, self.specs['block_storage_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def server(self):

        # Perform Request
        url = ('%s/block_storages/%s/server' %
               (self.base_url, self.specs['block_storage_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def wait_for(self, timeout=25, interval=5):

        # Capture start time
        start = time.time()
        duration = 0

        # Check initial block storage status
        url = '%s/block_storages/%s' % (self.base_url,
                                        self.specs['block_storage_id'])

        r = requests_retry_session().get(url, headers=self.specs['api_token'])
        response = r.json()

        # Store initial block storage state and percent values
        block_storage_state = response['state']

        # Keep polling the block storage's status until good
        while block_storage_state not in self.good_states:

            # Wait 15 seconds before polling again
            time.sleep(interval)

            # Check block storage status again
            r = requests_retry_session().get(
                url, headers=self.specs['api_token'])
            response = r.json()

            # Update block storage state and percent values
            block_storage_state = response['state']

            # Check for timeout
            seconds = (time.time() - start)
            duration = seconds / 60
            if duration > timeout:
                print('The operation timed out after %s minutes.' % timeout)
                return

        return {'duration': duration}


class SshKey(object):

    # Init Function
    def __init__(self, name=None, description=None,
                 state=None, servers=None, md5=None,
                 public_key=None, creation_date=None):

        self.specs = {
            'name': name,
            'description': description,
            'state': state,
            'servers': servers,
            'md5': md5,
            'public_key': public_key,
            'creation_date': creation_date
        }

        self.base_url = 'https://cloudpanel-api.1and1.com/v1'
        self.success_codes = (200, 201, 202)
        self.good_states = ('ACTIVE', 'ENABLED', 'POWERED_ON', 'POWERED_OFF')

    def __repr__(self):
        return ('SshKey: name=%s, description=%s, '
                'state=%s, servers=%s, md5=%s, '
                'public_key=%s, creation_date=%s, ' %
                (self.specs['name'], self.specs['description'],
                 self.specs['state'], self.specs['servers'],
                 self.specs['md5'], self.specs['public_key'],
                 self.specs['creation_date']))

    def get(self):

        # Perform Request
        url = ('%s/ssh_keys/%s' %
               (self.base_url, self.specs['ssh_key_id']))

        r = requests_retry_session().get(url, headers=self.specs['api_token'])

        # Handle Potential Response Errors
        if r.status_code not in self.success_codes:
            error_message = ('Error Code: %s. Error Message: %s.' %
                             (r.status_code, r.text))
            raise Exception(error_message)

        return r.json()

    def wait_for(self, timeout=25, interval=5):

        # Capture start time
        start = time.time()
        duration = 0

        # Check initial ssh_key status
        url = '%s/ssh_keys/%s' % (self.base_url,
                                  self.specs['ssh_key_id'])

        r = requests_retry_session().get(url, headers=self.specs['api_token'])
        response = r.json()

        # Store initial ssh_key state and percent values
        ssh_key_state = response['state']

        # Keep polling the ssh_key's status until good
        while ssh_key_state not in self.good_states:

            # Wait 15 seconds before polling again
            time.sleep(interval)

            # Check ssh_key status again
            r = requests_retry_session().get(
                url, headers=self.specs['api_token'])
            response = r.json()

            # Update ssh_key state and percent values
            ssh_key_state = response['state']

            # Check for timeout
            seconds = (time.time() - start)
            duration = seconds / 60
            if duration > timeout:
                print('The operation timed out after %s minutes.' % timeout)
                return

        return {'duration': duration}

# This file is Copyright 2019 Volatility Foundation and licensed under the Volatility Software License 1.0
# which is available at https://www.volatilityfoundation.org/license/vsl-v1.0
#

import struct
from typing import Optional, Tuple

from volatility3.framework import exceptions, interfaces, constants
from volatility3.framework.layers import segmented


class LimeFormatException(exceptions.LayerException):
    """Thrown when an error occurs with the underlying Lime file format."""


class LimeLayer(segmented.SegmentedLayer):
    """A Lime format TranslationLayer.

    Lime is generally used to store physical memory images where there
    are large holes in the physical layer
    """

    MAGIC = 0x4c694d45
    VERSION = 1

    # Magic[4], Version[4], Start[8], End[8], Reserved[8]
    # XXX move this to a custom SymbolSpace?
    _header_struct = struct.Struct('<IIQQQ')

    def __init__(self, context: interfaces.context.ContextInterface, config_path: str, name: str) -> None:
        super().__init__(context, config_path, name)

        # The base class loads the segments on initialization, but otherwise this must to get the right min/max addresses

    def _load_segments(self) -> None:
        base_layer = self._context.layers[self._base_layer]
        base_maxaddr = base_layer.maximum_address
        maxaddr = 0
        offset = 0
        header_size = self._header_struct.size
        segments = []

        while offset < base_maxaddr:
            start, end = self._check_header(base_layer, offset)

            if start < maxaddr or end < start:
                raise LimeFormatException(
                    self.name, f"Bad start/end 0x{start:x}/0x{end:x} at file offset 0x{offset:x}")

            segment_length = end - start + 1
            segments.append((start, offset + header_size, segment_length, segment_length))
            maxaddr = end
            offset = offset + header_size + segment_length

        if len(segments) == 0:
            raise LimeFormatException(self.name, f"No LiME segments defined in {self._base_layer}")

        self._segments = segments

    @classmethod
    def _check_header(cls, base_layer: interfaces.layers.DataLayerInterface, offset: int = 0) -> Tuple[int, int]:
        try:
            header_data = base_layer.read(offset, cls._header_struct.size)
        except exceptions.InvalidAddressException:
            raise LimeFormatException(base_layer.name,
                                      f"Offset 0x{offset:0x} does not exist within the base layer")
        (magic, version, start, end, reserved) = cls._header_struct.unpack(header_data)
        if magic != cls.MAGIC:
            raise LimeFormatException(base_layer.name, f"Bad magic 0x{magic:x} at file offset 0x{offset:x}")
        if version != cls.VERSION:
            raise LimeFormatException(base_layer.name,
                                      f"Unexpected version {version:d} at file offset 0x{offset:x}")
        return start, end


class LimeStacker(interfaces.automagic.StackerLayerInterface):
    stack_order = 10

    @classmethod
    def stack(cls,
              context: interfaces.context.ContextInterface,
              layer_name: str,
              progress_callback: constants.ProgressCallback = None) -> Optional[interfaces.layers.DataLayerInterface]:
        try:
            LimeLayer._check_header(context.layers[layer_name])
        except LimeFormatException:
            return None
        new_name = context.layers.free_layer_name("LimeLayer")
        context.config[interfaces.configuration.path_join(new_name, "base_layer")] = layer_name
        return LimeLayer(context, new_name, new_name)

""" Provides named dictionary """
#-------------------------------------------------------------------------------
import collections

#-------------------------------------------------------------------------------
class NamedDict(collections.OrderedDict):
  """ A named dictionary is an ordered dictionary with a name """
  _name = None
  
#-------------------------------------------------------------------------------
  def __init__(self, name, *args, **kwds):
    self.name = name
    super().__init__(*args, **kwds)

#-------------------------------------------------------------------------------
  @property
  def name(self):
    return self._name

  @name.setter
  def name(self, name):
    assert isinstance(name, str), \
        "Name must be a string, not {}".format(type(name))
    self._name = name

#-------------------------------------------------------------------------------
  def __repr__(self):
    return "{}: {}".format(self.name, super().__repr__())

#-------------------------------------------------------------------------------

#! /usr/bin/env python
from __future__ import print_function

import pytest
import sys
import os
import subprocess


PYTEST_ARGS = {
    'default': ['tests'],
    'fast': ['tests', '-q'],
}

FLAKE8_ARGS = ['rest_framework_tracking', 'tests', '--ignore=E501']


sys.path.append(os.path.dirname(__file__))


def exit_on_failure(ret, message=None):
    if ret:
        sys.exit(ret)


def flake8_main(args):
    print('Running flake8 code linting')
    ret = subprocess.call(['flake8'] + args)
    print('flake8 failed' if ret else 'flake8 passed')
    return ret


def split_class_and_function(string):
    class_string, function_string = string.split('.', 1)
    return "%s and %s" % (class_string, function_string)


def is_function(string):
    # `True` if it looks like a test function is included in the string.
    return string.startswith('test_') or '.test_' in string


def is_class(string):
    # `True` if first character is uppercase - assume it's a class name.
    return string[0] == string[0].upper()


if __name__ == "__main__":
    try:
        sys.argv.remove('--nolint')
    except ValueError:
        run_flake8 = True
    else:
        run_flake8 = False

    try:
        sys.argv.remove('--lintonly')
    except ValueError:
        run_tests = True
    else:
        run_tests = False

    try:
        sys.argv.remove('--fast')
    except ValueError:
        style = 'default'
    else:
        style = 'fast'
        run_flake8 = False

    if len(sys.argv) > 1:
        pytest_args = sys.argv[1:]
        first_arg = pytest_args[0]
        if first_arg.startswith('-'):
            # `runtests.py [flags]`
            pytest_args = ['tests'] + pytest_args
        elif is_class(first_arg) and is_function(first_arg):
            # `runtests.py TestCase.test_function [flags]`
            expression = split_class_and_function(first_arg)
            pytest_args = ['tests', '-k', expression] + pytest_args[1:]
        elif is_class(first_arg) or is_function(first_arg):
            # `runtests.py TestCase [flags]`
            # `runtests.py test_function [flags]`
            pytest_args = ['tests', '-k', pytest_args[0]] + pytest_args[1:]
    else:
        pytest_args = PYTEST_ARGS[style]

    if run_tests:
        exit_on_failure(pytest.main(pytest_args))
        # ipdb support: comment the previous line and uncomment the nextone.
        # pytest.main(pytest_args + ['-s'])
    if run_flake8:
        exit_on_failure(flake8_main(FLAKE8_ARGS))

def raises(err, lamda):
    try:
        lamda()
        return False
    except err:
        return True


def expand_tuples(L):
    """

    >>> expand_tuples([1, (2, 3)])
    [(1, 2), (1, 3)]

    >>> expand_tuples([1, 2])
    [(1, 2)]
    """
    if not L:
        return [()]
    elif not isinstance(L[0], tuple):
        rest = expand_tuples(L[1:])
        return [(L[0],) + t for t in rest]
    else:
        rest = expand_tuples(L[1:])
        return [(item,) + t for t in rest for item in L[0]]


# Taken from theano/theano/gof/sched.py
# Avoids licensing issues because this was written by Matthew Rocklin
def _toposort(edges):
    """ Topological sort algorithm by Kahn [1] - O(nodes + vertices)

    inputs:
        edges - a dict of the form {a: {b, c}} where b and c depend on a
    outputs:
        L - an ordered list of nodes that satisfy the dependencies of edges

    >>> _toposort({1: (2, 3), 2: (3, )})
    [1, 2, 3]

    Closely follows the wikipedia page [2]

    [1] Kahn, Arthur B. (1962), "Topological sorting of large networks",
    Communications of the ACM
    [2] http://en.wikipedia.org/wiki/Toposort#Algorithms
    """
    incoming_edges = reverse_dict(edges)
    incoming_edges = dict((k, set(val)) for k, val in incoming_edges.items())
    S = set((v for v in edges if v not in incoming_edges))
    L = []

    while S:
        n = S.pop()
        L.append(n)
        for m in edges.get(n, ()):
            assert n in incoming_edges[m]
            incoming_edges[m].remove(n)
            if not incoming_edges[m]:
                S.add(m)
    if any(incoming_edges.get(v, None) for v in edges):
        raise ValueError("Input has cycles")
    return L


def reverse_dict(d):
    """Reverses direction of dependence dict

    >>> d = {'a': (1, 2), 'b': (2, 3), 'c':()}
    >>> reverse_dict(d)  # doctest: +SKIP
    {1: ('a',), 2: ('a', 'b'), 3: ('b',)}

    :note: dict order are not deterministic. As we iterate on the
        input dict, it make the output of this function depend on the
        dict order. So this function output order should be considered
        as undeterministic.

    """
    result = {}
    for key in d:
        for val in d[key]:
            result[val] = result.get(val, tuple()) + (key, )
    return result


# Taken from toolz
# Avoids licensing issues because this version was authored by Matthew Rocklin
def groupby(func, seq):
    """ Group a collection by a key function

    >>> names = ['Alice', 'Bob', 'Charlie', 'Dan', 'Edith', 'Frank']
    >>> groupby(len, names)  # doctest: +SKIP
    {3: ['Bob', 'Dan'], 5: ['Alice', 'Edith', 'Frank'], 7: ['Charlie']}

    >>> iseven = lambda x: x % 2 == 0
    >>> groupby(iseven, [1, 2, 3, 4, 5, 6, 7, 8])  # doctest: +SKIP
    {False: [1, 3, 5, 7], True: [2, 4, 6, 8]}

    See Also:
        ``countby``
    """

    d = dict()
    for item in seq:
        key = func(item)
        if key not in d:
            d[key] = list()
        d[key].append(item)
    return d

#
# This file is part of pyasn1-modules software.
#
# Created by Russ Housley.
#
# Copyright (c) 2019, Vigil Security, LLC
# License: http://snmplabs.com/pyasn1/license.html
#
# Diffie-Hellman Proof-of-Possession Algorithms
#
# ASN.1 source from:
# https://www.rfc-editor.org/rfc/rfc6955.txt
#

from pyasn1.type import namedtype
from pyasn1.type import univ

from pyasn1_modules import rfc3279
from pyasn1_modules import rfc5280
from pyasn1_modules import rfc5652


# Imports from RFC 5652

MessageDigest = rfc5652.MessageDigest

IssuerAndSerialNumber = rfc5652.IssuerAndSerialNumber


# Imports from RFC 5280

id_pkix = rfc5280.id_pkix


# Imports from RFC 3279

Dss_Sig_Value = rfc3279.Dss_Sig_Value

DomainParameters = rfc3279.DomainParameters


# Static DH Proof-of-Possession

class DhSigStatic(univ.Sequence):
    componentType = namedtype.NamedTypes(
        namedtype.OptionalNamedType('issuerAndSerial', IssuerAndSerialNumber()),
        namedtype.NamedType('hashValue', MessageDigest())
    )


# Object Identifiers

id_dh_sig_hmac_sha1 = id_pkix + (6, 3, )

id_dhPop_static_sha1_hmac_sha1 = univ.ObjectIdentifier(id_dh_sig_hmac_sha1)


id_alg_dh_pop = id_pkix + (6, 4, )

id_alg_dhPop_sha1 = univ.ObjectIdentifier(id_alg_dh_pop)

id_alg_dhPop_sha224 = id_pkix + (6, 5, )

id_alg_dhPop_sha256 = id_pkix + (6, 6, )

id_alg_dhPop_sha384 = id_pkix + (6, 7, )

id_alg_dhPop_sha512 = id_pkix + (6, 8, )


id_alg_dhPop_static_sha224_hmac_sha224 = id_pkix + (6, 15, )

id_alg_dhPop_static_sha256_hmac_sha256 = id_pkix + (6, 16, )

id_alg_dhPop_static_sha384_hmac_sha384 = id_pkix + (6, 17, )

id_alg_dhPop_static_sha512_hmac_sha512 = id_pkix + (6, 18, )


id_alg_ecdhPop_static_sha224_hmac_sha224 = id_pkix + (6, 25, )

id_alg_ecdhPop_static_sha256_hmac_sha256 = id_pkix + (6, 26, )

id_alg_ecdhPop_static_sha384_hmac_sha384 = id_pkix + (6, 27, )

id_alg_ecdhPop_static_sha512_hmac_sha512 = id_pkix + (6, 28, )


# Update the Algorithm Identifier map in rfc5280.py

_algorithmIdentifierMapUpdate = {
    id_alg_dh_pop: DomainParameters(),
    id_alg_dhPop_sha224: DomainParameters(),
    id_alg_dhPop_sha256: DomainParameters(),
    id_alg_dhPop_sha384: DomainParameters(),
    id_alg_dhPop_sha512: DomainParameters(),
    id_dh_sig_hmac_sha1: univ.Null(""),
    id_alg_dhPop_static_sha224_hmac_sha224: univ.Null(""),
    id_alg_dhPop_static_sha256_hmac_sha256: univ.Null(""),
    id_alg_dhPop_static_sha384_hmac_sha384: univ.Null(""),
    id_alg_dhPop_static_sha512_hmac_sha512: univ.Null(""),
    id_alg_ecdhPop_static_sha224_hmac_sha224: univ.Null(""),
    id_alg_ecdhPop_static_sha256_hmac_sha256: univ.Null(""),
    id_alg_ecdhPop_static_sha384_hmac_sha384: univ.Null(""),
    id_alg_ecdhPop_static_sha512_hmac_sha512: univ.Null(""),
}

rfc5280.algorithmIdentifierMap.update(_algorithmIdentifierMapUpdate)

# -*- coding: utf-8 -*-
#
# Configuration file for the Sphinx documentation builder.
#
# This file does only contain a selection of the most common options. For a
# full list see the documentation:
# http://www.sphinx-doc.org/en/stable/config

# -- Path setup --------------------------------------------------------------

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#
# import os
# import sys
# sys.path.insert(0, os.path.abspath('.'))


# -- Project information -----------------------------------------------------

project = 'analyze pairing and clustering of molecular systems'
copyright = "2018, Matthew W. Thompson"
author = 'Matthew W. Thompson'

# The short X.Y version
version = ''
# The full version, including alpha/beta/rc tags
release = ''


# -- General configuration ---------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#
# needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = [
    'sphinx.ext.autodoc',
    'sphinx.ext.mathjax',
]

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix(es) of source filenames.
# You can specify multiple suffix as a list of string:
#
# source_suffix = ['.rst', '.md']
source_suffix = '.rst'

# The master toctree document.
master_doc = 'index'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#
# This is also used if you do content translation via gettext catalogs.
# Usually you set "language" from the command line for these cases.
language = None

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
# This pattern also affects html_static_path and html_extra_path .
exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'


# -- Options for HTML output -------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
#
html_theme = 'sphinx_rtd_theme'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#
# html_theme_options = {}

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# Custom sidebar templates, must be a dictionary that maps document names
# to template names.
#
# The default sidebars (for documents that don't match any pattern) are
# defined by theme itself.  Builtin themes are using these templates by
# default: ``['localtoc.html', 'relations.html', 'sourcelink.html',
# 'searchbox.html']``.
#
# html_sidebars = {}


# -- Options for HTMLHelp output ---------------------------------------------

# Output file base name for HTML help builder.
htmlhelp_basename = 'pairingdoc'


# -- Options for LaTeX output ------------------------------------------------

latex_elements = {
    # The paper size ('letterpaper' or 'a4paper').
    #
    # 'papersize': 'letterpaper',

    # The font size ('10pt', '11pt' or '12pt').
    #
    # 'pointsize': '10pt',

    # Additional stuff for the LaTeX preamble.
    #
    # 'preamble': '',

    # Latex figure (float) alignment
    #
    # 'figure_align': 'htbp',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title,
#  author, documentclass [howto, manual, or own class]).
latex_documents = [
    (master_doc, 'pairing.tex', 'pairing Documentation',
     'pairing', 'manual'),
]


# -- Options for manual page output ------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    (master_doc, 'pairing', 'pairing Documentation',
     [author], 1)
]


# -- Options for Texinfo output ----------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
    (master_doc, 'pairing', 'pairing Documentation',
     author, 'pairing', 'analyze pairing and clustering of molecular systems',
     'Miscellaneous'),
]


# -- Extension configuration -------------------------------------------------

#! /usr/bin/env python
# for fgas ppruns only
ppruns=[ "2009-03-28_W-S-I+" ,"2009-09-19_W-J-V" ,"2009-04-29_W-S-Z+" ,"2009-04-29_W-J-B" ,"2010-03-12_W-J-B" ,"2010-03-12_W-S-Z+" ,"2010-03-12_W-C-RC" ,"2010-11-04_W-J-B" ,"2010-11-04_W-S-Z+" ,"2012-07-23_W-C-RC" ,"2013-06-10_W-S-Z+" ,"2007-02-13_W-S-I+" ,"2007-02-13_W-J-V" ,"2010-03-12_W-J-V" ,"2010-03-12_W-S-I+" ,"2010-12-05_W-J-V" ,"2015-12-15_W-J-B" ,"2015-12-15_W-C-RC" ,"2015-12-15_W-S-Z+"]
ppruns_MACS1115=[ "2009-04-29_W-S-Z+" ,"2009-04-29_W-J-B" ,"2010-03-12_W-J-B" ,"2010-03-12_W-S-Z+" ,"2010-03-12_W-C-RC"]
ppruns_preH=[ "2010-03-12_W-S-I+" ,"2010-12-05_W-J-V" ,"2007-02-13_W-J-V" ,"2007-02-13_W-S-I+" ,"2009-03-28_W-S-I+" ,"2010-03-12_W-J-V"]
ppruns_10_3= ['2009-03-28_W-S-I+', '2009-09-19_W-J-V', '2009-04-29_W-S-Z+', '2009-04-29_W-J-B', '2010-03-12_W-J-B', '2010-03-12_W-S-Z+', '2010-03-12_W-C-RC', '2010-11-04_W-J-B', '2010-11-04_W-S-Z+', '2012-07-23_W-C-RC', '2013-06-10_W-S-Z+', '2010-03-12_W-J-V', '2010-03-12_W-S-I+', '2010-12-05_W-J-V', '2015-12-15_W-J-B', '2015-12-15_W-C-RC', '2015-12-15_W-S-Z+']
ppruns_10_2= ['2007-02-13_W-S-I+', '2007-02-13_W-J-V']
ppruns_postH= ['2009-09-19_W-J-V', '2009-04-29_W-S-Z+', '2009-04-29_W-J-B', '2010-03-12_W-J-B', '2010-03-12_W-S-Z+', '2010-03-12_W-C-RC', '2010-11-04_W-J-B', '2010-11-04_W-S-Z+', '2012-07-23_W-C-RC', '2013-06-10_W-S-Z+', '2015-12-15_W-J-B', '2015-12-15_W-C-RC', '2015-12-15_W-S-Z+']
ppruns_nonMACS1115= ['2009-03-28_W-S-I+', '2009-09-19_W-J-V', '2010-11-04_W-J-B', '2010-11-04_W-S-Z+', '2012-07-23_W-C-RC', '2013-06-10_W-S-Z+', '2007-02-13_W-S-I+', '2007-02-13_W-J-V', '2010-03-12_W-J-V', '2010-03-12_W-S-I+', '2010-12-05_W-J-V', '2015-12-15_W-J-B', '2015-12-15_W-C-RC', '2015-12-15_W-S-Z+']
ppruns_dec= ['2009-03-28_W-S-I+', '2009-09-19_W-J-V', '2010-11-04_W-J-B', '2010-11-04_W-S-Z+', '2012-07-23_W-C-RC', '2013-06-10_W-S-Z+', '2010-03-12_W-J-V', '2010-03-12_W-S-I+', '2010-12-05_W-J-V', '2015-12-15_W-J-B', '2015-12-15_W-C-RC', '2015-12-15_W-S-Z+']

ic_cldata={'MACS0429-02':{},'MACS1226+21':{},'RXJ2129':{},'MACS1115+01':{},"MACS0416-24":{},'MACS0159-08':{}, 'Zw2089':{}, 'Zw2701':{}, 'A2204':{}}
ic_cldata['MACS1226+21']['FILTERs']=["W-J-B","W-J-V","W-C-RC","W-C-IC","W-S-Z+"]
ic_cldata['MACS1226+21']['PPRUNs']=["W-C-IC_2010-02-12", "W-C-IC_2011-01-06","W-C-RC_2010-02-12", "W-J-B_2010-02-12", "W-J-V_2010-02-12", "W-S-Z+_2011-01-06"]
ic_cldata['MACS1226+21']['FILTERs_matching_PPRUNs']=["W-C-IC", "W-C-IC","W-C-RC", "W-J-B", "W-J-V", "W-S-Z+"] 
ic_cldata['MACS0429-02']['FILTERs']=["W-J-B","W-S-Z+"]
ic_cldata['MACS0429-02']['FILTERs_matching_PPRUNs']=["W-J-B","W-S-Z+"]
ic_cldata['MACS0429-02']['PPRUNs']=["W-J-B_2015-12-15","W-S-Z+_2015-12-15"]
ic_cldata['RXJ2129']['FILTERs']=["W-J-B","W-C-RC","W-S-Z+"]
ic_cldata['RXJ2129']['FILTERs_matching_PPRUNs']=["W-J-B","W-C-RC","W-S-Z+"]
ic_cldata['RXJ2129']['PPRUNs']=["W-J-B_2010-11-04","W-C-RC_2012-07-23","W-S-Z+_2010-11-04"]
ic_cldata['MACS1115+01']['FILTERs']=["W-J-B","W-C-RC","W-S-Z+"] 
ic_cldata['MACS1115+01']['FILTERs_matching_PPRUNs']=["W-S-Z+","W-J-B","W-J-B","W-S-Z+","W-C-RC"]
ic_cldata['MACS1115+01']['PPRUNs']=["W-S-Z+_2009-04-29","W-J-B_2009-04-29","W-J-B_2010-03-12","W-S-Z+_2010-03-12","W-C-RC_2010-03-12"]
ic_cldata['MACS0416-24']['FILTERs']=["W-J-B","W-C-RC","W-S-Z+"] 
ic_cldata['MACS0416-24']['FILTERs_matching_PPRUNs']=["W-J-B","W-C-RC","W-S-Z+"] 
ic_cldata['MACS0416-24']['PPRUNs']=["W-J-B_2010-11-04","W-C-RC_2010-11-04","W-S-Z+_2010-11-04"] 
ic_cldata['MACS0159-08']['FILTERs']=["W-J-B"]
ic_cldata['MACS0159-08']['FILTERs_matching_PPRUNs']=["W-J-B"]
ic_cldata['MACS0159-08']['PPRUNs']=["W-J-B_2015-12-15"]
ic_cldata['Zw2089']['FILTERs']=['W-J-B','W-J-V','W-C-RC','W-S-I+','W-S-Z+']
ic_cldata['Zw2089']['FILTERs_matching_PPRUNs']=['W-C-RC','W-J-B','W-J-V','W-S-I+','W-S-I+','W-S-Z+']
ic_cldata['Zw2089']['PPRUNs']=['W-C-RC_2015-12-15','W-J-B_2015-12-15','W-J-V_2010-12-05','W-S-I+_2007-02-13','W-S-I+_2009-03-28','W-S-Z+_2015-12-15']
ic_cldata['Zw2701']['FILTERs']=['W-J-B','W-J-V','W-C-RC','W-S-I+']
ic_cldata['Zw2701']['FILTERs_matching_PPRUNs']=['W-C-RC','W-J-B','W-J-V','W-J-V','W-S-I+']
ic_cldata['Zw2701']['PPRUNs']=['W-C-RC_2015-12-15','W-J-B_2015-12-15','W-J-V_2010-03-12','W-J-V_2010-12-05','W-S-I+_2010-03-12']
ic_cldata['A2204']['FILTERs']=['W-J-V','W-S-I+']
ic_cldata['A2204']['FILTERs_matching_PPRUNs']=['W-J-V','W-S-I+']
ic_cldata['A2204']['PPRUNs']=['W-J-V_2009-09-19','W-S-I+_2009-03-28']
ra_cluster={}
dec_cluster={}
ra_cluster['MACS0429-02']=67.40041667 ; dec_cluster['MACS0429-02']=-2.88555556
ra_cluster['RXJ2129']=322.41625000 ; dec_cluster['RXJ2129']=0.08888889
ra_cluster['MACS1226+21']=186.71268; dec_cluster['MACS1226+21']=21.831938
ra_cluster['A2204']=248.19666667; dec_cluster['A2204']=5.57555556
ra_cluster['MACS0159-08']=29.9579;dec_cluster['MACS0159-08']=-8.83028
#MACSJ0429.6-0253	67.4	-2.88375
#RXJ2129.6+0005	322.408	0.094
ra_cluster['Zw2089']=135.158;dec_cluster['Z2089']=20.916
ra_cluster['Zw2701']=148.198;dec_cluster['Z2701']=51.891
ra_cluster['MACS1115+01']=168.972;dec_cluster['MACS1115+01']=1.49639
ra_cluster['MACS0416-24']=64.0413;dec_cluster['MACS0416-24']=6-24.0662


clusters =['MACS0429-02','MACS1226+21','RXJ2129','MACS1115+01',"MACS0416-24",'MACS0159-08', 'Zw2089', 'Zw2701', 'A2204']
fgas_clusters =['MACS0429-02','RXJ2129','MACS1115+01','MACS0159-08', 'Zw2089', 'Zw2701', 'A2204']
active_fgas_clusters =['Zw2089','MACS0429-02','RXJ2129', 'MACS1115+01', 'A2204']
clusters_refcats={}
clusters_refcats['MACS0429-02']='PANSTARRS'
clusters_refcats['RXJ2129']='SDSS'
clusters_refcats['MACS1226+21']='SDSS'
clusters_refcats['A2204']='PANSTARRS'
clusters_refcats['MACS0159-08']='SDSS'
clusters_refcats['Zw2089']='SDSS'
clusters_refcats['Zw2701']='SDSS'
clusters_refcats['MACS1115+01']='SDSS'
clusters_refcats['MACS0416-24']='PANSTARRS'

stuff_todo=['Coadding (1 day)',
'By-hand masking (1 day)',
'Coadd Masking (1/3 day)',
'Photometric Measurement and Calibration',
'Photo-Zs', 'cc masses', 'p(z) masses']

for cl in active_fgas_clusters:
	for item in stuff_todo:
		print ":".join([item,cl])

mask_todo=['coadd init','standard by-hand masking','backmasking','autosuppression','asteroid','star (ALL BANDS)','edgemask (ALL BANDS)','check background-sub errors (ALL BANDS)','coadd final']
for cl in active_fgas_clusters:
	for item in  mask_todo:
		print ":".join([item,cl])

# bottom-up dp: traverses iteratively starting from the smallest subset (bottom) going up
# ex: fib(1), fib(2), fib(3), fib(4), ... , fib(n)
def knapsack_bottom_up_dp(weights, values, W):
    # generating array for storing optimal values
    n = len(weights)
    opt_vals = [[0 for _ in range(W + 1)] for _ in range(n + 1)]
    
    # computing possible optimal values
    for i in range(1, n + 1):
        for w in range(1, W + 1):
            wi = weights[i - 1]
            # if weight of the current item is greater than the current weight
            # take the previous optimal value from previous top slot (i - 1)
            if wi > w:
                opt_vals[i][w] = opt_vals[i - 1][w]
            # otherwise, take the maximum between:
            # putting the current item into the knapsack or not
            else:
                opt_vals[i][w] = max(values[i - 1] + opt_vals[i - 1][w - wi], 
                                     opt_vals[i - 1][w])

    # backtracking
    opt_subset = backtrack(n, W, weights, values, opt_vals)

    # for i in opt_vals: print(i)
    return (opt_vals[-1][-1], opt_subset)

# top-down: recursively computes values starting from the biggest (top) going down
# ex: fib(n), fib(n-1), fib(n-2), ... , fib(1)
def knapsack_top_down_dp(weights, values, W):
    # generating array for storing optimal values with 0 in edges and -1 elsewhere
    n = len(weights)
    opt_vals = [[0 for _ in range(W + 1)]]
    [opt_vals.append([0 if j == 0 else -1 for j in range(W + 1)]) for _ in range(n)]

    # run recursion
    max_val = helper(weights, values, opt_vals, n, W)

    # backtracking
    opt_subset = backtrack(n, W, weights, values, opt_vals)

    # for i in opt_vals: print(i)
    return (max_val, opt_subset)
  
def helper(weights, values, opt_vals, i, w):
    # base case
    if opt_vals[i][w] >= 0:
        return opt_vals[i][w]

    # skip the item if the wieght of item is bigger than the remaining weight in the knapsack
    if weights[i - 1] > w :
        max_val = helper(weights, values, opt_vals, i - 1, w)
    # otherwise, recursively compute maximum between picking the item or not picking the item
    else:
        max_val = max(values[i - 1] + helper(weights, values, opt_vals, i - 1, w - weights[i - 1]),
                      helper(weights, values, opt_vals, i - 1, w))
    
    # memorize the computed maximum value
    opt_vals[i][w] = max_val
    return max_val

def backtrack(n, W, weights, values, opt_vals):
    opt_subset = [0 for i in range(n)]
    i, w = n, W
    while i > 0 and w > 0:
        wi = weights[i - 1]
        if w - wi >= 0 and opt_vals[i][w] == values[i - 1] + opt_vals[i - 1][w - wi]:
            opt_subset[i - 1] = 1
            w -= wi
        i -= 1 
    return opt_subset

# brute force: generate all possible 2^n variants and determine the maximum optimal value
# brute force: without bit manipulation
import itertools
def knapsack_brute_force(weights, values, W):
    # initializing length, maximum total value and optimal subset of selected items
    n, max_val, opt_subset = len(weights), 0, []

    # creating a generator, that traveses through all possible combinations of selecting n items
    combinations = map(list, itertools.product([0, 1], repeat=n))

    # iterating over all combinations
    for cmb in combinations:
        # calcualting total weight and total value for current combination
        tot_weights = sum([a*b for a,b in zip(weights, cmb)])
        tot_values = sum([a*b for a,b in zip(values, cmb)])

        # updating maximum total value and optimal subset to current 
        if tot_weights <= W and tot_values > max_val:
            max_val = tot_values
            opt_subset = cmb
    
    return (max_val, opt_subset)

# brute force: with bit manipulation
def knapsack_brute_force_bm(weights, values, W):
    # initializing length, maximum total value and optimal subset of selected items
    n, max_val = len(weights), 0
    opt_subset = [0]*n
    i, m = 1, 2**n

    # iterating over all combinations
    while i < m:
        j, tot_weights, tot_values, cur = i, 0, 0, 0
        cur_subset = [0]*n
        while j:
            if j & 1:
                tot_weights += weights[cur]
                tot_values += values[cur]
                cur_subset[cur] = 1
            j >>= 1
            cur += 1
        i+=1
        
        # updating maximum total value and optimal subset to current 
        if tot_weights <= W and tot_values > max_val:
            max_val = tot_values
            opt_subset = cur_subset
    
    return (max_val, opt_subset)

# correctness testing
def corr_test():
    functions = [
        ("BOTTOM UP:", knapsack_bottom_up_dp), 
        ("TOP DOWN:", knapsack_top_down_dp),
        ("BRUTE FORCE:", knapsack_brute_force),
        ("BRUTE FORCE (bit manip.):", knapsack_brute_force_bm)
    ]

    # source of the test cases: http://people.sc.fsu.edu/~jburkardt/datasets/knapsack_01/knapsack_01.html
    test_cases = [
        [([2,2,3], [2,3,4], 6), [0, 1, 1]],
        [([2,2,3], [7,2,1], 6), [1, 1, 0]],
        [([23,31,29,44,53,38,63,85,89,82], [92,57,49,68,60,43,67,84,87,72], 165), [1,1,1,1,0,1,0,0,0,0]],
        [([12,7,11,8,9], [24,13,23,15,16], 26), [0,1,1,1,0]],
        [([56,59,80,64,75,17], [50,50,64,46,50,5], 190), [1,1,0,0,1,0]],
        [([31,10,20,19,4,3,6], [70,20,39,37,7,5,10], 50), [1,0,0,1,0,0,0]],
        [([25,35,45,5,25,3,2,2], [350,400,450,20,70,8,5,5], 104), [1,0,1,1,1,0,1,1]],
        [([41,50,49,59,55,57,60], [442,525,511,593,546,564,617], 170), [0,1,0,1,0,0,1]]
    ]
    for fn in functions:
        for tc in test_cases:
            max_val, opt_subset = fn[1](*tc[0])
            is_correct = opt_subset == tc[1]
            print(fn[0], max_val)
            print("Correct:", is_correct)
            print("Output:", opt_subset)
            print("Answer:", tc[1])

import random
import time
import numpy as np
import pandas as pd

def main():
    # Brute force vs. DP bottom-up vs. DP top-down
    test(*get_inputs_BF_vs_DPbu_vs_DPtd())

    # DP bottom-up vs. DP top-down
    test(*get_inputs_DPbu_vs_DPtd())
    
# generate inputs for testing DP bottom-up vs. DP top-down
def get_inputs_BF_vs_DPbu_vs_DPtd():
    # N = np.arange(1, 26, 1)         #[1, 2, ..., 25]
    # K = np.arange(0.2, 1.1, 0.2)    #[0.1, 0.2, ..., 1]
    # wi_vi_pow = np.arange(3, 10, 2)  #[3, 5, 7, 9]
    N = np.arange(1, 3, 1)         #[1, 2, ..., 25]
    K = np.arange(0.2, 0.3, 0.2)    #[0.1, 0.2, ..., 1]
    wi_vi_pow = np.arange(3, 4, 2)  #[3, 5, 7, 9]
    algorithms = [
        ("Brute force", knapsack_brute_force), 
        ("DP bottom-up",knapsack_bottom_up_dp),
        ("DP top-down", knapsack_top_down_dp)
    ]
    filename = "DP bottom-up vs. DP top-down"
    return (N, K, wi_vi_pow, algorithms, filename)

# generate inputs for testing DP bottom-up vs. DP top-down
def get_inputs_DPbu_vs_DPtd():
    # N = [2**i for i in range(0,32)]
    # K = np.arange(0.2, 1.1, 0.2)
    # wi_vi_pow = np.arange(3, 10, 2)
    N = [2**i for i in range(0,2)]
    K = np.arange(0.2, 0.3, 0.2)
    wi_vi_pow = np.arange(3, 4, 2)
    algorithms = [
        ("DP bottom-up", knapsack_bottom_up_dp),
        ("DP top-down", knapsack_top_down_dp)
    ]
    filename = "DP bottom-up vs. DP top-down"
    return (N, K, wi_vi_pow, algorithms, filename)
    
# full testing
def test(N, K, wi_vi_pow, algorithms, filename):
    # arrays to store columns of the output table
    runtimes = [[] for _ in algorithms]
    n_arr = []
    W_arr = []
    wi_vi_range_arr = []

    # run over all combinations of the inputs
    # different number of input items 
    for ni in N:
        # different range of weights and values (ni = n) => 1,2,3,...,n
        for wi_vi in wi_vi_pow:
            # generate weights and values and compute sum of weights 
            # (range = (1, 10^wi_vi)) => (1, 10^3),...,(1, 10^m)
            weights = np.random.randint(10**wi_vi, size=ni)
            values = np.random.randint(10**wi_vi, size=ni)
            sum_weights = sum(weights)
            # different capacity of the knapsack 
            # (W = sum(weights) * ki) => W*1,W*0.8,...,W*0.2
            for ki in K:
                # add inputs values into the table columns           
                n_arr.append(ni)
                W_arr.append(int(sum_weights * ki))
                wi_vi_range_arr.append(10**wi_vi)
                # run algorithms and time performance
                print("Inputs: n={}, W={}".format(ni, W_arr[-1]))
                for i in range(len(algorithms)):
                    print("Running: {} with wi_vi_range: 1-{}".format(algorithms[i][0], wi_vi_range_arr[-1]))
                    start = time.clock()
                    algorithms[i][1](weights, values, int(sum_weights * ki))
                    end = time.clock()
                    runtimes[i].append(end - start)
        # save table as csv            
        save(runtimes, algorithms, n_arr, W_arr, wi_vi_range_arr, filename)
    # save table as csv    
    save(runtimes, algorithms, n_arr, W_arr, wi_vi_range_arr, filename)                

# save table as csv
def save(runtimes, algorithms, n_arr, W_arr, wi_vi_range_arr, filename):
    total_runtime = sum([sum(rn) for rn in runtimes])
    print("Saving to csv\nTotal runtime: {}".format(total_runtime))
    df_algorithms = pd.concat([pd.Series(rn) for rn in runtimes], keys=[alg[0] for alg in algorithms], axis=1)
    df_inputs = pd.DataFrame({"n": n_arr, "W": W_arr, "wi, vi range": wi_vi_range_arr})
    df_concat = pd.concat([df_algorithms, df_inputs], axis = 1)
    df_concat.to_csv(filename+".csv")

if __name__ == "__main__":
    main()
"""
The following program leverages the regression coefficients generated after training the model in task 2 as an input file
"""
from __future__ import print_function
import sys
import re
from operator import add
import numpy as np 
from pyspark import SparkContext

if __name__ == "__main__":

    sc = SparkContext(appName="LogisticRegression_task3")
    
    # Read the dataset 
    d_corpus = sc.textFile(sys.argv[1])
    
    # Each entry in validLines will be a line from the text file
    validDocLines = d_corpus.filter(lambda x : 'id' in x and 'url=' in x)

    # Now, we transform it into a set of (docID, text) pairs
    keyAndText = validDocLines.map(lambda x : (x[x.index('id="') + 4 : x.index('" url=')], x[x.index('">') + 2:][:-6])) 

    # leveraged the code from assignment 2
    # remove all non letter characters
    regex = re.compile('[^a-zA-Z]')
    keyAndWordsList = keyAndText.map(lambda x : (str(x[0]), regex.sub(' ', x[1]).lower().split()))
    
    # Now get the top 20,000 words... first change (docID, ["word1", "word2", "word3", ...])
    # to ("word1", 1) ("word2", 1)...
    conslidatedWords = keyAndWordsList.flatMap(lambda x: x[1]).map(lambda x: (x,1))

    # Now, count all of the words, giving us ("word1", 1433), ("word2", 3423423), etc.
    allCounts = conslidatedWords.reduceByKey(add)

    # Get the top 20,000 words in a local array in a sorted format based on frequency
    topWordsinDict = allCounts.top(20000, key = lambda x : x[1])

    # We'll create a RDD that has a set of (word, dictNum) pairs
    # start by creating an RDD that has the number 0 through 20000
    # 20000 is the number of words that will be in our dictionary
    top20000Words = sc.parallelize(range(20000))

    # Now, we transform (0), (1), (2), ... to ("MostCommonWord", 1)
    # ("NextMostCommon", 2), ...
    # the number will be the spot in the dictionary used to tell us
    # where the word is located
    dictionary = top20000Words.map (lambda x : (topWordsinDict[x][0], x))
    
    
    # The following function gets a list of dictionaryPos values,
    # and then creates a TF vector
    # corresponding to those values... for example,
    # if we get [3, 4, 1, 1, 2] we would in the
    # end have [0, 2/5, 1/5, 1/5, 1/5] because 0 appears zero times,
    # 1 appears twice, 2 appears once, etc.

    def buildArray(listOfIndices):
        
        returnVal = np.zeros(20000)
        
        for index in listOfIndices:
            returnVal[index] = returnVal[index] + 1
        
        mysum = np.sum(returnVal)
        
        returnVal = np.divide(returnVal, mysum)
        
        return returnVal
        
    def getLabel(x):
      if x[:2] == 'AU':
        return 1

      else:
        return 0
    
    
    # Leverage the regression coefficients genereated by task2 (model training) to make the prediction
    #filePathOutputTask2 = sys.argv[2]
    
    # Open the file containing regression coefficients and read it
    
    #task2Lines = filePathOutputTask2.map(lambda x: x.split("Final Regression Coeffients:\n["))
    filePathOutputTask2 =sc.textFile(sys.argv[2])
    
    #filePathOutputTask2.map(lambda x: )
    #with open(filePath) as file:
     #   allLines = file.read()
     
    # Extract out all of the lines present in the output of task 2
    task2Lines = filePathOutputTask2.map(lambda x: x.split(","))
    
    # Extract the line containing the regression coefficients and remove '[' and ']' from the extremes
    listOfLines = task2Lines.collect()[10]
    listOfLines[0] = listOfLines[0][1:]
    listOfLines[len(listOfLines)-1] = listOfLines[len(listOfLines)-1][:len(listOfLines[len(listOfLines)-1])-2]

    # Convert the list of regression coefficients to numpy array to be used as an input for prediction in task 3
    regressionCoefficients = np.array(listOfLines, dtype = np.float64 )
        
    # Split the file and extract the 'Regression Coefficients'    
    #listOfLines = allLines.split("Final Regression Coeffients:\n[")
    #listOfLines[len(listOfLines)-1] = listOfLines[len(listOfLines)-1][:len(listOfLines[len(listOfLines)-1])-2]
    #regressionCoefficients = np.array(listOfLines[1].split(','), dtype = np.float64 )
    
    # Threshold for logistic regression
    threshold = 0.3

    # Prediction Function using logistic regression
    def predictionLogisticRegresison(x):
      value = 1/(1+np.exp(-(  np.dot( x, regressionCoefficients )  )  )) 
      # return value
      if value >= threshold:
        return 1
      else:
        return 0
        
    ###################################################### PREDICTION/ EVALUATION - TAKS 3 ########
    # Read the dataset 
    testData = sc.textFile(sys.argv[3])

    # Each entry in validLines will be a line from the text file
    validDocLinesTest = testData.filter(lambda x : 'id' in x and 'url=' in x)

    # Now, we transform it into a set of (docID, text) pairs
    keyAndTextTest = validDocLinesTest.map(lambda x : (x[x.index('id="') + 4 : x.index('" url=')], x[x.index('">') + 2:][:-6])) 

    # remove all non letter characters
    keyAndWordsListTest = keyAndTextTest.map(lambda x : (str(x[0]), regex.sub(' ', x[1]).lower().split()))

    # Get a RDD that has, for each (docID, ["word1", "word2", "word3", ...]),
    # ("word1", docID), ("word2", docId), ...
    allWordsWithDocIDTest = keyAndWordsListTest.flatMap(lambda x: ((j, x[0]) for j in x[1]))

    # Join and link them, to get a set of ("word1", (dictionaryPos, docID)) pairs
    allDictionaryWordsTest = dictionary.join(allWordsWithDocIDTest)

    # Drop the actual word itself to get a set of (docID, dictionaryPos) pairs
    justDocAndPosTest = allDictionaryWordsTest.map(lambda x: (x[1][1],x[1][0]))

    # Get a set of (docID, [dictionaryPos1, dictionaryPos2, dictionaryPos3...]) pairs
    allDictionaryWordsInEachDocTest = justDocAndPosTest.groupByKey()

    # The following line this gets us a set of
    # (docID,  [dictionaryPos1, dictionaryPos2, dictionaryPos3...]) pairs
    # and converts the dictionary positions to a bag-of-words numpy array...
    allDocsAsNumpyArraysTest = allDictionaryWordsInEachDocTest.map(lambda x: (x[0], buildArray(x[1])))

    # Now, create a version of allDocsAsNumpyArrays where, in the array,
    # every entry is either zero or one.
    # A zero means that the word does not occur,
    # and a one means that it does.
    zeroOrOneTest = allDocsAsNumpyArraysTest.map(lambda x: (x[0],np.where(x[1] > 0, 1, 0)))
    
    # Create a RDD of testing data and derive features and labels ... x[0]-> label, x[1]-> features
    yLabelAndXFeatures = zeroOrOneTest.map(lambda x: (getLabel(x[0]),x[1]))
    
    # Make the prediction using the function 'predictionLogisticRegresison'
    yLabelAndXFeaturesPrediction = yLabelAndXFeatures.map(lambda x: (x[0],x[1],predictionLogisticRegresison(x[1])))

    # Function to calculate True Positives
    def calculateTruePositives(x):
      if (x[0] == 1 and x[2] == 1): # the article was Australian court case (x[0]) and the prediction was also Australian court case x[2]
        return 1
      else:
        return 0

    # Function to calculate False Positives
    def calculateFalsePositives(x):
      if (x[0] == 0 and x[2] == 1): # the article was not Australian court case (x[0]) but the prediction was Australian court case x[2]
        return 1
      else:
        return 0

    # Function to calculate False Negatives
    def calculateFalseNegatives(x):
      if (x[0] == 1 and x[2] == 0): # the article was Australian court case (x[0]) but the prediction was not Australian court case x[2]
        return 1
      else:
        return 0
    
    # Function to calculate True Negatives
    def calculateTrueNegatives(x):
      if (x[0] == 0 and x[2] == 0): # the article was not Australian court case (x[0]) and the prediction was not Australian court case x[2]
        return 1
      else:
        return 0

    # Out of total positive labels predicted, how many correctly classified as positive, that is PPV
    def precision(x):
      # Number of true positives/ (Number of true positives + Number of false positives) 
      # return truePositive/(truePositive + falsePositive)
      return x[1][0]/(float(x[1][0] + x[1][1]))

    # Out of actual positive labels, how many correctly classified as positive, that is, TPR
    def recall(x):
      # Number of true positives/ (Number of true positives + Number of false Negatives) 
      # return truePositive/(truePositive + falseNegative)
      return x[1][0]/(float(x[1][0] +  x[1][2]))
      
      
    # Calculate 'True Positives', 'False Positives' and 'False Negatives'
    calcTP_FP_FN = yLabelAndXFeaturesPrediction.map(lambda x: (1, np.array([calculateTruePositives(x), calculateFalsePositives(x), calculateFalseNegatives(x),calculateTrueNegatives(x)]))).reduceByKey(np.add)
    
    print('')
    print ('#'*20)
    print('Number of True Positives:', calcTP_FP_FN.collect()[0][1][0])
    print('Number of False Positives:', calcTP_FP_FN.collect()[0][1][1])
    print('Number of False Negatives:', calcTP_FP_FN.collect()[0][1][2])
    print('Number of True Negatives:', calcTP_FP_FN.collect()[0][1][3])
    print('')
    
    
    # if 'Number of True Positives: 0 and 'Number of False Positives: 0, then F1 score is N/A
    if calcTP_FP_FN.collect()[0][1][0] == 0  and calcTP_FP_FN.collect()[0][1][1] == 0:
        calculateF1score = 'N/A'
        print('F1 score for classifier =','N/A')
        print ('#'*20)
        print('')
    else:    
        calculateF1score = calcTP_FP_FN.map(lambda x: (precision(x), recall(x))).map(lambda x: 2*x[0]*x[1] / (x[0] + x[1])).collect()[0]
        print('F1 score for classifier =',round(calculateF1score*100,2),'%')
        print ('#'*20)
        print('')
    
    # List to store the results of task 3
    ansForTask3 = []
    
    if calculateF1score != 'N/A':
        ansForTask3.append(('F1 score for classifier =',round(calculateF1score*100,2),'%'))
    else:
        ansForTask3.append(('F1 score for classifier =','N/A'))
    ansForTask3.append('')
    ansForTask3.append(('Number of True Positives', calcTP_FP_FN.collect()[0][1][0]))
    ansForTask3.append(('Number of False Positives', calcTP_FP_FN.collect()[0][1][1]))
    ansForTask3.append(('Number of False Negatives', calcTP_FP_FN.collect()[0][1][2]))
    ansForTask3.append(('Number of True Negatives', calcTP_FP_FN.collect()[0][1][3]))
    
    # Save the results of task3 in a text file
    sc.parallelize(ansForTask3).coalesce(1, shuffle = False).saveAsTextFile(sys.argv[4]) 
    
    sc.stop()
from datetime import datetime

from flaskblog import db


class User(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(20), unique=True, nullable=False)
    email = db.Column(db.String(120), unique=True, nullable=False)
    image_file = db.Column(db.String(20), nullable=False, default='default.jpg')
    password = db.Column(db.String(60), nullable=False)
    posts = db.relationship('Post', backref='author', lazy=True)

    def __repr__(self):
        return f"User('{self.username}', '{self.email}', '{self.image_file}')"


class Post(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    title = db.Column(db.String(100), nullable=False)
    date_posted = db.Column(db.DateTime, nullable=False, default=datetime.utcnow)
    content = db.Column(db.Text, nullable=False)
    user_id = db.Column(db.Integer, db.ForeignKey('user.id'), nullable=False)

    def __repr__(self):
        return f"User('{self.title}', '{self.date_posted}')"

import os
from pathlib import Path

from yaml import load, SafeLoader


def _service_path(folder: str, filename: str) -> str:
    current_directory = os.path.dirname(os.path.realpath(__file__))
    return os.path.normpath(os.path.join(current_directory, "..", folder, filename))


def load_config() -> dict:
    config_file_path = Path(__file__).parent.parent / "config/config.yml"
    with open(config_file_path, "r") as f:
        data = f.read()
        config_dict = load(data, SafeLoader)
        return config_dict


class Config:
    conf = load_config()
    if conf is None:
        conf = dict()

    # Server config
    BASE_URL = conf.get("BASE_URL", "http://cv.local")
    HTTP_PORT = conf.get("HTTP_PORT", 8099)
    STATIC_PATH = conf.get("STATIC_PATH", Path(__file__).parent.parent / "templates/static")
    TEMPLATES_PATH = conf.get("STATIC_PATH", Path(__file__).parent.parent / "templates")
    LOG_LEVEL = conf.get("LOG_LEVEL", "info")
    LOG_FORMAT = conf.get("LOG_FORMAT", "color")
    WEB_SECURE_COOKIES = conf.get("WEB_SECURE_COOKIES", False)

    # Database config
    DEFAULT_PG_URL = conf.get("PDB_URL", "postgresql://api:hackme@127.0.0.1:5488/app_sharer")
    PG_POOL_MIN_SIZE = conf.get("PG_POOL_MIN_SIZE", 10)
    PG_POOL_MAX_SIZE = conf.get("PG_POOL_MAX_SIZE", 10)


config = Config()

#
# Copyright (c) 2013 Juniper Networks, Inc. All rights reserved.
#

from setuptools import setup, find_packages

setup(
    name='nodemgr',
    version='0.1dev',
    packages=['nodemgr'],
    package_data={'': ['*.html', '*.css', '*.xml']},
    zip_safe=False,
    long_description="Nodemgr Implementation",
    entry_points={
        'console_scripts': [
            'contrail-nodemgr = nodemgr.main:main',
        ],
    },
)

#!/usr/bin/env python

"""
A basic tool for working with the Cards corpus transcripts.
"""

__author__ = "Christopher Potts"
__date__ = "2012-03-03"
__credits__ = "Thanks to Karl Schultz for designing the data collecting program, and \
               to David Clausen, Alex Djalali, Sven Lauer, Victoria Schwanda, Chriz Czyzewicz, \
               and the rest of the SUBTLE team for their help with data collection. \
               This research was supported in part by ONR grant No. N00014-10-1-0109 and \
               ARO grant No. W911NF-07-1-0216."
__license__ = "Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License: \
               http://creativecommons.org/licenses/by-nc-sa/3.0/"
__version__ = "2.0"
__maintainer__ = "Christopher Potts"
__email__ = "See the author's website"

######################################################################

import re
import os
import sys
import csv
import codecs
import datetime
#import pytz
from glob import iglob

######################################################################
###### ACTIONS CAPTURED BY PRAGBOT TRANSCRIPTS
#
# These variables are useful to have named, and this also serves as a
# reference for the complete list of meta-data values.

# Official names for the two players, as written in the transcripts.
PLAYER1 = "Player 1"
PLAYER2 = "Player 2"

# META-DATA ABOUT THE GAME
ORIGINAL_FILENAME  = r"ORIGINAL_FILENAME"   # Filename as created by the program; included for book-keeping purposes.
COLLECTION_SITE    = r"COLLECTION_SITE"     # 'Amazon Mechanical Turk' or 'Penn'
TASK_COMPLETED     = r"TASK_COMPLETED"      # Completion time - also encode in ORIGINAL_FILENAME
PLAYER             = r"PLAYER_[12]"         # Player Id in the format A[0-9]{5}, else 'UNKNOWN'
P1_MAX_LINEOFSIGHT = r"P1_MAX_LINEOFSIGHT"  # Distance the players could 'see'
P2_MAX_LINEOFSIGHT = r"P2_MAX_LINEOFSIGHT"
ENVIRONMENT        = r"CREATE_ENVIRONMENT"  # ASCII representation of the world
P1_MAX_TURNS       = r"P1_MAX_TURNS"        # Player's maximum allowed turns
P2_MAX_TURNS       = r"P2_MAX_TURNS"
P1_MAX_CARDS       = r"P1_MAX_CARDS"        # Number of cards the player could hold at any given time
P2_MAX_CARDS       = r"P2_MAX_CARDS"
PLAYER_2_TASK_ID   = r"PLAYER_2_TASK_ID"
PLAYER_1_TASK_ID   = r"PLAYER_1_TASK_ID"
GOAL_DESCRIPTION   = r"GOAL_DESCRIPTION"    # High-level goal (always the same in the v1 release)
# ACTIONS
## utterances
UTTERANCE          = r"CHAT_MESSAGE_PREFIX"
## locations
INITIAL_LOCATION   = r"PLAYER_INITIAL_LOCATION"
MOVE               = r"PLAYER_MOVE"
## card movements
PICKUP             = r"PLAYER_PICKUP_CARD"
DROP               = r"PLAYER_DROP_CARD"
# finish
TASK_COMPLETE      = r"TASK_COMPLETE_CLICKED"
CLOSE_SOCKETS      = r"CLOSE_SOCKETS"

######################################################################

class Corpus:
    """
    Corpus instances are built from the directory name of the
    corpus. Thus, if your program is in the same directory as the root
    of the corpus, you can use

    corpus = Corpus('transcripts')

    to build a corpus object.  Relative and full absolute paths also
    work.

    Corpus objects exist mainly as iterators. The methods
    iter_transcripts() and iter_events() allow you to move through the
    entire corpus efficiently.
    """
        
    def __init__(self, dirname):
        """
        Argument:
        dirname -- the root of the corpus transcripts
        """
        self.dirname = dirname

    def iter_transcripts(self, display_progress=True):
        """
        Iterate through the transcripts, by yielding Transcript objects one-by-one.

        Keyword argument:
        display_progress -- should create an overwriting progress bar to stderr if set to True (default: True)        
        """
        trans_no = 1
        for filename in iglob(os.path.join(self.dirname, '*/*.csv')):
            if display_progress:
                sys.stderr.write('\r') ; sys.stderr.write('transcript %s' % trans_no) ; sys.stderr.flush() ; trans_no += 1
            yield Transcript(filename)
        if display_progress:
            sys.stderr.write('\n')

    def iter_events(self, display_progress=True):
        """
        Iterate through the events, by yielding Event objects
        one-by-one.  This is useful if you don't need to rely on the
        transcripts as a unit --- say, because you're just counting
        words for the whole corpus.

        Keyword argument:
        display_progress -- should create an overwriting progress bar to stderr if set to True (default: True)        
        """
        for trans in self.iter_transcripts(display_progress=display_progress):
            for event in trans.iter_events():
                yield event

######################################################################

class Transcript:
    """
    Transcript objects correspond to individual files.
    You can build a Transcript object directly with

    trans = Transcript(filename)

    where filename is the absolute or relative path to the file you
    want to study.
    """
    
    def __init__(self, filename):
        """
        Argument:
        filename -- the source filename

        At intialization, the code turns the filename contents into a
        CSV reader and then turns each row into an Event instance. The
        attribute self.events is an ordered list of those Event
        instances.
        """
        self.filename = filename
        csvreader = csv.reader(codecs.open(self.filename, 'r', 'utf8'))
        self.events = []
        for row in csvreader:                
            self.events.append(Event(row))

    def iter_events(self):
        """
        Iterate through self.events.
        """
        for event in iter(self.events):
            yield event

######################################################################

class Event:
    """
    Events are the basic unit of the corpus. Event objects are not
    really designed to be instantiated directly, but rather only as
    part of a Trancript or Corpus instance. However, you can build
    then directly from a 4-membered list of strings. Here, I've
    copied and pasted a row from one of the CSV files and turned
    it into a list:

    event = Event(['Player 1','4555','PLAYER_INITIAL_LOCATION','16,25']
    """
    def __init__(self, row):
        """
        Argument:
        row -- a row of a csvreader (or a list)

        The attributes created:

        self.agent (str): Server, Player 1, or Player 2

        self.time (int): an integer representing the time of the
                         event relative to the start of the game

        self.action (str): a string capturing the type of action; see the
                           top of this file for details

        self.contents (str): the actual contents of the event; the structure
                             depends on self.action; see self.parse_contents()
                             for details
        """
        self.agent, time, self.action, self.contents = row
        self.time = int(time)

    def parse_contents(self):
        """
        This method seeks to do something useful with the contents of
        the event. Summary:

        -- utterances: return the list of strings as tokenized by Tokenizer().tokenize() [see below]
        -- pickup, drop: return a triple (x-coord, y-coord, card), where the coordinates are the location of the action
        -- move, initial location: return (x-coord, y-coord) of the resulting position
        -- task completed: return parsed date
        -- max cards, max turns, max line-of-sight: return int() of the value
        -- all else: return self.contents
        """
        # Utterances are tokenized using a basic card-aware tokenizer:
        if self.action == UTTERANCE:
            return Tokenizer().tokenize(self.contents)
        # Card manipulations return a trip (x-coord, y-coord, card)
        elif self.action in (PICKUP, DROP):
            loc, card = self.contents.split(":")
            lr, tb = loc.split(",")
            return (int(lr), int(tb), card)
        # Locations: pairs (x-coord, y-coord)
        elif self.action in (MOVE, INITIAL_LOCATION):
            lr, tb = self.contents.split(",")
            return (int(lr), int(tb))
        # Completion times are parsed as dates:
        elif self.action == TASK_COMPLETED:
            time_format = "%Y-%m-%d %H:%M:%S"
            dt = datetime.datetime.strptime(self.contents.replace(' EDT', ''), time_format)
            ##### Uncomment if localization is important:
            # eastern = pytz.timezone('US/Eastern')
            # dt = eastern.localize(dt)
            return dt
        # These values are parsed as integers:
        elif self.action in (P1_MAX_CARDS, P2_MAX_CARDS, P1_MAX_TURNS, P2_MAX_TURNS, P1_MAX_LINEOFSIGHT, P2_MAX_LINEOFSIGHT):
            return int(self.contents)
        # All other values are returned as strings:
        else:
            return self.contents
  
    def __repr__(self):
        """Computable representation of the object."""
        return '[%s]' % ','.join(map((lambda x : '"%s"' % x), (self.agent, self.time, self.action, self.contents)))        

######################################################################    

class Tokenizer:
    """
    This is a very basic tokenizer that seeks to keep intact emoticons
    and card references in a basic way. The class-level variables are
    put into a regular expression word_re (order matters) and then the
    input string is parsed with token_re.findall(). The output list
    treats things like '4 H', 'four of hearts', and >:( as single
    tokens. All characters are retained except whitespace not deemed
    to be word-internal.

    The tokenizer can be used on any string:

    words = Tokenizer().tokenize(s)

    where s is a str or unicode instance. The return value is a list
    of strings or unicode instances.
    """

    # Emoticon identification:
    emoticons = r"""
        (?:
            [<>]?
            [:;=8] # eyes
            [\-o\*\']? # optional nose
            [\)\]\(\[dDpP/\:\}\{@\|\\] # mouth      
            |
            [\)\]\(\[dDpP/\:\}\{@\|\\] # mouth
            [\-o\*\']? # optional nose
            [:;=8] # eyes
           [<>]?
        )"""

    # A few final kinds of plain words, and a last resort:
    words = r"""
        (?:[a-zA-Z][a-zA-Z'\-_]+[a-zA-Z]) # Words with apostrophes or dashes.
        |
        (?:[\w_]+)                        # Words without apostrophes or dashes.
        """

    # Ranks:
    ranks = r"""[Tt]wo|[Tt]hree|[Ff]our|[Ff]ive|[Ss]ix|[Ss]even|[Ee]ight|[Nn]ine|[Tt]en|[Jj]ack|[Qq]ueen|[Kk]ing|[Aa]ce
                |
                2|3|4|5|6|7|8|9|10|[Jj]|[Qq]|[Kk]|A"""

    # Suits:
    suits = r"[Hh]earts?|[Dd]iamonds?|[Ss]pades?|[Cc]lubs?|[Hh]|[Ss]|[Dd]|[Cc]"

    # Last-ditch effort to create tokens; finall splits on whitespace:
    misc_punc = r"""
        (?:[+\-]?\d+[,/.:-]\d+[+\-]?)     # Numbers, including fractions, decimals.
        |
        (?:\.(?:\s*\.){1,})               # Ellipsis dots. 
        |
        (?:\S)                            # Everything else that isn't whitespace.
        """

    # The actual tokenizing regular expression:
    token_re = re.compile(r"(%s)" % "|".join((emoticons, words, ranks, suits, misc_punc)), re.VERBOSE)
   
    def tokenize(self, s):
        """
        Tokenize the string s using token_re.findall(). Return value
        is a list of strings or unicode instances.
        """
        return Tokenizer.token_re.findall(s)

# -*- coding: utf-8 -*-
# Generated by Django 1.9.1 on 2016-11-14 19:51
from __future__ import unicode_literals

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('annotation', '0031_auto_20161111_1943'),
    ]

    operations = [
        migrations.AddField(
            model_name='masterobservation',
            name='observation_time',
            field=models.PositiveIntegerField(blank=True, null=True),
        ),
        migrations.AddField(
            model_name='observation',
            name='observation_time',
            field=models.PositiveIntegerField(blank=True, null=True),
        ),
    ]

import numpy as np

import mss

from redis import StrictRedis

import skimage.transform
import skimage.util

from serpent.config import config

import time

from serpent.game_frame import GameFrame
from serpent.game_frame_buffer import GameFrameBuffer

from serpent.frame_transformation_pipeline import FrameTransformationPipeline

from serpent.utilities import is_macos


redis_client = StrictRedis(**config["redis"])


class FrameGrabber:

    def __init__(self, width=640, height=480, x_offset=0, y_offset=0, fps=30, pipeline_string=None, buffer_seconds=5):
        self.width = width
        self.height = height

        self.x_offset = x_offset
        self.y_offset = y_offset

        self.frame_time = 1 / fps
        self.frame_buffer_size = buffer_seconds * fps

        self.redis_client = redis_client
        self.screen_grabber = mss.mss()

        self.frame_transformation_pipeline = None

        if pipeline_string is not None and isinstance(pipeline_string, str):
            self.frame_transformation_pipeline = FrameTransformationPipeline(pipeline_string=pipeline_string)

        self.is_retina_display = False
        self.is_retina_display = self._perform_retina_display_check()

        # Clear any previously stored frames
        self.redis_client.delete(config["frame_grabber"]["redis_key"])
        self.redis_client.delete(config["frame_grabber"]["redis_key"] + "_PIPELINE")

    def start(self):
        while True:
            cycle_start = time.time()

            frame = self.grab_frame()

            if self.frame_transformation_pipeline is not None:
                frame_pipeline = self.frame_transformation_pipeline.transform(frame)
            else:
                frame_pipeline = frame

            frame_shape = str(frame.shape).replace("(", "").replace(")", "")
            frame_dtype = str(frame.dtype)

            frame_bytes = f"{cycle_start}~{frame_shape}~{frame_dtype}~".encode("utf-8") + frame.tobytes()

            self.redis_client.lpush(config["frame_grabber"]["redis_key"], frame_bytes)
            self.redis_client.ltrim(config["frame_grabber"]["redis_key"], 0, self.frame_buffer_size)

            if self._has_png_transformation_pipeline():
                frame_pipeline_shape = "PNG"
                frame_pipeline_dtype = "PNG"

                frame_pipeline_bytes = f"{cycle_start}~{frame_pipeline_shape}~{frame_pipeline_dtype}~".encode("utf-8") + frame_pipeline
            else:
                frame_pipeline_shape = str(frame_pipeline.shape).replace("(", "").replace(")", "")
                frame_pipeline_dtype = str(frame_pipeline.dtype)

                frame_pipeline_bytes = f"{cycle_start}~{frame_pipeline_shape}~{frame_pipeline_dtype}~".encode("utf-8") + frame_pipeline.tobytes()

            self.redis_client.lpush(config["frame_grabber"]["redis_key"] + "_PIPELINE", frame_pipeline_bytes)
            self.redis_client.ltrim(config["frame_grabber"]["redis_key"] + "_PIPELINE", 0, self.frame_buffer_size)

            cycle_end = time.time()

            cycle_duration = (cycle_end - cycle_start)
            cycle_duration -= int(cycle_duration)

            frame_time_left = self.frame_time - cycle_duration

            if frame_time_left > 0:
                time.sleep(frame_time_left)

    def grab_frame(self):
        frame = np.array(
            self.screen_grabber.grab({
                "top": self.y_offset,
                "left": self.x_offset,
                "width": self.width,
                "height": self.height
            }),
            dtype="uint8"
        )

        frame = frame[..., [2, 1, 0, 3]]

        if self.is_retina_display:
            frame = skimage.util.img_as_ubyte(skimage.transform.resize(frame, (frame.shape[0] // 2, frame.shape[1] // 2)))
            frame = frame[:self.height, :self.width, :]

        return frame[..., :3]

    def _perform_retina_display_check(self):
        retina_display = False

        if is_macos():
            frame = self.grab_frame()

            if frame.shape[0] > self.height:
                retina_display = True

        return retina_display

    def _has_png_transformation_pipeline(self):
        return self.frame_transformation_pipeline and self.frame_transformation_pipeline.pipeline_string and self.frame_transformation_pipeline.pipeline_string.endswith("|PNG")

    @classmethod
    def get_frames(cls, frame_buffer_indices, frame_type="FULL", **kwargs):
        while True:
            if redis_client.llen(config["frame_grabber"]["redis_key"]) > 149:
                break
            
            time.sleep(0.1)

        game_frame_buffer = GameFrameBuffer(size=len(frame_buffer_indices))

        for i in frame_buffer_indices:
            redis_key = config["frame_grabber"]["redis_key"]
            redis_key = redis_key + "_PIPELINE" if frame_type == "PIPELINE" else redis_key

            frame_data = redis_client.lindex(redis_key, i)

            timestamp, shape, dtype, frame_bytes = frame_data.split("~".encode("utf-8"), maxsplit=3)

            if dtype == "PNG".encode("utf-8"):
                frame_array = frame_bytes
            else:
                frame_shape = [int(i) for i in shape.decode("utf-8").split(", ")]
                frame_array = np.fromstring(frame_bytes, dtype=dtype.decode("utf-8")).reshape(frame_shape)

            game_frame = GameFrame(frame_array, timestamp=float(timestamp))

            game_frame_buffer.add_game_frame(game_frame)

        return game_frame_buffer

    @classmethod
    def get_frames_with_pipeline(cls, frame_buffer_indices, **kwargs):
        while True:
            if redis_client.llen(config["frame_grabber"]["redis_key"]) > 149:
                break

            time.sleep(0.1)

        game_frame_buffers = [
            GameFrameBuffer(size=len(frame_buffer_indices)),
            GameFrameBuffer(size=len(frame_buffer_indices))
        ]

        for i in frame_buffer_indices:
            redis_keys = [config["frame_grabber"]["redis_key"], config["frame_grabber"]["redis_key"] + "_PIPELINE"]

            for index, redis_key in enumerate(redis_keys):
                frame_data = redis_client.lindex(redis_key, i)

                timestamp, shape, dtype, frame_bytes = frame_data.split("~".encode("utf-8"), maxsplit=3)

                if dtype == "PNG".encode("utf-8"):
                    frame_array = frame_bytes
                else:
                    frame_shape = [int(i) for i in shape.decode("utf-8").split(", ")]
                    frame_array = np.fromstring(frame_bytes, dtype=dtype.decode("utf-8")).reshape(frame_shape)

                game_frame = GameFrame(frame_array, timestamp=float(timestamp))

                game_frame_buffers[index].add_game_frame(game_frame)

        return game_frame_buffers

# -*- coding: utf-8 -*-

"""Top-level package for Easy graphviz."""

__author__ = """Gus Dunn"""
__email__ = 'w.gus.dunn@gmail.com'
__version__ = '0.1.1'

import re

import sqlparse

from django.db.backends.base.introspection import (
    BaseDatabaseIntrospection, FieldInfo, TableInfo,
)
from django.db.models.indexes import Index

field_size_re = re.compile(r'^\s*(?:var)?char\s*\(\s*(\d+)\s*\)\s*$')


def get_field_size(name):
    """ Extract the size number from a "varchar(11)" type name """
    m = field_size_re.search(name)
    return int(m.group(1)) if m else None


# This light wrapper "fakes" a dictionary interface, because some SQLite data
# types include variables in them -- e.g. "varchar(30)" -- and can't be matched
# as a simple dictionary lookup.
class FlexibleFieldLookupDict:
    # Maps SQL types to Django Field types. Some of the SQL types have multiple
    # entries here because SQLite allows for anything and doesn't normalize the
    # field type; it uses whatever was given.
    base_data_types_reverse = {
        'bool': 'BooleanField',
        'boolean': 'BooleanField',
        'smallint': 'SmallIntegerField',
        'smallint unsigned': 'PositiveSmallIntegerField',
        'smallinteger': 'SmallIntegerField',
        'int': 'IntegerField',
        'integer': 'IntegerField',
        'bigint': 'BigIntegerField',
        'integer unsigned': 'PositiveIntegerField',
        'decimal': 'DecimalField',
        'real': 'FloatField',
        'text': 'TextField',
        'char': 'CharField',
        'blob': 'BinaryField',
        'date': 'DateField',
        'datetime': 'DateTimeField',
        'time': 'TimeField',
    }

    def __getitem__(self, key):
        key = key.lower()
        try:
            return self.base_data_types_reverse[key]
        except KeyError:
            size = get_field_size(key)
            if size is not None:
                return ('CharField', {'max_length': size})
            raise KeyError


class DatabaseIntrospection(BaseDatabaseIntrospection):
    data_types_reverse = FlexibleFieldLookupDict()

    def get_table_list(self, cursor):
        """Return a list of table and view names in the current database."""
        # Skip the sqlite_sequence system table used for autoincrement key
        # generation.
        cursor.execute("""
            SELECT name, type FROM sqlite_master
            WHERE type in ('table', 'view') AND NOT name='sqlite_sequence'
            ORDER BY name""")
        return [TableInfo(row[0], row[1][0]) for row in cursor.fetchall()]

    def get_table_description(self, cursor, table_name):
        """
        Return a description of the table with the DB-API cursor.description
        interface.
        """
        return [
            FieldInfo(
                info['name'],
                info['type'],
                None,
                info['size'],
                None,
                None,
                info['null_ok'],
                info['default'],
            ) for info in self._table_info(cursor, table_name)
        ]

    def get_sequences(self, cursor, table_name, table_fields=()):
        pk_col = self.get_primary_key_column(cursor, table_name)
        return [{'table': table_name, 'column': pk_col}]

    def get_relations(self, cursor, table_name):
        """
        Return a dictionary of {field_name: (field_name_other_table, other_table)}
        representing all relationships to the given table.
        """
        # Dictionary of relations to return
        relations = {}

        # Schema for this table
        cursor.execute(
            "SELECT sql, type FROM sqlite_master "
            "WHERE tbl_name = %s AND type IN ('table', 'view')",
            [table_name]
        )
        create_sql, table_type = cursor.fetchone()
        if table_type == 'view':
            # It might be a view, then no results will be returned
            return relations
        results = create_sql[create_sql.index('(') + 1:create_sql.rindex(')')]

        # Walk through and look for references to other tables. SQLite doesn't
        # really have enforced references, but since it echoes out the SQL used
        # to create the table we can look for REFERENCES statements used there.
        for field_desc in results.split(','):
            field_desc = field_desc.strip()
            if field_desc.startswith("UNIQUE"):
                continue

            m = re.search(r'references (\S*) ?\(["|]?(.*)["|]?\)', field_desc, re.I)
            if not m:
                continue
            table, column = [s.strip('"') for s in m.groups()]

            if field_desc.startswith("FOREIGN KEY"):
                # Find name of the target FK field
                m = re.match(r'FOREIGN KEY\s*\(([^\)]*)\).*', field_desc, re.I)
                field_name = m.groups()[0].strip('"')
            else:
                field_name = field_desc.split()[0].strip('"')

            cursor.execute("SELECT sql FROM sqlite_master WHERE tbl_name = %s", [table])
            result = cursor.fetchall()[0]
            other_table_results = result[0].strip()
            li, ri = other_table_results.index('('), other_table_results.rindex(')')
            other_table_results = other_table_results[li + 1:ri]

            for other_desc in other_table_results.split(','):
                other_desc = other_desc.strip()
                if other_desc.startswith('UNIQUE'):
                    continue

                other_name = other_desc.split(' ', 1)[0].strip('"')
                if other_name == column:
                    relations[field_name] = (other_name, table)
                    break

        return relations

    def get_key_columns(self, cursor, table_name):
        """
        Return a list of (column_name, referenced_table_name, referenced_column_name)
        for all key columns in given table.
        """
        key_columns = []

        # Schema for this table
        cursor.execute("SELECT sql FROM sqlite_master WHERE tbl_name = %s AND type = %s", [table_name, "table"])
        results = cursor.fetchone()[0].strip()
        results = results[results.index('(') + 1:results.rindex(')')]

        # Walk through and look for references to other tables. SQLite doesn't
        # really have enforced references, but since it echoes out the SQL used
        # to create the table we can look for REFERENCES statements used there.
        for field_index, field_desc in enumerate(results.split(',')):
            field_desc = field_desc.strip()
            if field_desc.startswith("UNIQUE"):
                continue

            m = re.search(r'"(.*)".*references (.*) \(["|](.*)["|]\)', field_desc, re.I)
            if not m:
                continue

            # This will append (column_name, referenced_table_name, referenced_column_name) to key_columns
            key_columns.append(tuple(s.strip('"') for s in m.groups()))

        return key_columns

    def get_primary_key_column(self, cursor, table_name):
        """Return the column name of the primary key for the given table."""
        # Don't use PRAGMA because that causes issues with some transactions
        cursor.execute(
            "SELECT sql, type FROM sqlite_master "
            "WHERE tbl_name = %s AND type IN ('table', 'view')",
            [table_name]
        )
        row = cursor.fetchone()
        if row is None:
            raise ValueError("Table %s does not exist" % table_name)
        create_sql, table_type = row
        if table_type == 'view':
            # Views don't have a primary key.
            return None
        fields_sql = create_sql[create_sql.index('(') + 1:create_sql.rindex(')')]
        for field_desc in fields_sql.split(','):
            field_desc = field_desc.strip()
            m = re.match(r'(?:(?:["`\[])(.*)(?:["`\]])|(\w+)).*PRIMARY KEY.*', field_desc)
            if m:
                return m.group(1) if m.group(1) else m.group(2)
        return None

    def _table_info(self, cursor, name):
        cursor.execute('PRAGMA table_info(%s)' % self.connection.ops.quote_name(name))
        # cid, name, type, notnull, default_value, pk
        return [{
            'name': field[1],
            'type': field[2],
            'size': get_field_size(field[2]),
            'null_ok': not field[3],
            'default': field[4],
            'pk': field[5],  # undocumented
        } for field in cursor.fetchall()]

    def _get_foreign_key_constraints(self, cursor, table_name):
        constraints = {}
        cursor.execute('PRAGMA foreign_key_list(%s)' % self.connection.ops.quote_name(table_name))
        for row in cursor.fetchall():
            # Remaining on_update/on_delete/match values are of no interest.
            id_, _, table, from_, to = row[:5]
            constraints['fk_%d' % id_] = {
                'columns': [from_],
                'primary_key': False,
                'unique': False,
                'foreign_key': (table, to),
                'check': False,
                'index': False,
            }
        return constraints

    def get_constraints(self, cursor, table_name):
        """
        Retrieve any constraints or keys (unique, pk, fk, check, index) across
        one or more columns.
        """
        constraints = {}
        # Find inline check constraints.
        try:
            table_schema = cursor.execute(
                "SELECT sql FROM sqlite_master WHERE type='table' and name=%s" % (
                    self.connection.ops.quote_name(table_name),
                )
            ).fetchone()[0]
        except TypeError:
            # table_name is a view.
            pass
        else:
            # Check constraint parsing is based of SQLite syntax diagram.
            # https://www.sqlite.org/syntaxdiagrams.html#table-constraint
            def next_ttype(ttype):
                for token in tokens:
                    if token.ttype == ttype:
                        return token

            statement = sqlparse.parse(table_schema)[0]
            tokens = statement.flatten()
            for token in tokens:
                name = None
                if token.match(sqlparse.tokens.Keyword, 'CONSTRAINT'):
                    # Table constraint
                    name_token = next_ttype(sqlparse.tokens.Literal.String.Symbol)
                    name = name_token.value[1:-1]
                    token = next_ttype(sqlparse.tokens.Keyword)
                if token.match(sqlparse.tokens.Keyword, 'UNIQUE'):
                    constraints[name] = {
                        'unique': True,
                        'columns': [],
                        'primary_key': False,
                        'foreign_key': False,
                        'check': False,
                        'index': False,
                    }
                if token.match(sqlparse.tokens.Keyword, 'CHECK'):
                    # Column check constraint
                    if name is None:
                        column_token = next_ttype(sqlparse.tokens.Literal.String.Symbol)
                        column = column_token.value[1:-1]
                        name = '__check__%s' % column
                        columns = [column]
                    else:
                        columns = []
                    constraints[name] = {
                        'check': True,
                        'columns': columns,
                        'primary_key': False,
                        'unique': False,
                        'foreign_key': False,
                        'index': False,
                    }
        # Get the index info
        cursor.execute("PRAGMA index_list(%s)" % self.connection.ops.quote_name(table_name))
        for row in cursor.fetchall():
            # Sqlite3 3.8.9+ has 5 columns, however older versions only give 3
            # columns. Discard last 2 columns if there.
            number, index, unique = row[:3]
            # Get the index info for that index
            cursor.execute('PRAGMA index_info(%s)' % self.connection.ops.quote_name(index))
            for index_rank, column_rank, column in cursor.fetchall():
                if index not in constraints:
                    constraints[index] = {
                        "columns": [],
                        "primary_key": False,
                        "unique": bool(unique),
                        "foreign_key": False,
                        "check": False,
                        "index": True,
                    }
                constraints[index]['columns'].append(column)
            # Add type and column orders for indexes
            if constraints[index]['index'] and not constraints[index]['unique']:
                # SQLite doesn't support any index type other than b-tree
                constraints[index]['type'] = Index.suffix
                cursor.execute(
                    "SELECT sql FROM sqlite_master "
                    "WHERE type='index' AND name=%s" % self.connection.ops.quote_name(index)
                )
                orders = []
                # There would be only 1 row to loop over
                for sql, in cursor.fetchall():
                    order_info = sql.split('(')[-1].split(')')[0].split(',')
                    orders = ['DESC' if info.endswith('DESC') else 'ASC' for info in order_info]
                constraints[index]['orders'] = orders
        # Get the PK
        pk_column = self.get_primary_key_column(cursor, table_name)
        if pk_column:
            # SQLite doesn't actually give a name to the PK constraint,
            # so we invent one. This is fine, as the SQLite backend never
            # deletes PK constraints by name, as you can't delete constraints
            # in SQLite; we remake the table with a new PK instead.
            constraints["__primary__"] = {
                "columns": [pk_column],
                "primary_key": True,
                "unique": False,  # It's not actually a unique constraint.
                "foreign_key": False,
                "check": False,
                "index": False,
            }
        constraints.update(self._get_foreign_key_constraints(cursor, table_name))
        return constraints

# This file is a part of Arjuna
# Copyright 2015-2021 Rahul Verma

# Website: www.RahulVerma.net

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#   http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import uuid
import time
import logging
import shutil
from arjuna import ArjunaOption


from arjuna.tpi.config import Configuration

from arjuna.configure.configurator import TestConfigurator
from arjuna.drive.invoker.databroker import TestSessionDataBrokerHandler
from arjuna.interact.gui.gom.guimgr import GuiManager

class TestSessionController:
    
    def __init__(self):
        self.__id = uuid.uuid4()
        self.__DEF_CONF_NAME = "ref"
        self.__default_ref_config = None
        self.__config_map = {}
        self.__cli_central_config = None
        self.__cli_test_config = None
        self.__configurator = None
        self.__project_config_loaded = False
        self.__guimgr = None

    @property
    def id(self):
        return self.__id

    @property
    def configurator(self):
        return self.__configurator

    @property
    def data_broker(self):
        return self.__data_broker   

    @property
    def gui_manager(self):
        return self.__guimgr

    def init(self, root_dir, cli_config=None, run_id=None, linked_projects=[]):
        self.__configurator = TestConfigurator(root_dir, cli_config, run_id, linked_projects)
        ref_config = self.__configurator.ref_config
        data_env_confs = self.__configurator.file_confs
        self.__guimgr = GuiManager(ref_config)
        ref_conf = self._create_config(ref_config)
        self.__add_to_map(ref_conf)
        for run_env_conf in [self._create_config(econf, name=name) for name, econf in data_env_confs.items()]:
            self.__add_to_map(run_env_conf)

        return ref_conf

    def __msession_config(self, ref_conf_name):
        from arjuna import Arjuna
        if ref_conf_name is None:
            ref_conf_name = "ref"
        return Arjuna.get_config(ref_conf_name)

    def _create_config(self, config, name=None):
        config = Configuration(
            self,
            name and name or self.__DEF_CONF_NAME,
            config
        )
        return config

    def finish(self):
        pass

    def __add_to_map(self, config):
        from arjuna import Arjuna
        Arjuna.register_config(config)

    def load_options_from_file(self, fpath, *, conf_stage):
        return self.configurator.load_options_from_file(fpath, conf_stage=conf_stage)

    def register_config(self, name, arjuna_options, user_options, *, conf_stage, parent_config=None):
        config = self.configurator.register_new_config(arjuna_options, user_options, parent_config=parent_config, conf_stage=conf_stage)
        conf = self._create_config(config, name=name)
        self.__add_to_map(conf)
        return conf

    def create_file_data_source(self, record_type, file_name, *arg_pairs):
        raise NotImplementedError()
        # response = self._send_request(
        #     ArjunaComponent.DATA_SOURCE,
        #     DataSourceActionType.CREATE_FILE_DATA_SOURCE,
        #     *arg_pairs
        # )
        # return response.get_data_source_id()

    def define_gui(self, automator, label=None, name=None, qual_name=None, def_file_path=None):
        return self.gui_manager.define_gui(automator, label=label, name=name, qual_name=qual_name, def_file_path=def_file_path)

#    ____  ____
#   /   /\/   /
#  /___/  \  /   Copyright (c) 2021, Xilinx.
#  \   \   \/    Author: Vctor Mayoral Vilches <victorma@xilinx.com>
#   \   \
#   /   /
#  /___/   /\
#  \   \  /  \
#   \___\/\___\
#
# Licensed under the Apache License, Version 2.0
#
__version__ = "0.2.0"

import torch.nn as nn
import torch
from torch.autograd import Variable
import torch.nn.functional as F


class AngleLoss(nn.Module):  # lossgamma
    def __init__(self, gamma=0, lambda_min=5, lambda_max=1500):
        super(AngleLoss, self).__init__()
        self.gamma = gamma
        self.it = 0
        self.lambda_min = lambda_min
        self.lambda_max = lambda_max

    def forward(self, x, y):  # outputtarget
        self.it += 1
        cos_theta, phi_theta = x  # output[cos_theta, phi_theta]
        y = y.view(-1, 1)

        index = cos_theta.data * 0.0
        index.scatter_(1, y.data.view(-1, 1), 1)  # label
        index = index.byte()
        # index = Variable(index)   # warning occurs, change to following line. see link blew:
        # https://github.com/pytorch/pytorch/issues/29365
        #index = torch.tensor(index, dtype=torch.bool)
        index = index.clone().detach().bool()

        lamb = max(self.lambda_min, self.lambda_max / (1 + 0.1 * self.it))  # lambdacos(\theta)\phi(\theta)
        output = cos_theta * 1.0
        output[index] -= cos_theta[index]*(1.0+0)/(1 + lamb)  # \cos(\theta)
        output[index] += phi_theta[index]*(1.0+0)/(1 + lamb)  # \phi(\theta)

        logpt = F.log_softmax(output, dim=1)
        logpt = logpt.gather(1, y)
        logpt = logpt.view(-1)
        pt = Variable(logpt.data.exp())

        loss = -1 * (1-pt)**self.gamma * logpt
        loss = loss.mean()

        return loss


class AngleLossWithCE(nn.Module):
    def __init__(self, lambda_min=5, lambda_max=1500, weight=[1, 1]):
        super().__init__()
        self.embeddingLoss = AngleLoss(lambda_min, lambda_max)
        self.clsLoss = nn.CrossEntropyLoss()
        self.weight = weight

    def forward(self, x1, x2, label):
        embeddingLoss = self.embeddingLoss(x1, label)
        clsLoss = self.clsLoss(x2, label)
        total_loss = embeddingLoss * self.weight[0] + clsLoss * self.weight[1]
        return total_loss


# http://papers.nips.cc/paper/6653-learning-with-average-top-k-loss.pdf
# not sure working or not
class TopKLossWithBCE(nn.Module):
    def __init__(self, p=0.7):
        super().__init__()
        self.p = p
        self.bce = nn.BCEWithLogitsLoss(reduction='none')

    def forward(self, pred, gt):
        k = int(pred.shape[0] * self.p)
        loss = self.bce(pred, gt)
        loss = loss.topk(k, dim=0)[0]
        loss = loss.mean()
        return loss


# https://gist.github.com/SuperShinyEyes/dcc68a08ff8b615442e3bc6a9b55a354
class F1_Loss(nn.Module):
    '''Calculate F1 score. Can work with gpu tensors

    The original implmentation is written by Michal Haltuf on Kaggle.

    Returns
    -------
    torch.Tensor
        `ndim` == 1. epsilon <= val <= 1

    Reference
    ---------
    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric
    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score
    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6
    - http://www.ryanzhang.info/python/writing-your-own-loss-function-module-for-pytorch/
    '''

    def __init__(self, classes=2, epsilon=1e-7):
        super().__init__()
        self.epsilon = epsilon
        self.classes = classes

    def forward(self, y_pred, y_true, ):
        assert y_pred.ndim == 2
        assert y_true.ndim == 1
        y_true = F.one_hot(y_true, self.classes).to(torch.float32)
        y_pred = F.softmax(y_pred, dim=1)

        tp = (y_true * y_pred).sum(dim=0).to(torch.float32)
        tn = ((1 - y_true) * (1 - y_pred)).sum(dim=0).to(torch.float32)
        fp = ((1 - y_true) * y_pred).sum(dim=0).to(torch.float32)
        fn = (y_true * (1 - y_pred)).sum(dim=0).to(torch.float32)

        precision = tp / (tp + fp + self.epsilon)
        recall = tp / (tp + fn + self.epsilon)

        f1 = 2 * (precision * recall) / (precision + recall + self.epsilon)
        f1 = f1.clamp(min=self.epsilon, max=1 - self.epsilon)
        return 1 - f1.mean()


class F1LossWithBCE(nn.Module):
    def __init__(self, classes=264, weights=[1, 1]):
        super().__init__()
        self.classes = classes
        self.weights = weights
        self.bce = nn.BCEWithLogitsLoss()
        self.f1 = F1_Loss(classes=self.classes)

    def forward(self, pred, gt):
        bce = self.bce(pred, gt)
        f1 = self.f1(pred, gt)
        loss = self.weights[0] * bce + self.weights[1] * f1
        return loss



__all__ = ['Particle']

from sympy import sympify
from sympy.physics.mechanics.point import Point


class Particle(object):
    """A particle.

    Particles have a non-zero mass and lack spatial extension; they take up no
    space.

    Values need to be supplied on initialization, but can be changed later.

    Parameters
    ==========
    name : str
        Name of particle
    point : Point
        A physics/mechanics Point which represents the position, velocity, and
        acceleration of this Particle
    mass : sympifyable
        A SymPy expression representing the Particle's mass

    Examples
    ========

    >>> from sympy.physics.mechanics import Particle, Point
    >>> from sympy import Symbol
    >>> po = Point('po')
    >>> m = Symbol('m')
    >>> pa = Particle('pa', po, m)
    >>> # Or you could change these later
    >>> pa.mass = m
    >>> pa.point = po

    """

    def __init__(self, name, point, mass):
        if not isinstance(name, str):
            raise TypeError('Supply a valid name.')
        self._name = name
        self.set_mass(mass)
        self.set_point(point)
        self._pe = sympify(0)

    def __str__(self):
        return self._name

    __repr__ = __str__

    def get_mass(self):
        """Mass of the particle."""
        return self._mass

    def set_mass(self, mass):
        self._mass = sympify(mass)

    mass = property(get_mass, set_mass)

    def get_point(self):
        """Point of the particle."""
        return self._point

    def set_point(self, p):
        if not isinstance(p, Point):
            raise TypeError("Particle point attribute must be a Point object.")
        self._point = p

    point = property(get_point, set_point)

    def linear_momentum(self, frame):
        """Linear momentum of the particle.

        The linear momentum L, of a particle P, with respect to frame N is
        given by

        L = m * v

        where m is the mass of the particle, and v is the velocity of the
        particle in the frame N.

        Parameters
        ==========

        frame : ReferenceFrame
            The frame in which linear momentum is desired.

        Examples
        ========

        >>> from sympy.physics.mechanics import Particle, Point, ReferenceFrame
        >>> from sympy.physics.mechanics import dynamicsymbols
        >>> m, v = dynamicsymbols('m v')
        >>> N = ReferenceFrame('N')
        >>> P = Point('P')
        >>> A = Particle('A', P, m)
        >>> P.set_vel(N, v * N.x)
        >>> A.linear_momentum(N)
        m*v*N.x

        """

        return self.mass * self.point.vel(frame)

    def angular_momentum(self, point, frame):
        """Angular momentum of the particle about the point.

        The angular momentum H, about some point O of a particle, P, is given
        by:

        H = r x m * v

        where r is the position vector from point O to the particle P, m is
        the mass of the particle, and v is the velocity of the particle in
        the inertial frame, N.

        Parameters
        ==========

        point : Point
            The point about which angular momentum of the particle is desired.

        frame : ReferenceFrame
            The frame in which angular momentum is desired.

        Examples
        ========

        >>> from sympy.physics.mechanics import Particle, Point, ReferenceFrame
        >>> from sympy.physics.mechanics import dynamicsymbols
        >>> m, v, r = dynamicsymbols('m v r')
        >>> N = ReferenceFrame('N')
        >>> O = Point('O')
        >>> A = O.locatenew('A', r * N.x)
        >>> P = Particle('P', A, m)
        >>> P.point.set_vel(N, v * N.y)
        >>> P.angular_momentum(O, N)
        m*r*v*N.z

        """

        return self.point.pos_from(point) ^ (self.mass * self.point.vel(frame))

    def kinetic_energy(self, frame):
        """Kinetic energy of the particle

        The kinetic energy, T, of a particle, P, is given by

        'T = 1/2 m v^2'

        where m is the mass of particle P, and v is the velocity of the
        particle in the supplied ReferenceFrame.

        Parameters
        ==========

        frame : ReferenceFrame
            The Particle's velocity is typically defined with respect to
            an inertial frame but any relevant frame in which the velocity is
            known can be supplied.

        Examples
        ========

        >>> from sympy.physics.mechanics import Particle, Point, ReferenceFrame
        >>> from sympy import symbols
        >>> m, v, r = symbols('m v r')
        >>> N = ReferenceFrame('N')
        >>> O = Point('O')
        >>> P = Particle('P', O, m)
        >>> P.point.set_vel(N, v * N.y)
        >>> P.kinetic_energy(N)
        m*v**2/2

        """

        return (self.mass / sympify(2) * self.point.vel(frame) &
                self.point.vel(frame))

    def set_potential_energy(self, scalar):
        """Used to set the potential energy of the Particle.

        Parameters
        ==========

        scalar : Sympifyable
            The potential energy (a scalar) of the Particle.

        Examples
        ========

        >>> from sympy.physics.mechanics import Particle, Point
        >>> from sympy import symbols
        >>> m, g, h = symbols('m g h')
        >>> O = Point('O')
        >>> P = Particle('P', O, m)
        >>> P.set_potential_energy(m * g * h)

        """

        self._pe = sympify(scalar)

    @property
    def potential_energy(self):
        """The potential energy of the Particle.

        Examples
        ========

        >>> from sympy.physics.mechanics import Particle, Point
        >>> from sympy import symbols
        >>> m, g, h = symbols('m g h')
        >>> O = Point('O')
        >>> P = Particle('P', O, m)
        >>> P.set_potential_energy(m * g * h)
        >>> P.potential_energy
        g*h*m

        """

        return self._pe

import eth_tester.backends.mock.main as mock_backend
import eth_tester.backends.mock.factory as factory

DEFAULT_ALLOC = 0

ACCOUNT_DEFAULTS = {
        'balance': DEFAULT_ALLOC * mock_backend.denoms.ether,
        'code': b'',
        'nonce': 0,
        'storage': {},
    }


POLYGON_GENESIS = factory.make_genesis_block()


def update_defaults(**kwargs):
    ACCOUNT_DEFAULTS.update(**kwargs)


def account_defaults():
    return ACCOUNT_DEFAULTS

"""
Python Challenge Level 15.

The page title is 'whom?' and the image is a calendar of January 1??6 with Monday
Jan 26th circled. There are two comments in the page source, 'he ain't the
youngest, he is the second' and 'todo: buy flowers for tomorrow'.
"""
import calendar
import datetime

GREGORIAN_CALENDAR_START = 1582


def ends_in_6(year):
    """
    Does the year end in '6'?
    """
    return year % 10 == 6


def jan_26th_is_monday(year):
    """
    In this year, is January 26th a monday?
    """
    return calendar.weekday(year, 1, 26) == 0

matching_years = []
for year in range(GREGORIAN_CALENDAR_START, 2000):
    # Determine the years which could match the calendar conditions:
    if jan_26th_is_monday(year) and calendar.isleap(year) and ends_in_6(year):
        matching_years.append(year)

# 'he ain't the youngest, he is the second' - take the second youngest year
year = matching_years[-2]

# 'todo: buy flowers for tomorrow
print(datetime.date(year, 1, 26 + 1))
# '1756-01-27', which is Mozart's birthday
# http://www.pythonchallenge.com/pc/return/mozart.html is the next URL.

#!/usr/bin/env python3
"""Alta3 Research || Author RZFeeser@alta3.com
Learning how to use functions"""

## Installs the crayons package.
## python3 -m pip install crayons
## import statements ALWAYS go up top
import crayons


def main():
    """run time code. Always indent under function"""

    # print 'red string' in red
    print(crayons.red('red string'))

    # Red White and Blue text
    #print('{} white {}'.format(crayons.red('red'), crayons.blue('blue'))) # format string (old ver of str templating)
    print(f"{crayons.red('red')} white {crayons.blue('blue')}")  # f-string (newest version of str templating)

    crayons.disable() # disables the crayons package

    # this line should NOT have color as crayons is disabled
    print(f"{crayons.red('red')} white {crayons.blue('blue')}")  # f-string (newest version of string templating)

    crayons.DISABLE_COLOR = False # enable the crayons package

    # This line will print in color because color is enabled
    print(f"{crayons.red('red')} white {crayons.blue('blue')}")  # f-string (newest version of string templating)

    # print 'red string' in red
    print(crayons.red('red string', bold=True))

    # print 'yellow string' in yellow
    print(crayons.yellow('yellow string', bold=True))

    # print 'magenta string' in magenta
    print(crayons.magenta('magenta string', bold=True))

    # print 'white string' in white
    print(crayons.white('white string', bold=True))

    print(crayons.green('Nebiu Tadele in green'))
    print(crayons.blue('Nebiu Tadele in blue and in bold', bold=True))

# this condition is only true if our script is run directly
# it is NOT true if our code is imported into another script
if __name__ == "__main__":
    main()


#
# This file is part of pySMT.
#
#   Copyright 2014 Andrea Micheli and Marco Gario
#
#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.
#
from pysmt.shortcuts import (And, Iff, Or, Symbol, Implies, Not,
                             Exists, ForAll,
                             Times, Plus, Minus, Equals, Real,
                             is_valid)
from pysmt.test import TestCase, skipIfNoSolverForLogic, main
from pysmt.rewritings import prenex_normal_form, nnf, conjunctive_partition, aig
from pysmt.rewritings import disjunctive_partition
from pysmt.rewritings import TimesDistributor
from pysmt.test.examples import get_example_formulae
from pysmt.exceptions import SolverReturnedUnknownResultError
from pysmt.logics import BOOL, QF_NRA, QF_LRA, QF_LIA
from pysmt.typing import REAL


class TestRewritings(TestCase):

    def test_prenex_basic(self):
        a,b,c = (Symbol(x) for x in "abc")
        f = Not(And(a, Exists([b], And(a, b)), ForAll([c], Or(a, c))))
        prenex = prenex_normal_form(f)
        # Two prenex normal forms are possible
        my_prenex_1 = Exists([c], ForAll([b], Not(And(a, And(a, b), Or(a, c)))))
        my_prenex_2 = ForAll([b], Exists([c], Not(And(a, And(a, b), Or(a, c)))))
        self.assertTrue(prenex == my_prenex_1 or prenex == my_prenex_2)

    @skipIfNoSolverForLogic(BOOL)
    def test_prenex_simple_exists(self):
        a,b = (Symbol(x) for x in "ab")
        f = And(b, Exists([b], Implies(a, b)))
        prenex = prenex_normal_form(f)
        self.assertTrue(prenex.is_exists())
        self.assertValid(Iff(f, prenex), logic=BOOL)

    @skipIfNoSolverForLogic(BOOL)
    def test_prenex_simple_forall(self):
        a,b = (Symbol(x) for x in "ab")
        f = Or(b, ForAll([b], Implies(a, b)))
        prenex = prenex_normal_form(f)
        self.assertTrue(prenex.is_forall())
        self.assertValid(Iff(f, prenex), logic=BOOL)

    @skipIfNoSolverForLogic(BOOL)
    def test_prenex_negated_exists(self):
        a,b = (Symbol(x) for x in "ab")
        f = Implies(Exists([b], Implies(a, b)), b)
        prenex = prenex_normal_form(f)
        self.assertTrue(prenex.is_forall())
        self.assertValid(Iff(f, prenex), logic=BOOL)

    @skipIfNoSolverForLogic(BOOL)
    def test_prenex_negated_forall(self):
        a,b = (Symbol(x) for x in "ab")
        f = Implies(ForAll([b], Implies(a, b)), b)
        prenex = prenex_normal_form(f)
        self.assertTrue(prenex.is_exists())
        self.assertValid(Iff(f, prenex), logic=BOOL)

    def test_prenex_examples(self):
        for (f, _, _, logic) in get_example_formulae():
            if self.env.factory.has_solvers(logic=logic):
                prenex = prenex_normal_form(f)
                if ( prenex is not None):
                    try:
                        ok = is_valid(Iff(f, prenex), logic=logic)
                    except SolverReturnedUnknownResultError:
                        ok = not logic.quantifier_free
                    self.assertTrue(ok)

    def test_nnf_examples(self):
        for (f, _, _, logic) in get_example_formulae():
            if self.env.factory.has_solvers(logic=logic):
                rf = nnf(f)
                try:
                    ok = is_valid(Iff(f, rf), logic=logic)
                except SolverReturnedUnknownResultError:
                    ok = not logic.quantifier_free
                self.assertTrue(ok)

    def test_conj_partitioning(self):
        for (f, _, _, logic) in get_example_formulae():
            if self.env.factory.has_solvers(logic=logic):
                conjuncts = list(conjunctive_partition(f))
                try:
                    ok = is_valid(Iff(f, And(conjuncts)), logic=logic)
                except SolverReturnedUnknownResultError:
                    ok = not logic.quantifier_free
                self.assertTrue(ok)

    def test_disj_partitioning(self):
        for (f, _, _, logic) in get_example_formulae():
            if self.env.factory.has_solvers(logic=logic):
                disjuncts = list(disjunctive_partition(f))
                try:
                    ok = is_valid(Iff(f, Or(disjuncts)), logic=logic)
                except SolverReturnedUnknownResultError:
                    ok = not logic.quantifier_free
                self.assertTrue(ok)

    def test_aig_examples(self):
        for (f, _, _, logic) in get_example_formulae():
            if self.env.factory.has_solvers(logic=logic):
                f_aig = aig(f)
                try:
                    ok = is_valid(Iff(f, f_aig), logic=logic)
                except SolverReturnedUnknownResultError:
                    ok = not logic.quantifier_free
                self.assertTrue(ok, "Was: %s\n Got:%s" % (f, f_aig))

    @skipIfNoSolverForLogic(QF_NRA)
    def test_times_distributivity(self):
        r = Symbol("r", REAL)
        s = Symbol("s", REAL)
        td = TimesDistributor()

        f = Times(Plus(r, Real(1)), Real(3))
        fp = td.walk(f)
        self.assertValid(Equals(f, fp), (f, fp))

        f = Times(Plus(r, Real(1)), s)
        fp = td.walk(f)
        self.assertValid(Equals(f, fp), (f, fp))

        f = Times(Plus(r, Real(1), s), Real(3))
        fp = td.walk(f)
        self.assertValid(Equals(f, fp), (f, fp))

        f = Times(Minus(r, Real(1)), Real(3))
        fp = td.walk(f)
        self.assertValid(Equals(f, fp), (f, fp))

        f = Times(Minus(r, Real(1)), s)
        fp = td.walk(f)
        self.assertValid(Equals(f, fp), (f, fp))

        f = Times(Minus(Real(1), s), Real(3))
        fp = td.walk(f)
        self.assertValid(Equals(f, fp), (f, fp))

        f = Times(Minus(r, Real(1)), Plus(r, s))
        fp = td.walk(f)
        self.assertValid(Equals(f, fp), (f, fp))

        # (r + 1) * (s-1) = r*s + (-r) + s - 1
        f = Times(Plus(r, Real(1)), Minus(s, Real(1)))
        fp = td.walk(f).simplify()
        target = Plus(Times(r, s),
                      Times(r, Real(-1)),
                      s,
                      Real(-1))
        self.assertValid(Equals(fp, target), fp)
        self.assertTrue(fp.is_plus(), fp)

    @skipIfNoSolverForLogic(QF_NRA)
    def test_times_distributivity_smtlib_nra(self):
        from pysmt.test.smtlib.parser_utils import formulas_from_smtlib_test_set
        test_set = formulas_from_smtlib_test_set(logics=[QF_LRA, QF_NRA])
        for (_, fname, f, _) in test_set:
            td = TimesDistributor()
            _ = td.walk(f)
            for (old, new) in td.memoization.items():
                if not old.is_times(): continue
                if old is new: continue # Nothing changed
                self.assertValid(Equals(old, new),
                                 (old, new), solver_name="z3")

if __name__ == "__main__":
    main()

import argparse
import tools.find_mxnet
import mxnet as mx
import os
import sys
from train.train_net import train_net

def parse_args():
    parser = argparse.ArgumentParser(description='Train a Single-shot detection network')
    parser.add_argument('--dataset', dest='dataset', help='which dataset to use',
                        default='pascal', type=str)
    parser.add_argument('--image-set', dest='image_set', help='train set, can be trainval or train',
                        default='trainval', type=str)
    parser.add_argument('--year', dest='year', help='can be 2007, 2012',
                        default='2007,2012', type=str)
    parser.add_argument('--val-image-set', dest='val_image_set', help='validation set, can be val or test',
                        default='test', type=str)
    parser.add_argument('--val-year', dest='val_year', help='can be 2007, 2010, 2012',
                        default='2007', type=str)
    parser.add_argument('--devkit-path', dest='devkit_path', help='VOCdevkit path',
                        default=os.path.join(os.getcwd(), 'data', 'VOCdevkit'), type=str)
    parser.add_argument('--network', dest='network', type=str, default='vgg16_reduced',
                        choices=['vgg16_reduced'], help='which network to use')
    parser.add_argument('--batch-size', dest='batch_size', type=int, default=32,
                        help='training batch size')
    parser.add_argument('--resume', dest='resume', type=int, default=-1,
                        help='resume training from epoch n')
    parser.add_argument('--finetune', dest='finetune', type=int, default=-1,
                        help='finetune from epoch n, rename the model before doing this')
    parser.add_argument('--pretrained', dest='pretrained', help='pretrained model prefix',
                        default=os.path.join(os.getcwd(), 'model', 'vgg16_reduced'), type=str)
    parser.add_argument('--epoch', dest='epoch', help='epoch of pretrained model',
                        default=1, type=int)
    parser.add_argument('--prefix', dest='prefix', help='new model prefix',
                        default=os.path.join(os.getcwd(), 'model', 'ssd'), type=str)
    parser.add_argument('--gpus', dest='gpus', help='GPU devices to train with',
                        default='0', type=str)
    parser.add_argument('--begin-epoch', dest='begin_epoch', help='begin epoch of training',
                        default=0, type=int)
    parser.add_argument('--end-epoch', dest='end_epoch', help='end epoch of training',
                        default=100, type=int)
    parser.add_argument('--frequent', dest='frequent', help='frequency of logging',
                        default=20, type=int)
    parser.add_argument('--data-shape', dest='data_shape', type=int, default=300,
                        help='set image shape')
    parser.add_argument('--lr', dest='learning_rate', type=float, default=0.001,
                        help='learning rate')
    parser.add_argument('--momentum', dest='momentum', type=float, default=0.9,
                        help='momentum')
    parser.add_argument('--wd', dest='weight_decay', type=float, default=0.0001,
                        help='weight decay')
    parser.add_argument('--mean-r', dest='mean_r', type=float, default=123,
                        help='red mean value')
    parser.add_argument('--mean-g', dest='mean_g', type=float, default=117,
                        help='green mean value')
    parser.add_argument('--mean-b', dest='mean_b', type=float, default=104,
                        help='blue mean value')
    parser.add_argument('--lr-epoch', dest='lr_refactor_epoch', type=int, default=50,
                        help='refactor learning rate every N epoch')
    parser.add_argument('--lr-ratio', dest='lr_refactor_ratio', type=float, default=0.9,
                        help='ratio to refactor learning rate')
    parser.add_argument('--log', dest='log_file', type=str, default="train.log",
                        help='save training log to file')
    parser.add_argument('--monitor', dest='monitor', type=int, default=0,
                        help='log network parameters every N iters if larger than 0')
    args = parser.parse_args()
    return args

if __name__ == '__main__':
    args = parse_args()
    ctx = [mx.gpu(int(i)) for i in args.gpus.split(',')]
    ctx = mx.cpu() if not ctx else ctx
    train_net(args.network, args.dataset, args.image_set, args.year,
              args.devkit_path, args.batch_size,
              args.data_shape, [args.mean_r, args.mean_g, args.mean_b],
              args.resume, args.finetune, args.pretrained,
              args.epoch, args.prefix, ctx, args.begin_epoch, args.end_epoch,
              args.frequent, args.learning_rate, args.momentum, args.weight_decay,
              args.val_image_set, args.val_year, args.lr_refactor_epoch,
              args.lr_refactor_ratio, args.monitor, args.log_file)

from copy import deepcopy
from ditk import logging
from ding.model import DQN
from ding.policy import DQNPolicy
from ding.envs import DingEnvWrapper, SubprocessEnvManagerV2
from ding.data import DequeBuffer
from ding.config import compile_config
from ding.framework import task
from ding.framework.context import OnlineRLContext
from ding.framework.middleware import OffPolicyLearner, StepCollector, interaction_evaluator, data_pusher, \
    eps_greedy_handler, CkptSaver, nstep_reward_enhancer, termination_checker
from ding.utils import set_pkg_seed
from dizoo.atari.envs.atari_env import AtariEnv
from dizoo.atari.config.serial.pong.pong_dqn_config import main_config, create_config


def main():
    logging.getLogger().setLevel(logging.INFO)
    cfg = compile_config(main_config, create_cfg=create_config, auto=True)
    with task.start(async_mode=False, ctx=OnlineRLContext()):
        collector_cfg = deepcopy(cfg.env)
        collector_cfg.is_train = True
        evaluator_cfg = deepcopy(cfg.env)
        evaluator_cfg.is_train = False
        collector_env = SubprocessEnvManagerV2(
            env_fn=[lambda: AtariEnv(collector_cfg) for _ in range(cfg.env.collector_env_num)], cfg=cfg.env.manager
        )
        evaluator_env = SubprocessEnvManagerV2(
            env_fn=[lambda: AtariEnv(evaluator_cfg) for _ in range(cfg.env.evaluator_env_num)], cfg=cfg.env.manager
        )

        set_pkg_seed(cfg.seed, use_cuda=cfg.policy.cuda)

        model = DQN(**cfg.policy.model)
        buffer_ = DequeBuffer(size=cfg.policy.other.replay_buffer.replay_buffer_size)
        policy = DQNPolicy(cfg.policy, model=model)

        task.use(interaction_evaluator(cfg, policy.eval_mode, evaluator_env))
        task.use(eps_greedy_handler(cfg))
        task.use(StepCollector(cfg, policy.collect_mode, collector_env))
        task.use(nstep_reward_enhancer(cfg))
        task.use(data_pusher(cfg, buffer_))
        task.use(OffPolicyLearner(cfg, policy.learn_mode, buffer_))
        task.use(CkptSaver(cfg, policy, train_freq=1000))
        task.use(termination_checker(max_train_iter=int(1e7)))
        task.run()


if __name__ == "__main__":
    main()

import io
import numpy as np
import json
import requests
import h5py


seq_len = 100
file_name = "output_video.mp4"
data_file = "flame_params.hdf5"


def byteify(x):
    memfile = io.BytesIO()
    np.save(memfile, x)
    memfile.seek(0)
    return memfile.read().decode("latin-1")


def get_face(x, seq_len):
    return {
        "expression": byteify(x["tf_exp"][:seq_len]),
        "pose": byteify(x["tf_pose"][:seq_len]),
        "shape": byteify(x["tf_shape"][:seq_len]),
        "rotation": byteify(x["tf_rot"][:seq_len]),
    }


with h5py.File(data_file, "r") as f:
    p1 = f["sessions/1/participants/P1"]
    p2 = f["sessions/1/participants/P2"]

    serialized = json.dumps(
        {
            "seqs": [get_face(p1, seq_len), get_face(p2, seq_len)],
            "file_name": file_name,
            "fps": 25,
        }
    )
try:
    resp = requests.post("http://localhost:8000/render", data=serialized, timeout=600)
    resp.raise_for_status()
    print(resp.json())
except requests.exceptions.HTTPError:
    print("render request: failed on the server..")
except requests.exceptions.Timeout:
    print("render request: timed out")
except requests.exceptions.ConnectionError:
    print("render request: connection error")

import pyspark.sql.types

firmware_cpes_schema = pyspark.sql.types.StructType([
    pyspark.sql.types.StructField('cpe', pyspark.sql.types.StringType()),
    pyspark.sql.types.StructField('firmware_hash', pyspark.sql.types.StringType()),
    pyspark.sql.types.StructField('evidence', pyspark.sql.types.StructType([
        pyspark.sql.types.StructField('type', pyspark.sql.types.StringType()),
        pyspark.sql.types.StructField('firmware_hash', pyspark.sql.types.StringType()),
        pyspark.sql.types.StructField('product', pyspark.sql.types.StringType()),
        pyspark.sql.types.StructField('version', pyspark.sql.types.StringType()),
    ]))
])


import six

from odin import bases
from odin import resources, ResourceAdapter
from odin.utils import getmeta


TYPE_SERIALIZERS = {}


class OdinEncoder(object):
    def __init__(self, include_virtual_fields=True, include_type_field=True):
        self.include_virtual_fields = include_virtual_fields
        self.include_type_field = include_type_field

    def default(self, o):
        if isinstance(o, (resources.ResourceBase, ResourceAdapter)):
            meta = getmeta(o)
            obj = o.to_dict(self.include_virtual_fields)
            if self.include_type_field:
                obj[meta.type_field] = meta.resource_name
            return obj
        elif isinstance(o, bases.ResourceIterable):
            return list(o)
        elif o.__class__ in TYPE_SERIALIZERS:
            return TYPE_SERIALIZERS[o.__class__](o)
        return o

    def encode(self, o):
        _encoder = _make_encoder(self.default)
        return _encoder(o)


load = resources.build_object_graph
load.__doc__ = """
    Load a :py:class:`dict` into a Resource structure.

    This method is an alias of :py:func:`odin.

    :param d: Dict to load

    :param resource: A resource type, resource name or list of resources and names to use as the base for creating a
        resource. If a list is supplied the first item will be used if a resource type is not supplied.
    :raises ValidationError: During building of the object graph and issues discovered are raised as a ValidationError.

    """


def dump(resource, cls=OdinEncoder, **kwargs):
    """
    Dump a resource structure into a nested :py:class:`dict`.

    While a resource includes a *to_dict* method this method is not recursive. The dict codec recursively iterates
    through the resource structure to produce a full dict. This is useful for testing for example.

    :param resource: The root resource to dump
    :param cls: Encoder class to utilise
    :return:

    """
    encoder = cls(**kwargs)
    return encoder.encode(resource)


def _make_encoder(_default):
    def _encode_list(lst):
        return [_encode(o) for o in lst]

    def _encode_dict(dct):
        return {k: _encode(o) for k, o in six.iteritems(dct)}

    def _encode(o):
        if isinstance(o, (list, tuple)):
            return _encode_list(o)
        elif isinstance(o, dict):
            return _encode_dict(o)
        else:
            o = _default(o)
            if isinstance(o, (list, tuple, dict)):
                return _encode(o)
            return o
    return _encode

import logging
import time
import os
from collections import OrderedDict
from copy import deepcopy
from typing import Tuple

import numpy as np
from tqdm.auto import tqdm

from .reporter import FakeReporter
from ..searcher import searcher_factory
from ..searcher.local_searcher import LocalSearcher
from ..utils import EasyDict

logger = logging.getLogger(__name__)


class LocalReporter:
    """
    Reporter implementation for LocalSequentialScheduler
    """

    def __init__(self, trial, searcher_config, training_history: dict, config_history: dict):
        self.trial = trial
        self.training_history = training_history
        self.training_history[trial] = []
        self.searcher_config = deepcopy(searcher_config)
        self.config_history = config_history
        self.trial_started = time.time()
        self.last_reported_time = self.trial_started
        self.last_result = None

    def __call__(self, *args, **kwargs):
        result = deepcopy(kwargs)
        if 'done' not in result:
            result['trial'] = self.trial

            now = time.time()
            result['time_this_iter'] = now - self.last_reported_time
            result['time_since_start'] = now - self.trial_started
            self.last_reported_time = now

            self.training_history[self.trial].append(result)

            if self.trial not in self.config_history:
                self.config_history[self.trial] = self.searcher_config
                if 'util_args' in self.searcher_config:
                    self.searcher_config.pop('util_args')

            self.last_result = result

    def terminate(self):
        pass  # compatibility


class LocalSequentialScheduler(object):
    """ Simple scheduler which schedules all HPO jobs in sequence without any parallelism.
    The next trial scheduling will be decided based on the available time left withing `time_out` setting
    and average time required for a trial to complete multiplied by the fill_factor (0.95) by default to
    accommodate variance in runtimes per HPO run.

    Parameters
    ----------
    train_fn : callable
        A task launch function for training.
    resource : dict
        Computation resources. For example, `{'num_cpus':2, 'num_gpus':1}`
    searcher : str
        Searcher (get_config decisions). If str, this is passed to
        searcher_factory along with search_options.
    search_options : dict
        If searcher is str, these arguments are passed to searcher_factory.
    num_trials : int
        Maximum number of jobs run in experiment. One of `num_trials`,
        `time_out` must be given.
    time_out : float
        If given, jobs are started only until this time_out (wall clock time).
        One of `num_trials`, `time_out` must be given.
    reward_attr : str
        Name of reward (i.e., metric to maximize) attribute in data obtained
        from reporter
    time_attr : str
        Name of resource (or time) attribute in data obtained from reporter.
        This attribute is optional for FIFO scheduling, but becomes mandatory
        in multi-fidelity scheduling (e.g., Hyperband).
        Note: The type of resource must be int.
    """

    def __init__(self, train_fn, search_space, util_args=None, searcher='auto', reward_attr='reward', resource=None, **kwargs):
        self.train_fn = train_fn
        self.training_history = None
        self.config_history = None
        self._reward_attr = reward_attr
        self.time_attr = kwargs.get('time_attr', None)
        self.resource = resource
        self.max_reward = kwargs.get('max_reward', None)
        self.searcher: LocalSearcher = self.get_searcher_(searcher, train_fn, search_space=search_space, **kwargs)
        self.init_limits_(kwargs)
        self.util_args = util_args
        self.metadata = {
            'search_space': search_space,
            'search_strategy': self.searcher,
            'stop_criterion': {
                'time_limits': self.time_out,
                'max_reward': self.max_reward},
            'resources_per_trial': self.resource
        }

    def init_limits_(self, kwargs):
        if kwargs.get('num_trials', None) is None:
            assert kwargs.get('time_out', None) is not None, "Need stopping criterion: Either num_trials or time_out"
        self.num_trials = kwargs.get('num_trials', 9999)
        self.time_out = kwargs.get('time_out', None)
        if self.num_trials is None:
            assert self.time_out is not None, "Need stopping criterion: Either num_trials or time_out"

    def get_searcher_(self, searcher, train_fn, search_space, **kwargs) -> LocalSearcher:
        scheduler_opts = {}
        if searcher == 'auto':
            searcher = 'local_random'
            scheduler_opts = {'scheduler': 'local'}
        elif searcher == 'random':
            # FIXME: Hack to be compatible with gluoncv
            searcher = 'local_random'

        search_options = kwargs.get('search_options', None)
        if isinstance(searcher, str):
            if search_options is None:
                search_options = dict()
            _search_options = search_options.copy()
            if searcher.startswith('local_'):
                _search_options['search_space'] = search_space
            else:
                _search_options['configspace'] = train_fn.cs
                _search_options['resource_attribute'] = kwargs.get('time_attr', None)
            _search_options['reward_attribute'] = self._reward_attr
            # Adjoin scheduler info to search_options, if not already done by
            # subclass
            if 'scheduler' not in _search_options:
                _search_options['scheduler'] = 'local'
            searcher = searcher_factory(searcher, **{**scheduler_opts, **_search_options})
        else:
            assert isinstance(searcher, LocalSearcher)
        return searcher

    def run(self, **kwargs):
        """Run multiple trials given specific time and trial numbers limits.
        """
        self.searcher.configure_scheduler(self)

        self.training_history = OrderedDict()
        self.config_history = OrderedDict()

        trial_run_times = []
        time_start = time.time()

        r = range(self.num_trials)
        for i in (tqdm(r) if self.num_trials < 1000 else r):
            trial_start_time = time.time()
            try:
                is_failed, result = self.run_trial(task_id=i)
            except Exception:
                # TODO: Add special exception type when there are no more new configurations to try (exhausted search space)
                logger.log(30, f'\tWARNING: Encountered unexpected exception during trial {i}, stopping HPO early.')
                logger.exception('Detailed Traceback:')  # TODO: Avoid logging if verbosity=0
                break
            trial_end_time = time.time()
            trial_run_times.append(np.NaN if is_failed else (trial_end_time - trial_start_time))

            if self.max_reward and self.get_best_reward() >= self.max_reward:
                logger.log(20, f'\tMax reward is reached')
                break

            if self.time_out is not None:
                avg_trial_run_time = np.nanmean(trial_run_times)
                avg_trial_run_time = 0 if np.isnan(avg_trial_run_time) else avg_trial_run_time
                if not self.has_enough_time_for_trial_(self.time_out, time_start, trial_start_time, trial_end_time, avg_trial_run_time):
                    logger.log(20, f'\tTime limit exceeded')
                    break

    @classmethod
    def has_enough_time_for_trial_(cls, time_out, time_start, trial_start_time, trial_end_time, avg_trial_run_time, fill_factor=0.95):
        """
        Checks if the remaining time is enough to run another trial.

        Parameters
        ----------
        time_out total
            timeout in m
        time_start
            trials start time
        trial_start_time
            last trial start time
        trial_end_time
            last trial end time
        avg_trial_run_time
            running average of all trial runs
        fill_factor: float
            discount of `avg_trial_run_time` allowed for a next trial. Default is 0.95 of `avg_trial_run_time`

        Returns
        -------
            True if there is enough time to run another trial give runs statistics and remaining time

        """
        time_spent = trial_end_time - time_start
        is_timeout_exceeded = time_spent >= time_out
        time_left = time_start + time_out - trial_end_time
        is_enough_time_for_another_trial = True
        if avg_trial_run_time:
            is_enough_time_for_another_trial = time_left > avg_trial_run_time * fill_factor
        return is_enough_time_for_another_trial and not is_timeout_exceeded

    @classmethod
    def get_average_trial_time_(cls, i, avg_trial_run_time, trial_start_time, time_end):
        trial_time = time_end - trial_start_time
        if avg_trial_run_time is None:
            avg_trial_run_time = trial_time
        else:
            avg_trial_run_time = ((avg_trial_run_time * i) + trial_time) / (i + 1)
        return avg_trial_run_time

    def run_trial(self, task_id=0) -> Tuple[bool, dict]:
        """
        Start a trial with a given task_id

        Parameters
        ----------
        task_id
            task

        Returns
        -------
        is_failed: bool
            True if task completed successfully
        trial_start_time
            Trial start time
        trial_end_time
            Trial end time

        """
        new_searcher_config = self.searcher.get_config()
        searcher_config = deepcopy(self.metadata['search_space'])
        searcher_config.update(new_searcher_config)
        reporter = LocalReporter(task_id, searcher_config, self.training_history, self.config_history)
        return self.run_job_(task_id, searcher_config, reporter)

    def run_job_(self, task_id, searcher_config, reporter):
        args = dict()
        if self.util_args is not None:
            args['util_args'] = deepcopy(self.util_args)
        args.update(searcher_config)

        args['task_id'] = task_id
        args = EasyDict(args)  # TODO: Remove, currently used for compatibility with gluoncv
        self.searcher.register_pending(searcher_config)
        is_failed = False
        try:
            result = self.train_fn(args, reporter=reporter)
            if type(reporter) is not FakeReporter and reporter.last_result:
                self.searcher.update(config=searcher_config, **reporter.last_result)
        except Exception as e:
            logger.error(f'Exception during a trial: {e}')
            self.searcher.evaluation_failed(config=searcher_config)
            reporter(traceback=e)
            is_failed = True
            result = {'traceback': str(e)}
        return is_failed, result

    def run_with_config(self, config):
        """Run with config for final fit.
        It launches a single training trial under any fixed values of the hyperparameters.
        For example, after HPO has identified the best hyperparameter values based on a hold-out dataset,
        one can use this function to retrain a model with the same hyperparameters on all the available labeled data
        (including the hold out set). It can also returns other objects or states.
        """
        is_failed, result = self.run_job_('run_with_config', config, FakeReporter())
        return result

    def join_jobs(self, timeout=None):
        pass  # Compatibility

    def get_best_config(self):
        """Get the best configuration from the finished jobs.
        """
        return self.searcher.get_best_config()

    def get_best_reward(self):
        """Get the best reward from the finished jobs.
        """
        return self.searcher.get_best_reward()

    def get_training_curves(self, filename=None, plot=False, use_legend=True):
        """Get Training Curves
        """
        if filename is None and not plot:
            logger.warning('Please either provide filename or allow plot in get_training_curves')
        import matplotlib.pyplot as plt

        eval_metric = self.__get_training_history_metric('eval_metric', default='validation_performance')
        sign_mult = int(self.__get_training_history_metric('greater_is_better', default=True)) * 2 - 1

        plt.ylabel(eval_metric)
        plt.xlabel(self.time_attr)
        plt.title("Performance vs Training-Time in each HPO Trial")
        for task_id, task_res in self.training_history.items():
            rewards = [x[self._reward_attr] * sign_mult for x in task_res]
            x = [x[self.time_attr] for x in task_res]
            plt.plot(x, rewards, label=f'task {task_id}')
        if use_legend:
            plt.legend(loc='best')
        if filename:
            logger.info(f'Saving Training Curve in {filename}')
            file_dir = os.path.split(os.path.abspath(filename))[0]
            if not os.path.exists(file_dir):
                os.makedirs(file_dir)
            plt.savefig(filename)
        if plot:
            plt.show()

    def __get_training_history_metric(self, metric, default=None):
        for _, task_res in self.training_history.items():
            if task_res and metric in task_res[0]:
                return task_res[0][metric]
        return default

    def get_best_task_id(self):
        """Get the task id that results in the best configuration/best reward.

        If there are duplicated configurations, we return the id of the first one.
        """
        best_config = self.get_best_config()
        for task_id, config in self.config_history.items():
            if best_config == config:
                return task_id
        raise RuntimeError('The best config {} is not found in config history = {}. '
                           'This should never happen!'.format(best_config, self.config_history))

from django.apps import AppConfig


class LogicConfig(AppConfig):
    default_auto_field = "django.db.models.BigAutoField"
    name = "Logic"

import abc
from typing import Any, Generic, TypeVar
from types import SimpleNamespace

from amino import List, Lists, Nil, Maybe
from amino.util.string import ToStr

A = TypeVar('A')


def is_algebra(bases: List[type]) -> bool:
    return bases.exists(lambda a: hasattr(a, '__algebra_base__'))


def find_algebra(name: str, bases: List[type]) -> Maybe[type]:
    return bases.find(lambda a: hasattr(a, '__algebra_variants__'))


def setup_algebra(name: str, inst: type, bases: List[type]) -> None:
    if is_algebra(bases):
        inst.__algebra_variants__ = List()
    else:
        raise Exception(f'algebra subclass has no algebra superclass: {name}')


def setup_variant(name: str, inst: type, bases: List[type], algebra: type) -> None:
    inst.__algebra_index__ = len(algebra.__algebra_variants__)
    algebra.__algebra_variants__.append(inst)


def setup_algebraic_type(name: str, inst: type, bases: List[type]) -> None:
    return (
        find_algebra(name, bases)
        .cata_f(
            lambda a: setup_variant(name, inst, bases, a),
            lambda: setup_algebra(name, inst, bases)
        )
    )


class AlgebraMeta(abc.ABCMeta):

    def __new__(
            cls,
            name: str,
            bases: list,
            namespace: SimpleNamespace,
            algebra_base: bool=False,
            **kw: Any,
    ) -> None:
        inst = super().__new__(cls, name, bases, namespace, **kw)
        if not hasattr(inst, '__args__') or inst.__args__ is None:
            if algebra_base:
                inst.__algebra_base__ = None
            else:
                setup_algebraic_type(name, inst, Lists.wrap(bases))
        return inst


class Algebra(Generic[A], ToStr, metaclass=AlgebraMeta, algebra_base=True):
    pass


__all__ = ('AlgebraMeta', 'Algebra')

# -*- coding: utf-8 -*-
# Copyright 2019 Cohesity Inc.

class Type21Enum(object):

    """Implementation of the 'Type21' enum.

    Specifies the type of the CloudDeploy target.
    'kAzure' indicates that Azure as a cloud deploy target type.
    'kAws' indicates that AWS as a cloud deploy target type.
    'kGcp' indicates that GCP as a cloud deploy target type.

    Attributes:
        KAZURE: TODO: type description here.
        KAWS: TODO: type description here.
        KGCP: TODO: type description here.

    """

    KAZURE = 'kAzure'

    KAWS = 'kAws'

    KGCP = 'kGcp'


"""
A simple IO staging mechanism



"""

#-----------------------------------------------------------------------------
# Copyright (c) 2013, yt Development Team.
#
# Distributed under the terms of the Modified BSD License.
#
# The full license is in the file COPYING.txt, distributed with this software.
#-----------------------------------------------------------------------------

import np
from yt.utilities.logger import ytLogger as mylog
from .parallel_analysis_interface import \
    ProcessorPool, parallel_objects
from yt.utilities.io_handler import BaseIOHandler
from contextlib import contextmanager
import time

try:
    from .parallel_analysis_interface import MPI
except ImportError:
    pass

YT_TAG_MESSAGE = 317 # Cell 317 knows where to go

class IOCommunicator(BaseIOHandler):
    def __init__(self, ds, wg, pool):
        mylog.info("Initializing IOCommunicator")
        self.ds = ds
        self.wg = wg # We don't need to use this!
        self.pool = pool
        self.comm = pool.comm
        # We read our grids here
        self.grids = []
        storage = {}
        grids = ds.index.grids.tolist()
        grids.sort(key=lambda a:a.filename)
        for sto, g in parallel_objects(grids, storage = storage):
            sto.result = self.comm.rank
            sto.result_id = g.id
            self.grids.append(g)
        self._id_offset = ds.index.grids[0]._id_offset
        mylog.info("Reading from disk ...")
        self.initialize_data()
        mylog.info("Broadcasting ...")
        self.comm.comm.bcast(storage, root = wg.ranks[0])
        mylog.info("Done.")
        self.hooks = []

    def initialize_data(self):
        ds = self.ds
        fields = [f for f in ds.field_list
                  if not ds.field_info[f].particle_type]
        pfields = [f for f in ds.field_list
                   if ds.field_info[f].particle_type]
        # Preload is only defined for Enzo ...
        if ds.index.io._dataset_type == "enzo_packed_3d":
            self.queue = ds.index.io.queue
            ds.index.io.preload(self.grids, fields)
            for g in self.grids:
                for f in fields:
                    if f not in self.queue[g.id]:
                        d = np.zeros(g.ActiveDimensions, dtype='float64')
                        self.queue[g.id][f] = d
                for f in pfields:
                    self.queue[g.id][f] = self._read(g, f)
        else:
            self.queue = {}
            for g in self.grids:
                for f in fields + pfields:
                    self.queue[g.id][f] = ds.index.io._read(g, f)

    def _read(self, g, f):
        fi = self.ds.field_info[f]
        if fi.particle_type and g.NumberOfParticles == 0:
            # because this gets upcast to float
            return np.array([],dtype='float64')
        try:
            temp = self.ds.index.io._read_data_set(g, f)
        except:# self.ds.index.io._read_exception as exc:
            if fi.not_in_all:
                temp = np.zeros(g.ActiveDimensions, dtype='float64')
            else:
                raise
        return temp

    def wait(self):
        status = MPI.Status()
        while 1:
            if self.comm.comm.Iprobe(MPI.ANY_SOURCE,
                                YT_TAG_MESSAGE,
                                status = status):
                msg = self.comm.comm.recv(
                        source = status.source, tag = YT_TAG_MESSAGE)
                if msg['op'] == "end":
                    mylog.debug("Shutting down IO.")
                    break
                self._send_data(msg, status.source)
                status = MPI.Status()
            else:
                time.sleep(1e-2)

    def _send_data(self, msg, dest):
        grid_id = msg['grid_id']
        field = msg['field']
        ts = self.queue[grid_id][field].astype("float64")
        mylog.debug("Opening send to %s (%s)", dest, ts.shape)
        self.hooks.append(self.comm.comm.Isend([ts, MPI.DOUBLE], dest = dest))

class IOHandlerRemote(BaseIOHandler):
    _dataset_type = "remote"

    def __init__(self, ds, wg, pool):
        self.ds = ds
        self.wg = wg # probably won't need
        self.pool = pool
        self.comm = pool.comm
        self.proc_map = self.comm.comm.bcast(None,
                root = pool['io'].ranks[0])
        super(IOHandlerRemote, self).__init__()

    def _read_data_set(self, grid, field):
        dest = self.proc_map[grid.id]
        msg = dict(grid_id = grid.id, field = field, op="read")
        mylog.debug("Requesting %s for %s from %s", field, grid, dest)
        if self.ds.field_info[field].particle_type:
            data = np.empty(grid.NumberOfParticles, 'float64')
        else:
            data = np.empty(grid.ActiveDimensions, 'float64')
        hook = self.comm.comm.Irecv([data, MPI.DOUBLE], source = dest)
        self.comm.comm.send(msg, dest = dest, tag = YT_TAG_MESSAGE)
        mylog.debug("Waiting for data.")
        MPI.Request.Wait(hook)
        return data

    def _read_data_slice(self, grid, field, axis, coord):
        sl = [slice(None), slice(None), slice(None)]
        sl[axis] = slice(coord, coord + 1)
        #sl = tuple(reversed(sl))
        return self._read_data_set(grid,field)[sl]

    def terminate(self):
        msg = dict(op='end')
        if self.wg.comm.rank == 0:
            for rank in self.pool['io'].ranks:
                mylog.debug("Sending termination message to %s", rank)
                self.comm.comm.send(msg, dest=rank, tag=YT_TAG_MESSAGE)

@contextmanager
def remote_io(ds, wg, pool):
    original_io = ds.index.io
    ds.index.io = IOHandlerRemote(ds, wg, pool)
    yield
    ds.index.io.terminate()
    ds.index.io = original_io

def io_nodes(fn, n_io, n_work, func, *args, **kwargs):
    from yt.mods import load
    pool, wg = ProcessorPool.from_sizes([(n_io, "io"), (n_work, "work")])
    rv = None
    if wg.name == "work":
        ds = load(fn)
        with remote_io(ds, wg, pool):
            rv = func(ds, *args, **kwargs)
    elif wg.name == "io":
        ds = load(fn)
        io = IOCommunicator(ds, wg, pool)
        io.wait()
    # We should broadcast the result
    rv = pool.comm.mpi_bcast(rv, root=pool['work'].ranks[0])
    pool.free_all()
    mylog.debug("Return value: %s", rv)
    return rv

# Here is an example of how to use this functionality.
if __name__ == "__main__":
    def gq(ds):
        dd = ds.all_data()
        return dd.quantities["TotalQuantity"]("CellMassMsun")
    q = io_nodes("DD0087/DD0087", 8, 24, gq)
    mylog.info(q)



# -*- coding: UTF-8 -*-
#addonHandler.py
#A part of NonVisual Desktop Access (NVDA)
#Copyright (C) 2012-2014 Rui Batista, NV Access Limited, Noelia Ruiz Martnez
#This file is covered by the GNU General Public License.
#See the file COPYING for more details.

import sys
import os.path
import gettext
import glob
import tempfile
import cPickle
import inspect
import itertools
import collections
import pkgutil
import shutil
from cStringIO import StringIO
import zipfile

from configobj import ConfigObj, ConfigObjError
from validate import Validator

import config
import globalVars
import languageHandler
from logHandler import log
import winKernel

MANIFEST_FILENAME = "manifest.ini"
stateFilename="addonsState.pickle"
BUNDLE_EXTENSION = "nvda-addon"
BUNDLE_MIMETYPE = "application/x-nvda-addon"
NVDA_ADDON_PROG_ID = "NVDA.Addon.1"
ADDON_PENDINGINSTALL_SUFFIX=".pendingInstall"
DELETEDIR_SUFFIX=".delete"

state={}

def loadState():
	global state
	statePath=os.path.join(globalVars.appArgs.configPath,stateFilename)
	try:
		state = cPickle.load(file(statePath, "r"))
	except:
		# Defaults.
		state = {
			"pendingRemovesSet":set(),
			"pendingInstallsSet":set(),
		}

def saveState():
	statePath=os.path.join(globalVars.appArgs.configPath,stateFilename)
	try:
		cPickle.dump(state, file(statePath, "wb"))
	except:
		log.debugWarning("Error saving state", exc_info=True)

def getRunningAddons():
	""" Returns currently loaded addons.
	"""
	return (addon for addon in getAvailableAddons() if addon.isRunning)

def completePendingAddonRemoves():
	"""Removes any addons that could not be removed on the last run of NVDA"""
	user_addons = os.path.abspath(os.path.join(globalVars.appArgs.configPath, "addons"))
	pendingRemovesSet=state['pendingRemovesSet']
	for addonName in list(pendingRemovesSet):
		addonPath=os.path.join(user_addons,addonName)
		if os.path.isdir(addonPath):
			addon=Addon(addonPath)
			try:
				addon.completeRemove()
			except RuntimeError:
				log.exception("Failed to remove %s add-on"%addonName)
				continue
		pendingRemovesSet.discard(addonName)

def completePendingAddonInstalls():
	user_addons = os.path.abspath(os.path.join(globalVars.appArgs.configPath, "addons"))
	pendingInstallsSet=state['pendingInstallsSet']
	for addonName in pendingInstallsSet:
		newPath=os.path.join(user_addons,addonName)
		oldPath=newPath+ADDON_PENDINGINSTALL_SUFFIX
		try:
			os.rename(oldPath,newPath)
		except:
			log.error("Failed to complete addon installation for %s"%addonName,exc_info=True)
	pendingInstallsSet.clear()

def removeFailedDeletions():
	user_addons = os.path.abspath(os.path.join(globalVars.appArgs.configPath, "addons"))
	for p in os.listdir(user_addons):
		if p.endswith(DELETEDIR_SUFFIX):
			path=os.path.join(user_addons,p)
			shutil.rmtree(path,ignore_errors=True)
			if os.path.exists(path):
				log.error("Failed to delete path %s, try removing manually"%path)

def initialize():
	""" Initializes the add-ons subsystem. """
	loadState()
	removeFailedDeletions()
	completePendingAddonRemoves()
	completePendingAddonInstalls()
	saveState()
	getAvailableAddons(refresh=True)

def terminate():
	""" Terminates the add-ons subsystem. """
	pass

def _getDefaultAddonPaths():
	""" Returns paths where addons can be found.
	For now, only <userConfig\addons is supported.
	@rtype: list(string)
	"""
	addon_paths = []
	user_addons = os.path.abspath(os.path.join(globalVars.appArgs.configPath, "addons"))
	if os.path.isdir(user_addons):
		addon_paths.append(user_addons)
	return addon_paths

def _getAvailableAddonsFromPath(path):
	""" Gets available add-ons from path.
	An addon is only considered available if the manifest file is loaded with no errors.
	@param path: path from where to find addon directories.
	@type path: string
	@rtype generator of Addon instances
	"""
	log.debug("Listing add-ons from %s", path)
	for p in os.listdir(path):
		if p.endswith(DELETEDIR_SUFFIX): continue
		addon_path = os.path.join(path, p)
		if os.path.isdir(addon_path) and addon_path not in ('.', '..'):
			log.debug("Loading add-on from %s", addon_path)
			try:
				a = Addon(addon_path)
				log.debug("Found add-on %s", a.manifest['name'])
				yield a
			except:
				log.error("Error loading Addon from path: %s", addon_path, exc_info=True)

_availableAddons = collections.OrderedDict()
def getAvailableAddons(refresh=False):
	""" Gets all available addons on the system.
	@rtype generator of Addon instances.
	"""
	if refresh:
		_availableAddons.clear()
		generators = [_getAvailableAddonsFromPath(path) for path in _getDefaultAddonPaths()]
		for addon in itertools.chain(*generators):
			_availableAddons[addon.path] = addon
	return _availableAddons.itervalues()

def installAddonBundle(bundle):
	"""Extracts an Addon bundle in to a unique subdirectory of the user addons directory, marking the addon as needing install completion on NVDA restart."""
	addonPath = os.path.join(globalVars.appArgs.configPath, "addons",bundle.manifest['name']+ADDON_PENDINGINSTALL_SUFFIX)
	bundle.extract(addonPath)
	addon=Addon(addonPath)
	# #2715: The add-on must be added to _availableAddons here so that
	# translations can be used in installTasks module.
	_availableAddons[addon.path]=addon
	try:
		addon.runInstallTask("onInstall")
	except:
		log.error("task 'onInstall' on addon '%s' failed"%addon.name,exc_info=True)
		del _availableAddons[addon.path]
		addon.completeRemove(runUninstallTask=False)
		raise AddonError("Installation failed")
	state['pendingInstallsSet'].add(bundle.manifest['name'])
	saveState()
	return addon

class AddonError(Exception):
	""" Represents an exception coming from the addon subsystem. """


class Addon(object):
	""" Represents an Add-on available on the file system."""
	def __init__(self, path):
		""" Constructs an L[Addon} from.
		@param path: the base directory for the addon data.
		@type path: string
		"""
		self.path = os.path.abspath(path)
		self._extendedPackages = set()
		self._isLoaded = False
		manifest_path = os.path.join(path, MANIFEST_FILENAME)
		with open(manifest_path) as f:
			translatedInput = None
			for translatedPath in _translatedManifestPaths():
				p = os.path.join(self.path, translatedPath)
				if os.path.exists(p):
					log.debug("Using manifest translation from %s", p)
					translatedInput = open(p, 'r')
					break
			self.manifest = AddonManifest(f, translatedInput)

	@property
	def isPendingInstall(self):
		"""True if this addon has not yet been fully installed."""
		return self.path.endswith(ADDON_PENDINGINSTALL_SUFFIX)

	@property
	def isPendingRemove(self):
		"""True if this addon is marked for removal."""
		return not self.isPendingInstall and self.name in state['pendingRemovesSet']

	def requestRemove(self):
		"""Markes this addon for removal on NVDA restart."""
		if self.isPendingInstall:
			self.completeRemove()
			state['pendingInstallsSet'].discard(self.name)
			#Force availableAddons to be updated
			getAvailableAddons(refresh=True)
		else:
			state['pendingRemovesSet'].add(self.name)
		saveState()

	def completeRemove(self,runUninstallTask=True):
		if runUninstallTask:
			try:
				# #2715: The add-on must be added to _availableAddons here so that
				# translations can be used in installTasks module.
				_availableAddons[self.path] = self
				self.runInstallTask("onUninstall")
			except:
				log.error("task 'onUninstall' on addon '%s' failed"%self.name,exc_info=True)
			finally:
				del _availableAddons[self.path]
		tempPath=tempfile.mktemp(suffix=DELETEDIR_SUFFIX,dir=os.path.dirname(self.path))
		try:
			os.rename(self.path,tempPath)
		except (WindowsError,IOError):
			raise RuntimeError("Cannot rename add-on path for deletion")
		shutil.rmtree(tempPath,ignore_errors=True)
		if os.path.exists(tempPath):
			log.error("Error removing addon directory %s, deferring until next NVDA restart"%self.path)

	@property
	def name(self):
		return self.manifest['name']

	def addToPackagePath(self, package):
		""" Adds this L{Addon} extensions to the specific package path if those exist.
		@param package: the python module representing the package.
		@type package: python module.
		"""
		extension_path = os.path.join(self.path, package.__name__)
		if not os.path.isdir(extension_path):
			# This addon does not have extension points for this package
			return
		# Python 2.x doesn't properly handle unicode import paths, so convert them before adding.
		converted_path = self._getPathForInclusionInPackage(package)
		package.__path__.insert(0, converted_path)
		self._extendedPackages.add(package)
		log.debug("Addon %s added to %s package path", self.manifest['name'], package.__name__)

	@property
	def isRunning(self):
		return not self.isPendingInstall

	def _getPathForInclusionInPackage(self, package):
		extension_path = os.path.join(self.path, package.__name__)
		return extension_path.encode("mbcs")

	def loadModule(self, name):
		""" loads a python module from the addon directory
		@param name: the module name
		@type name: string
		@returns the python module with C[name}
		@rtype python module
		"""
		log.debug("Importing module %s from plugin %s", name, self.name)
		importer = pkgutil.ImpImporter(self.path)
		loader = importer.find_module(name)
		if not loader:
			return None
		# Create a qualified full name to avoid modules with the same name on sys.modules.
		fullname = "addons.%s.%s" % (self.name, name)
		try:
			return loader.load_module(fullname)
		except ImportError:
			# in this case return None, any other error throw to be handled elsewhere
			return None

	def getTranslationsInstance(self, domain='nvda'):
		""" Gets the gettext translation instance for this addon.
		<addon-path<\locale will be used to find .mo files, if exists.
		If a translation file is not found the default fallback null translation is returned.
		@param domain: the tranlation domain to retrieve. The 'nvda' default should be used in most cases.
		@returns: the gettext translation class.
		"""
		localedir = os.path.join(self.path, "locale")
		return gettext.translation(domain, localedir=localedir, languages=[languageHandler.getLanguage()], fallback=True)

	def runInstallTask(self,taskName,*args,**kwargs):
		"""
		Executes the function having the given taskName with the given args and kwargs in the addon's installTasks module if it exists.
		"""
		if not hasattr(self,'_installTasksModule'):
			self._installTasksModule=self.loadModule('installTasks')
		if self._installTasksModule:
			func=getattr(self._installTasksModule,taskName,None)
			if func:
				func(*args,**kwargs)

	def getDocFilePath(self, fileName=None):
		"""Get the path to a documentation file for this add-on.
		The file should be located in C{doc\lang\file} inside the add-on,
		where C{lang} is the language code and C{file} is the requested file name.
		Failing that, the language without country is tried.
		English is tried as a last resort.
		An add-on can specify a default documentation file name
		via the docFileName parameter in its manifest.
		@param fileName: The requested file name or C{None} for the add-on's default.
		@type fileName: basestring
		@return: The path to the requested file or C{None} if it wasn't found.
		@rtype: basestring
		"""
		if not fileName:
			fileName = self.manifest["docFileName"]
			if not fileName:
				return None
		docRoot = os.path.join(self.path, "doc")
		lang = languageHandler.getLanguage()
		langs = [lang]
		if "_" in lang:
			lang = lang.split("_", 1)[0]
			langs.append(lang)
		if lang != "en":
			langs.append("en")
		for lang in langs:
			docFile = os.path.join(docRoot, lang, fileName)
			if os.path.isfile(docFile):
				return docFile
		return None

def getCodeAddon(obj=None, frameDist=1):
	""" Returns the L{Addon} where C{obj} is defined. If obj is None the caller code frame is assumed to allow simple retrieval of "current calling addon".
	@param obj: python object or None for default behaviour.
	@param frameDist: howmany frames is the caller code. Only change this for functions in this module.
	@return: L{Addon} instance or None if no code does not belong to a add-on package.
	@rtype: C{Addon}
	"""
	global _availableAddons
	if obj is None:
		obj = sys._getframe(frameDist)
	fileName  = inspect.getfile(obj)
	dir= unicode(os.path.abspath(os.path.dirname(fileName)), "mbcs")
	# if fileName is not a subdir of one of the addon paths
	# It does not belong to an addon.
	for p in _getDefaultAddonPaths():
		if dir.startswith(p):
			break
	else:
		raise AddonError("Code does not belong to an addon package.")
	curdir = dir
	while curdir not in _getDefaultAddonPaths():
		if curdir in _availableAddons.keys():
			return _availableAddons[curdir]
		curdir = os.path.abspath(os.path.join(curdir, ".."))
	# Not found!
	raise AddonError("Code does not belong to an addon")

def initTranslation():
	addon = getCodeAddon(frameDist=2)
	translations = addon.getTranslationsInstance()
	# Point _ to the translation object in the globals namespace of the caller frame
	# FIXME: shall we retrieve the caller module object explicitly?
	try:
		callerFrame = inspect.currentframe().f_back
		callerFrame.f_globals['_'] = translations.ugettext
		# Install our pgettext function.
		callerFrame.f_globals['pgettext'] = languageHandler.makePgettext(translations)
	finally:
		del callerFrame # Avoid reference problems with frames (per python docs)

def _translatedManifestPaths(lang=None, forBundle=False):
	if lang is None:
		lang = languageHandler.getLanguage() # can't rely on default keyword arguments here.
	langs=[lang]
	if '_' in lang:
		langs.append(lang.split('_')[0])
		if lang!='en' and not lang.startswith('en_'):
			langs.append('en')
	sep = "/" if forBundle else os.path.sep
	return [sep.join(("locale", lang, MANIFEST_FILENAME)) for lang in langs]


class AddonBundle(object):
	""" Represents the contents of an NVDA addon suitable for distribution.
	The bundle is compressed using the zip file format. Manifest information
	is available without the need for extraction."""
	def __init__(self, bundlePath):
		""" Constructs an L{AddonBundle} from a filename.
		@param bundlePath: The path for the bundle file.
		"""
		self._path = bundlePath if isinstance(bundlePath, unicode) else unicode(bundlePath, "mbcs")
		# Read manifest:
		translatedInput=None
		with zipfile.ZipFile(self._path, 'r') as z:
			for translationPath in _translatedManifestPaths(forBundle=True):
				try:
					translatedInput = z.open(translationPath, 'r')
					break
				except KeyError:
					pass
			self._manifest = AddonManifest(z.open(MANIFEST_FILENAME), translatedInput=translatedInput)

	def extract(self, addonPath):
		""" Extracts the bundle content to the specified path.
		The addon will be extracted to L{addonPath}
		@param addonPath: Path where to extract contents.
		@type addonPath: string
		"""
		with zipfile.ZipFile(self._path, 'r') as z:
			for info in z.infolist():
				if isinstance(info.filename, str):
					# #2505: Handle non-Unicode file names.
					# Most archivers seem to use the local OEM code page, even though the spec says only cp437.
					# HACK: Overriding info.filename is a bit ugly, but it avoids a lot of code duplication.
					info.filename = info.filename.decode("cp%d" % winKernel.kernel32.GetOEMCP())
				z.extract(info, addonPath)

	@property
	def manifest(self):
		""" Gets the manifest for the represented Addon.
		@rtype: AddonManifest
		"""
		return self._manifest

	def __repr__(self):
		return "<AddonBundle at %s>" % self._path

def createAddonBundleFromPath(path, destDir=None):
	""" Creates a bundle from a directory that contains a a addon manifest file."""
	basedir = os.path.abspath(path)
	# If  caller did not provide a destination directory name
	# Put the bundle at the same level of the addon's top directory,
	# That is, basedir/..
	if destDir is None:
		destDir = os.path.dirname(basedir)
	manifest_path = os.path.join(basedir, MANIFEST_FILENAME)
	if not os.path.isfile(manifest_path):
		raise AddonError("Can't find %s manifest file." % manifest_path)
	with open(manifest_path) as f:
		manifest = AddonManifest(f)
	if manifest.errors is not None:
		_report_manifest_errors(manifest)
		raise AddonError("Manifest file as errors.")
	bundleFilename = "%s-%s.%s" % (manifest['name'], manifest['version'], BUNDLE_EXTENSION)
	bundleDestination = os.path.join(destDir, bundleFilename)
	with zipfile.ZipFile(bundleDestination, 'w') as z:
		# FIXME: the include/exclude feature may or may not be useful. Also python files can be pre-compiled.
		for dir, dirnames, filenames in os.walk(basedir):
			relativePath = os.path.relpath(dir, basedir)
			for filename in filenames:
				pathInBundle = os.path.join(relativePath, filename)
				absPath = os.path.join(dir, filename)
				z.write(absPath, pathInBundle)
	return AddonBundle(bundleDestination)


def _report_manifest_errors(manifest):
	log.warning("Error loading manifest:\n%s", manifest.errors)

class AddonManifest(ConfigObj):
	""" Add-on manifest file. It contains metadata about an NVDA add-on package. """
	configspec = ConfigObj(StringIO(
	"""
# NVDA Add-on Manifest configuration specification
# Add-on unique name
name = string()
# short  summary (label) of the add-on to show to users.
summary = string()
# Long description with further information and instructions
description = string(default=None)
# Name of the author or entity that created the add-on
author = string()
# Version of the add-on. Should preferably in some standard format such as x.y.z
version = string()
# URL for more information about the add-on. New versions and such.
url= string(default=None)
# Name of default documentation file for the add-on.
docFileName = string(default=None)

"""))

	def __init__(self, input, translatedInput=None):
		""" Constructs an L{AddonManifest} instance from manifest string data
		@param input: data to read the manifest informatinon
		@type input: a fie-like object.
		@param translatedInput: translated manifest input
		@type translatedInput: file-like object
		"""
		super(AddonManifest, self).__init__(input, configspec=self.configspec, encoding='utf-8', default_encoding='utf-8')
		self._errors = []
		val = Validator()
		result = self.validate(val, copy=True, preserve_errors=True)
		if result != True:
			self._errors = result
		self._translatedConfig = None
		if translatedInput is not None:
			self._translatedConfig = ConfigObj(translatedInput, encoding='utf-8', default_encoding='utf-8')
			for k in ('summary','description'):
				val=self._translatedConfig.get(k)
				if val:
					self[k]=val

	@property
	def errors(self):
		return self._errors

import gc
import hashlib
import itertools
import logging
import math
import sys
import traceback
from logging import warning, debug

import numpy as np
from pulp import LpProblem, LpMinimize, LpVariable, LpInteger, CPLEX, LpStatus

from src.dbms.utils import sql_get_all_attributes, sql_table_column_data_type
from src.paql.constraints import *
from src.paql.expression_trees.expression_trees import ArithmeticExpression
from src.paql.expression_trees.syntax_tree import Expression
from src.paql.objectives import *
from src.utils.utils import op_to_opstr



class NotPackageQueryException(Exception):
    pass


class PaQLParserError(Exception):
    pass






class PackageQuery(object):
    allowed_dbms_data_types = {
        "integer",
        "bigint",
        "double precision",
        # "numeric",
        # "numeric(15,2)"
    }

    @property
    def table_name(self):
        assert len(self.rel_namespace.values()) == 1
        return self.rel_namespace.itervalues().next()


    @table_name.setter
    def table_name(self, table_name):
        assert len(self.rel_namespace.values()) == 1
        if self.table_name is not None and self.rel_namespace is not None:
            for rel, relname in self.rel_namespace.iteritems():
                if relname.lower() == self.table_name.lower():
                    self.rel_namespace[rel] = table_name

        self._paql_query_str_stale = True


    @property
    def bc_query(self):
        bc_query = "SELECT * FROM {}".format(
            ','.join([
                rel_name + " " + rel_alias for rel_alias, rel_name in self.rel_namespace.iteritems()
            ]))
        where_clause_str = self.where_expr.get_str()
        if where_clause_str:
            bc_query += " WHERE {}".format(where_clause_str)
        if self.limit is not None and self.limit["TYPE"] =="INPUT":
            bc_query += " LIMIT {}".format(self.limit["LIMIT"])
        return bc_query


    def __init__(self, d):
        assert isinstance(d, dict)

        self._paql_query_str = None
        self._paql_query_str_stale = True

        # self.package_rel_names = d["package rel names"]
        self.rel_namespace = d["namespace"]
        self.rel_repeats = d["repeats"]
        self.where_expr = d["where expr"]
        self.such_that_expr = d["such that expr"]

        if d["objective expr"] is not None:
            self.objective = PackageQueryObjective(
                sqlquery_expr=d["objective expr"].get_sql_arithmetic_expression(),
                sense=d["objective sense"])
        else:
            self.objective = None

        self.limit = d["limit"]

        # NOTE: For now, assuming that the query is single-table.
        # TODO: We need to take into account REPEAT! It's not implemented yet!
        # rel_names = self.rel_namespace.values()
        assert len(self.rel_namespace.values()) == 1, "Not a single-table package query!"
        # self.table_name = self.bc_query.lower().split("from")[1].split("where")[0].split()[0].strip()
        # self.table_name = rel_names[0]


    def __str__(self):
        raise NotImplementedError


    def md5(self):
        return hashlib.md5(str(self)).hexdigest()


    @classmethod
    def get_json_from_paql(cls, paql_str):
        from subprocess import Popen, PIPE

        p = Popen(["PaQL_Parser"], stdin=PIPE, stdout=PIPE, stderr=PIPE, close_fds=True)
        json_str, err = p.communicate(input=paql_str)
        p.wait()

        if err != "":
            raise PaQLParserError(err)

        return json_str


    @classmethod
    def from_paql(cls, paql_str):
        """
        Returns a new PackageQuery object from a PaQL query string. This is the method that you would call more often.
        :param paql_str: A string containing a PaQL query
        :rtype : PackageQuery
        """
        json_str = PackageQuery.get_json_from_paql(paql_str)

        try:
            package_query = cls.from_json(json_str)
        except ValueError as e:
            traceback.print_exc(file=sys.stdout)
            raise PaQLParserError(e)
        else:
            package_query._paql_query_str = paql_str
            package_query._paql_query_str_stale = False
            return package_query
        finally:
            gc.collect()


    @classmethod
    def from_json(cls, json_str):
        """
        Returns a new PackageQuery object from a JSON string. This method is usually called by from_PaQL() to
        transform the paql parser output (which is a JSON) into a PackageQuery object.
        This is the main entry point from the direct output of the paql parser.
        :param json_str: A string containing a JSON structure for a parsed PaQL query
        """
        import json

        q = json.loads(json_str)

        # The namespace of relations defined by the query. A dictionary alias -> relation-name.
        # This way, all references to relations can be just made based on the alias names, and we can avoid confusion
        # when nested queries contain the same relation names, etc.
        rel_namespace = { }

        # The mapping from relation aliases into their corresponding REPEAT values.
        rel_repeats = { }

        # The list of relation aliases which form the PACKAGE.
        package_rel_names = []

        # TODO: Ideally, if the query is not a package query we may want to just execute it as it is...
        # TODO: If it doesn't contain the PACKAGE clause, we should make sure it does not contain SUCH THAT either.

        # Check if it's package query and store reference to relation names
        for select_item in q["SELECT"]:
            assert type(select_item) == dict

            if select_item["NODE_TYPE"] == "*":
                raise NotPackageQueryException()

            elif select_item["NODE_TYPE"] == "COL_REF":
                raise NotPackageQueryException()

            elif select_item["NODE_TYPE"] == "PACKAGE":
                package_rel_names.extend(r["REL_NAME"] for r in select_item["PACKAGE_RELS"])

            else:
                raise Exception("Problem in SELECT clause, NODE_TYPE non recognized: " + select_item["NODE_TYPE"])

        # Store relation names and aliases, and repeat constraint for each of them
        # These are stored in a dictionary rel_namespace(key=rel_alias, val=rel_names)
        for from_ in q["FROM"]:
            assert type(from_) == dict
            rel_name = from_["REL_NAME"]
            rel_alias = from_.get("REL_ALIAS", rel_name)
            repeat = from_.get("REPEAT", -1)
            rel_namespace[rel_alias] = rel_name
            rel_repeats[rel_alias] = repeat

        # Make sure that all relation aliases referred in PACKAGE(...) are in the FROM clause as well
        assert all(p_rel_name in rel_namespace for p_rel_name in package_rel_names)

        # Stricter (for now): Make sure that they are exactly the same relation references
        assert set(package_rel_names) == set(rel_namespace.iterkeys())

        # Create WHERE clause expression tree
        where_clause = Expression(q["WHERE"])

        # Create SUCH THAT clause expression tree
        such_that_clause = Expression(q["SUCH-THAT"])

        # Create objective clause expression tree
        if q["OBJECTIVE"] is not None:
            objective_expr = Expression(q["OBJECTIVE"]["EXPR"])

            if q["OBJECTIVE"]["TYPE"] == "MAXIMIZE":
                # objective = { "type": "maximize", "expr": objective_expr }
                objective_sense = ObjectiveSenseMAX()

            elif q["OBJECTIVE"]["TYPE"] == "MINIMIZE":
                # objective = { "type": "minimize", "expr": objective_expr }
                objective_sense = ObjectiveSenseMIN()

            else:
                raise Exception("Unsupported objective type: `{}'".format(q["OBJECTIVE"]["TYPE"]))

        else:
            objective_expr = objective_sense = None

        query_dict = {
            # "package rel names": package_rel_names,
            "where expr": where_clause,
            "such that expr": such_that_clause,
            "objective expr": objective_expr,
            "objective sense": objective_sense,
            "namespace": rel_namespace,
            "repeats": rel_repeats,
            "limit": q["LIMIT"],
        }

        if such_that_clause.is_conjunctive() and where_clause.is_conjunctive():
            return ConjunctivePackageQuery(query_dict)
        else:
            return cls(query_dict)


    @staticmethod
    def from_uncoalesced_constraints(table_name, unc_bcs, unc_gcs, objective):
        """
        This method creates a new PackageQuery from sets of uncoalesced constraints and an objective.
        """
        bc_query = "SELECT * FROM {} {}".format(table_name, "WHERE true" if len(unc_bcs) > 0 else "")
        for attr, op, n in unc_bcs:
            bc_query += " AND {a} {o} {b}".format(a=attr, o=op_to_opstr(op), b=n)

        gc_queries = []
        gc_ranges = []
        for (aggr, attr), op, n in unc_gcs:
            gc_query = "SELECT {aggr}({attr}) FROM memory_representations".format(aggr=aggr, attr=attr)
            if op == operator.le:
                # gc_range = (-sys.maxint, n)
                gc_range = (-float("inf"), n)
            elif op == operator.ge:
                # gc_range = (n, sys.maxint)
                gc_range = (n, float("inf"))
            elif op == operator.eq:
                gc_range = (n, n)
            else:
                raise Exception("Operator '{}' not supported yet.".format(op))
            gc_queries.append(gc_query)
            gc_ranges.append(gc_range)

        return PackageQuery({
            "bc": bc_query,
            "gc": map(lambda x: (x[0], x[1][0], x[1][1]), zip(gc_queries, gc_ranges)),
            "objective": objective,
        })


    def get_objective_attributes(self):
        attrs = set()
        if self.objective is not None:
            for attr in self.objective.get_attributes():
                if attr != "*":
                    attrs.add(attr)
        return attrs


    def get_bcs_attributes(self):
        return set(attr for attr in self.coalesced_bcs) - {"*"}


    def get_gcs_attributes(self):
        gcs_attrs = set()
        for gc in self.coalesced_gcs:
            assert isinstance(gc, CGlobalConstraint)

            gcs_attrs.update(gc.get_attributes())

        return gcs_attrs


    def get_attributes(self):
        # FIXME: If this is a relaxed query, you should return all attributes including those of the original query.
        return self.get_bcs_attributes() | self.get_gcs_attributes() | self.get_objective_attributes()


    def get_data_attributes(self, db):
        all_data_attributes = sql_get_all_attributes(db, self.table_name)

        # Only pick the data attributes of the allowed data type
        data_attributes = set()
        for data_attr in all_data_attributes:
            attribute_type = sql_table_column_data_type(db, self.table_name, data_attr)
            if attribute_type in self.allowed_dbms_data_types:
                data_attributes.add(data_attr)
        return sorted(data_attributes)



    def get_paql_str(self, redo=False, recompute_gcs=True, coalesced=False):
        raise NotImplementedError


    def abs_ugc_errors(self, gc_scores, attrs=None):
        """
        Returns absolute errors for each (uncoalesced) global constraint.
        """
        if attrs is None:
            use_attrs = self.get_attributes()
        else:
            use_attrs = set(attrs)

        return {
            (aggr, attr): max(0, c - gc_scores[aggr, attr] if op == operator.ge else gc_scores[aggr, attr] - c)
            for (aggr, attr), op, c in self.uncoalesced_gcs if attr == "*" or attr in use_attrs
        }


    def error_mape(self, u_gc_scores, u_bc_scores):
        errorsum = .0
        n_gcs = 0
        n_bcs = 0

        for i, ((aggr, attr), op, c) in enumerate(self.uncoalesced_gcs):
            score = u_gc_scores[i]
            if not op(score, c):
                errorsum += abs((c - score) / c)
            n_gcs += 1

        for bscores in u_bc_scores:
            for i, (attr, op, c) in enumerate(self.uncoalesced_bcs):
                score = bscores[i]
                if not op(score, c):
                    errorsum += abs((c - score) / c)
                n_bcs += 1

        if n_gcs + n_bcs > 0:
            return errorsum / (n_gcs + n_bcs)

        else:
            assert errorsum == 0
            return 0


    def generate_data_for_selectivity(self, selectivity, n_tuples):
        """
        NOTE: This is currently unused. Not even sure if I completed it. But give a look at it again because
        there were some interesting ideas.
        """
        def generate_valid_and_invalid_subsets(n_vars, n_subsets, n_valid):
            # TODO: Read again this function. There's some interesting logic
            n_subsets = int(math.ceil(n_subsets))
            n_valid = int(math.ceil(n_valid))

            assert n_valid <= n_subsets == 2**n_vars

            valid = []
            invalid = []

            # This must be always valid (it is the sum of no tuples)
            # valid.append( (0,)*n_vars )
            valid.append(0)

            # Generate half of vars valid and half invalid
            for i in range(n_vars):
                if len(valid) < n_valid/2.:
                    # valid.append(tuple( bit for bit in ('{:0%dbms}' % n_tuples).format(2**i) ))
                    valid.append(2**i)
                elif len(invalid) < (n_subsets - n_valid)/2.:
                    # invalid.append(tuple( bit for bit in ('{:0%dbms}' % n_tuples).format(2**i) ))
                    invalid.append(2**i)
                else:
                    valid.append(2**i)

            # Generate more invalid (up to n_subsets-n_valid) by combining invalid + invalid
            while len(invalid) < n_subsets-n_valid:
                found = False
                for i in range(len(invalid)):
                    for j in range(len(invalid)):
                        new_invalid = invalid[i] | invalid[j]
                        if new_invalid not in invalid:
                            invalid.append(new_invalid)
                            found = True
                            break
                    if found:
                        break
                if not found:
                    break

            # If more invalid are needed, generate them by combining invalid + valid
            while len(invalid) < n_subsets-n_valid:
                found = False
                for i in range(len(invalid)):
                    for j in range(len(valid)):
                        new_invalid = invalid[i] | valid[j]
                        if new_invalid not in invalid:
                            invalid.append(new_invalid)
                            found = True
                            break
                    if found:
                        break
                if not found:
                    raise Exception

            # All the remaining ones are valid
            valid = set(range(n_subsets)) - set(invalid)

            assert len(valid) == n_valid
            assert len(valid) + len(invalid) == n_subsets

            if logging.getLogger().getEffectiveLevel() == logging.DEBUG:
                debug("n invalid = {}".format(n_subsets - n_valid))
                debug("{}".format(valid))
                debug("{}".format(invalid))
                debug("{}".format([ tuple( bit for bit in ('{:0%dbms}' % n_tuples).format(i) ) for i in valid ]))
                debug("{}".format([ tuple( bit for bit in ('{:0%dbms}' % n_tuples).format(i) ) for i in invalid ]))

            return valid, invalid


        def generate_set_of_problems(base_prob, vars, total_n_constraints, n_valid_constraints, a, b):
            problems = []

            for valid in itertools.combinations(range(total_n_constraints), int(math.ceil(n_valid_constraints))):
                valid = set(valid)

                invalid = set(range(total_n_constraints)) - valid
                assert set(valid) | invalid == set(range(total_n_constraints))

                # The empty package must always be valid. TODO: Really?
                # valid = [0] + list(valid)

                prob = base_prob.copy()

                # valid = generate_valid_and_invalid_subsets(n_tuples, total_n_constraints, n_valid_constraints)[0]
                # valid = np.random.choice(range(total_n_constraints), size=n_valid_constraints, replace=False)
                if logging.getLogger().getEffectiveLevel() == logging.DEBUG:
                    debug("VALID: {}".format(valid))
                    debug("INVALID: {}".format(sorted(set(range(total_n_constraints)) - set(valid))))

                # Add valid constraints to the problem
                n_valid_added = 0
                for i in valid:
                    package_bitmap = [ int(bit) for bit in ('{:0%dbms}' % n_tuples).format(i) ]
                    assert len(package_bitmap) == len(vars)

                    # Add a VALID constraint for this combination of tuples
                    prob += np.dot(vars, package_bitmap) >= a
                    prob += np.dot(vars, package_bitmap) <= b
                    n_valid_added += 1
                assert n_valid_added == len(valid)

                # Add invalid constraints to the problem
                n_invalid_added = 0

                if float(a) > -float("inf") and float(b) < float("inf"):
                    # In this case, we produce 2**(len(invalid)) new sub-problems, each for a different set of ways
                    # to break the constraints a <= sum() <= b
                    pairs_of_invalid_constraints = []
                    for i in invalid:
                        package_bitmap = [ int(bit) for bit in ('{:0%dbms}' % n_tuples).format(i) ]
                        pairs_of_invalid_constraints.append((
                            (package_bitmap, operator.le, a-1),
                            (package_bitmap, operator.ge, b+1),
                        ))

                    orig_prob = prob.copy()
                    for set_of_invalid in itertools.product(*pairs_of_invalid_constraints):
                        new_prob = orig_prob.copy()
                        for invalid_bitmap, op, c in set_of_invalid:
                            new_prob += op(np.dot(vars, invalid_bitmap), c)
                        problems.append(new_prob)

                else:
                    # In this case, we only generate one sub-problem by adding all invalid constraints
                    for i in invalid:
                        package_bitmap = [ int(bit) for bit in ('{:0%dbms}' % n_tuples).format(i) ]
                        assert len(package_bitmap) == len(vars)

                        # Add an INVALID (out of range) constraint for this combination of tuples
                        if float(a) > -float("inf") and float(b) < float("inf"):
                            raise Exception("Should never happen!")
                            # prob += np.dot(vars, package_bitmap) <= a-1
                        elif float(a) > -float("inf"):
                            prob += np.dot(vars, package_bitmap) <= a-1
                        elif float(b) < float("inf"):
                            prob += np.dot(vars, package_bitmap) >= b+1
                        else:
                            raise Exception
                    assert n_invalid_added == len(invalid)

                    problems.append(prob)

            return problems


        assert 0 <= selectivity <= 1
        assert n_tuples >= 0

        table_name_start = self.bc_query.lower().find("from ")
        table_name_end = self.bc_query[table_name_start+5:].lower().find(" ")
        table_name = self.bc_query[table_name_start+5:table_name_start+5+table_name_end]

        attribute_names = []
        ranges = []
        for j in range(len(self.gc_queries)):
            if 'sum(' in self.gc_queries[j].lower():
                attr_start = self.gc_queries[j].lower().find('sum(')
                attr_end = self.gc_queries[j][attr_start+4:].lower().find(')')
                attribute_names.append(self.gc_queries[j][attr_start+4:attr_start+4+attr_end])
                ranges.append(self.gc_ranges[j])
        debug("{} {}".format(attribute_names, ranges))
        assert len(attribute_names) == len(ranges)

        # Generate the data via CPLEX
        data_columns = []

        # Generate one column at a time. Each column is generated with a CPLEX problem
        for j in range(len(attribute_names)):
            a, b = ranges[j]

            total_n_constraints = 2**n_tuples
            n_valid_constraints = (1-selectivity) * total_n_constraints

            # Check satisfiability of requirements
            if n_valid_constraints == 0 and a <= 0 <= b:
                warning("Since a<=0<=b there is always at least one valid package, i.e. the empty package, "
                        "therefore selectivity=1 (where no package is valid) is impossible.")
                return None
            if n_valid_constraints == total_n_constraints and not a <= 0 <= b:
                warning("Since not a<=0<=b, the empty package may never be a valid package, "
                        "therefore selectivity=0 (where all packages are valid) is impossible.")
                return None

            # Create the base problem
            base_prob = LpProblem("package-builder", LpMinimize)
            base_prob += 0 # no objective

            # Add constraints to the problem
            vars = [
                LpVariable("{}_{}".format(attribute_names[j], i), -float("inf"), float("inf"), LpInteger)
                for i in range(n_tuples)
            ]

            # Generate all possible combination of problem constraints
            # One of them will be feasible and will give us the dataset
            problems = generate_set_of_problems(base_prob, vars, total_n_constraints, n_valid_constraints, a, b)

            # Now try to find one feasible problem
            for prob in problems:
                # Solve the problem
                debug("{}".format(prob))
                solver = CPLEX(msg=True, timeLimit=None)
                solver.solve(prob)

                # Check the problem status
                if LpStatus[prob.status]=='Infeasible':
                    debug("@@@@@@@@@@@@@@@@@ INFEASIBLE: CONTINUE")
                    continue

                elif LpStatus[prob.status]=='Undefined':
                    raise Exception("Problem is undefined.")

                elif LpStatus[prob.status]=='Optimal':
                    debug("################## OPTIMAL")
                    prob.roundSolution()
                    sol = [ v.varValue for v in prob.tuple_variables() if type(v.varValue) is float ]
                    data_columns.append(sol)
                    break

                else:
                    raise Exception("LP status: {}".format(LpStatus[prob.status]))

            else:
                raise Exception("Could not find feasible combination of constraints "
                                "for selectivity {} and {} tuples.".format(selectivity, n_tuples))

        tuples = np.array(data_columns).transpose()

        return table_name, attribute_names, tuples



class ConjunctivePackageQuery(PackageQuery):
    # TODO: later on, move the two staticmethods from_... outside. Make them just functions.
    # TODO: IMPORTANT! All base and gc queries MUST be instance of some class SQL_Query instead of just strings

    def __init__(self, query_dict):
        super(ConjunctivePackageQuery, self).__init__(query_dict)

        # Store the base and global constraints as coalesced and un-coalesced constraints
        gc_constraint_trees = []
        gc_ranges = []
        gcs = self.such_that_expr.get_ANDed_gc_list()
        for sqlquery_expr, gc_range_a, gc_range_b in gcs:
            if isinstance(sqlquery_expr, SQLQueryExpression):
                # Note: Technically, you'll get an expression tree of "constraint trees" (query plans). So you
                # should actually try to combine them into one single constraint tree. Right now I'm simplifying
                # by assuming that the expression tree is always a simple leaf (so directly a constraint tree).
                operator_tree_expr = sqlquery_expr.traverse_leaf_func(leaf_func="get_constraint_tree")
                assert isinstance(operator_tree_expr, ArithmeticExpression)
            else:
                raise Exception
            gc_constraint_trees.append(operator_tree_expr)
            gc_ranges.append((np.float64(gc_range_a), np.float64(gc_range_b)))
        self.coalesced_gcs = get_coalesced_global_constraints(gc_constraint_trees, gc_ranges)
        self.uncoalesced_gcs = get_uncoalesced_global_constraints(self.coalesced_gcs)
        self.coalesced_bcs = get_coalesced_base_constraints(self.bc_query)
        self.uncoalesced_bcs = get_uncoalesced_base_constraints(self.coalesced_bcs)


    def __str__(self):
        return (
            "/-------------------------------------------- PaQL Query ---------------------------------------------\\\n"
            "|  PaQL query:\n"
            "|     " + str(self._paql_query_str) + "\n"
            "|  Base SQL query:\n"
            "|     " + str(self.bc_query) + "\n"
            "|  Global SQL queries:\n"
            "|     " + ("|     ".join([ str(q) + "\n" for q in self.gc_queries ]) if self.gc_queries else "None\n") + ""
            "|  Glogal constraint ranges:\n"
            "|     " + ("|     ".join([ str(q) + "\n" for q in self.gc_ranges ]) if self.gc_ranges else "None\n") + ""
            "|  Optimization objective:\n"
            "|     " + (str(self.objective) if self.objective else "None") + "\n"
            "\-----------------------------------------------------------------------------------------------------/"
        )


    def get_paql_str(self, redo=False, recompute_gcs=True, coalesced=False):
        if redo or self._paql_query_str is None or self._paql_query_str_stale:

            if recompute_gcs:
                self.coalesced_gcs = get_coalesced_global_constraints(self.gc_queries, self.gc_ranges)
                self.uncoalesced_gcs = get_uncoalesced_global_constraints(self.coalesced_gcs)
                self.coalesced_bcs = get_coalesced_base_constraints(self.bc_query)
                self.uncoalesced_bcs = get_uncoalesced_base_constraints(self.coalesced_bcs)

            if self.rel_namespace is None:
                # raise Exception("rel_namespace is None")
                # return ""
                self.rel_namespace = { "R": self.table_name }

            bcs_str = []
            gcs_str = []
            obj_str = None

            if not coalesced:
                if len(self.uncoalesced_bcs) > 0:
                    for attr, op, n in self.uncoalesced_bcs:
                        bcs_str.append("{} {} {}".format(attr, op_to_opstr(op), n))

                if len(self.uncoalesced_gcs) > 0:
                    for (aggr, attr), op, n in self.uncoalesced_gcs:
                        gcs_str.append("{}({}) {} {}".format(aggr, attr, op_to_opstr(op), n))

            else:
                if len(self.coalesced_bcs) > 0:
                    for attr, (lb, ub) in self.coalesced_bcs.iteritems():
                        if float(lb) == -float("inf") and float(ub) == float("inf"):
                            continue
                        elif float(ub) == float("inf"):
                            bcs_str.append("{} {} {}".format(attr, op_to_opstr(operator.ge), lb))
                        elif float(lb) == -float("inf"):
                            bcs_str.append("{} {} {}".format(attr, op_to_opstr(operator.le), ub))
                        elif lb == ub:
                            bcs_str.append("{} {} {}".format(attr, op_to_opstr(operator.eq), ub))
                        else:
                            bcs_str.append("{} BETWEEN {} AND {}".format(attr, lb, ub))

                if len(self.coalesced_gcs) > 0:
                    for (aggr, attr), (lb, ub) in self.coalesced_gcs.iteritems():
                        if aggr.lower() == "count":
                            lb, ub = int(lb), int(ub)

                        uaggr = aggr.upper()

                        if float(lb) == -float("inf") and float(ub) == float("inf"):
                            continue
                        elif float(ub) == float("inf"):
                            gcs_str.append("{}({}) {} {}".format(uaggr, attr, op_to_opstr(operator.ge), lb))
                        elif float(lb) == -float("inf"):
                            gcs_str.append("{}({}) {} {}".format(uaggr, attr, op_to_opstr(operator.le), ub))
                        elif lb == ub:
                            gcs_str.append("{}({}) {} {}".format(uaggr, attr, op_to_opstr(operator.eq), ub))
                        else:
                            gcs_str.append("{}({}) BETWEEN {} AND {}".format(uaggr, attr, lb, ub))

            if self.objective is not None:
                if self.objective["type"] == "maximize":
                    obj_str = "MAXIMIZE "
                elif self.objective["type"] == "minimize":
                    obj_str = "MINIMIZE "
                else:
                    raise
                obj_str += self.objective["func"].get_str()

            self._paql_query_str = \
                "SELECT \n\tPACKAGE({pack}) \n" \
                "FROM \n\t{tables} {bcs}{gcs}{obj};".format(
                pack=", ".join(self.rel_namespace.keys()),
                tables=", ".join("{} {}".format(name, alias) for alias, name in self.rel_namespace.iteritems()),
                bcs="\nWHERE \n\t{} ".format(" AND\n\t".join(bcs_str)) if bcs_str else "",
                gcs="\nSUCH THAT \n\t{} ".format(" AND\n\t".join(gcs_str)) if gcs_str else "",
                obj="\n{}".format(obj_str) if obj_str is not None else "")

            self._paql_query_str_stale = False

        return self._paql_query_str

#!/usr/bin/env python2
# -*- coding: utf-8 -*-
##################################################
# GNU Radio Python Flow Graph
# Title: IP3
# Author: Alexandros-Apostolos A. Boulogeorgos
# Generated: Mon Aug 12 09:13:37 2019
##################################################

if __name__ == '__main__':
    import ctypes
    import sys
    if sys.platform.startswith('linux'):
        try:
            x11 = ctypes.cdll.LoadLibrary('libX11.so')
            x11.XInitThreads()
        except:
            print "Warning: failed to XInitThreads()"

from PyQt4 import Qt
from gnuradio import analog
from gnuradio import blocks
from gnuradio import channels
from gnuradio import eng_notation
from gnuradio import gr
from gnuradio import qtgui
from gnuradio.eng_option import eng_option
from gnuradio.filter import firdes
from gnuradio.qtgui import Range, RangeWidget
from optparse import OptionParser
import math
import sip
import sys


class amplifiers_nonlinearities(gr.top_block, Qt.QWidget):

    def __init__(self):
        gr.top_block.__init__(self, "IP3")
        Qt.QWidget.__init__(self)
        self.setWindowTitle("IP3")
        try:
            self.setWindowIcon(Qt.QIcon.fromTheme('gnuradio-grc'))
        except:
            pass
        self.top_scroll_layout = Qt.QVBoxLayout()
        self.setLayout(self.top_scroll_layout)
        self.top_scroll = Qt.QScrollArea()
        self.top_scroll.setFrameStyle(Qt.QFrame.NoFrame)
        self.top_scroll_layout.addWidget(self.top_scroll)
        self.top_scroll.setWidgetResizable(True)
        self.top_widget = Qt.QWidget()
        self.top_scroll.setWidget(self.top_widget)
        self.top_layout = Qt.QVBoxLayout(self.top_widget)
        self.top_grid_layout = Qt.QGridLayout()
        self.top_layout.addLayout(self.top_grid_layout)

        self.settings = Qt.QSettings("GNU Radio", "amplifiers_nonlinearities")
        self.restoreGeometry(self.settings.value("geometry").toByteArray())

        ##################################################
        # Variables
        ##################################################
        self.samp_rate = samp_rate = 100000
        self.signal_amp = signal_amp = 0
        self.sigfreq = sigfreq = samp_rate*1.0247385/21.0
        self.ip3 = ip3 = 0

        ##################################################
        # Blocks
        ##################################################
        self._signal_amp_range = Range(-150, 10, 5, 0, 200)
        self._signal_amp_win = RangeWidget(self._signal_amp_range, self.set_signal_amp, 'Singal Power', "counter_slider", float)
        self.top_grid_layout.addWidget(self._signal_amp_win, 2,0,1,1)
        self._sigfreq_range = Range(0, samp_rate/2, 1000, samp_rate*1.0247385/21.0, 200)
        self._sigfreq_win = RangeWidget(self._sigfreq_range, self.set_sigfreq, 'Signal Freq', "counter_slider", float)
        self.top_grid_layout.addWidget(self._sigfreq_win, 3,0,1,1)
        self._ip3_range = Range(0, 2, 0.01, 0, 200)
        self._ip3_win = RangeWidget(self._ip3_range, self.set_ip3, 'IP3', "counter_slider", float)
        self.top_grid_layout.addWidget(self._ip3_win, 3,1,1,1)
        self.qtgui_freq_sink_x_0 = qtgui.freq_sink_c(
        	2048, #size
        	firdes.WIN_FLATTOP, #wintype
        	0, #fc
        	samp_rate, #bw
        	'', #name
        	2 #number of inputs
        )
        self.qtgui_freq_sink_x_0.set_update_time(0.10)
        self.qtgui_freq_sink_x_0.set_y_axis(-200, 0)
        self.qtgui_freq_sink_x_0.set_y_label('Relative Gain', 'dB')
        self.qtgui_freq_sink_x_0.set_trigger_mode(qtgui.TRIG_MODE_FREE, 0.0, 0, "")
        self.qtgui_freq_sink_x_0.enable_autoscale(False)
        self.qtgui_freq_sink_x_0.enable_grid(False)
        self.qtgui_freq_sink_x_0.set_fft_average(1.0)
        self.qtgui_freq_sink_x_0.enable_axis_labels(True)
        self.qtgui_freq_sink_x_0.enable_control_panel(False)
        
        if not True:
          self.qtgui_freq_sink_x_0.disable_legend()
        
        if "complex" == "float" or "complex" == "msg_float":
          self.qtgui_freq_sink_x_0.set_plot_pos_half(not True)
        
        labels = ['Input', 'With IP3', '', '', '',
                  '', '', '', '', '']
        widths = [2, 2, 1, 1, 1,
                  1, 1, 1, 1, 1]
        colors = ["blue", "red", "green", "black", "cyan",
                  "magenta", "yellow", "dark red", "dark green", "dark blue"]
        alphas = [0.5, 0.5, 1.0, 1.0, 1.0,
                  1.0, 1.0, 1.0, 1.0, 1.0]
        for i in xrange(2):
            if len(labels[i]) == 0:
                self.qtgui_freq_sink_x_0.set_line_label(i, "Data {0}".format(i))
            else:
                self.qtgui_freq_sink_x_0.set_line_label(i, labels[i])
            self.qtgui_freq_sink_x_0.set_line_width(i, widths[i])
            self.qtgui_freq_sink_x_0.set_line_color(i, colors[i])
            self.qtgui_freq_sink_x_0.set_line_alpha(i, alphas[i])
        
        self._qtgui_freq_sink_x_0_win = sip.wrapinstance(self.qtgui_freq_sink_x_0.pyqwidget(), Qt.QWidget)
        self.top_grid_layout.addWidget(self._qtgui_freq_sink_x_0_win, 0,0,1,2)
        self.channels_distortion_3_gen_0 = channels.distortion_3_gen(ip3)
        self.blocks_throttle_0 = blocks.throttle(gr.sizeof_gr_complex*1, samp_rate,True)
        self.blocks_add_xx_0 = blocks.add_vcc(1)
        self.analog_sig_source_x_0_0 = analog.sig_source_c(samp_rate, analog.GR_COS_WAVE, 2.45*sigfreq, pow(10.0,signal_amp/20.0), 0)
        self.analog_sig_source_x_0 = analog.sig_source_c(samp_rate, analog.GR_COS_WAVE, sigfreq, 1, 0)

        ##################################################
        # Connections
        ##################################################
        self.connect((self.analog_sig_source_x_0, 0), (self.blocks_add_xx_0, 1))    
        self.connect((self.analog_sig_source_x_0_0, 0), (self.blocks_add_xx_0, 0))    
        self.connect((self.blocks_add_xx_0, 0), (self.blocks_throttle_0, 0))    
        self.connect((self.blocks_throttle_0, 0), (self.channels_distortion_3_gen_0, 0))    
        self.connect((self.blocks_throttle_0, 0), (self.qtgui_freq_sink_x_0, 0))    
        self.connect((self.channels_distortion_3_gen_0, 0), (self.qtgui_freq_sink_x_0, 1))    

    def closeEvent(self, event):
        self.settings = Qt.QSettings("GNU Radio", "amplifiers_nonlinearities")
        self.settings.setValue("geometry", self.saveGeometry())
        event.accept()

    def get_samp_rate(self):
        return self.samp_rate

    def set_samp_rate(self, samp_rate):
        self.samp_rate = samp_rate
        self.set_sigfreq(self.samp_rate*1.0247385/21.0)
        self.qtgui_freq_sink_x_0.set_frequency_range(0, self.samp_rate)
        self.blocks_throttle_0.set_sample_rate(self.samp_rate)
        self.analog_sig_source_x_0_0.set_sampling_freq(self.samp_rate)
        self.analog_sig_source_x_0.set_sampling_freq(self.samp_rate)

    def get_signal_amp(self):
        return self.signal_amp

    def set_signal_amp(self, signal_amp):
        self.signal_amp = signal_amp
        self.analog_sig_source_x_0_0.set_amplitude(pow(10.0,self.signal_amp/20.0))

    def get_sigfreq(self):
        return self.sigfreq

    def set_sigfreq(self, sigfreq):
        self.sigfreq = sigfreq
        self.analog_sig_source_x_0_0.set_frequency(2.45*self.sigfreq)
        self.analog_sig_source_x_0.set_frequency(self.sigfreq)

    def get_ip3(self):
        return self.ip3

    def set_ip3(self, ip3):
        self.ip3 = ip3
        self.channels_distortion_3_gen_0.set_beta(self.ip3)


def main(top_block_cls=amplifiers_nonlinearities, options=None):

    from distutils.version import StrictVersion
    if StrictVersion(Qt.qVersion()) >= StrictVersion("4.5.0"):
        style = gr.prefs().get_string('qtgui', 'style', 'raster')
        Qt.QApplication.setGraphicsSystem(style)
    qapp = Qt.QApplication(sys.argv)

    tb = top_block_cls()
    tb.start()
    tb.show()

    def quitting():
        tb.stop()
        tb.wait()
    qapp.connect(qapp, Qt.SIGNAL("aboutToQuit()"), quitting)
    qapp.exec_()


if __name__ == '__main__':
    main()

from rest_framework import serializers

from core.models import Tag, Ingredient, Recipe


class TagSerializer(serializers.ModelSerializer):
    """Serializer for tag objects"""

    class Meta:
        model = Tag
        fields = ('id', 'name')
        read_only_fields = ('id',)


class IngredientSerializer(serializers.ModelSerializer):
    """Serializer for ingredient objects"""

    class Meta:
        model = Ingredient
        fields = ('id', 'name')
        read_only_fields = ('id',)


class RecipeSerializer(serializers.ModelSerializer):
    """Serialize a recipe"""
    ingredients = serializers.PrimaryKeyRelatedField(
        many=True,
        queryset=Ingredient.objects.all(),
    )
    tags = serializers.PrimaryKeyRelatedField(
        many=True,
        queryset=Tag.objects.all()
    )

    class Meta:
        model = Recipe
        fields = (
                  'id', 'title', 'ingredients', 'tags', 'time_minutes',
                  'price', 'link'
        )
        read_only_fields = ('id',)


class RecipeDetailSerializer(RecipeSerializer):
    """Serialize a recipe detail"""
    ingredients = IngredientSerializer(many=True, read_only=True)
    tags = TagSerializer(many=True, read_only=True)


class RecipeImageSerializer(serializers.ModelSerializer):
    """Serializer for uploading images to recipes"""

    class Meta:
        model = Recipe
        fields = ('id', 'image')
        read_only_fields = ('id',)

"""Module for the LoadEnvironment class."""

import os
import sys
from pathlib import Path


class LoadEnvironment:
    """This class is used for loading the environment from .env and .env.* files."""

    def __init__(self, environment=None, override=True, only=None):
        """LoadEnvironment constructor.

        Keyword Arguments:
            env {string} -- An additional environment file that you want to load. (default: {None})
            override {bool} -- Whether or not the environment variables found should overwrite existing ones. (default: {False})
            only {string} -- If this is set then it will only load that environment. (default: {None})
        """
        from dotenv import load_dotenv

        self.env = load_dotenv

        if only:
            self._load_environment(only, override=override)
            return

        env_path = str(Path(".") / ".env")
        self.env(env_path, override=override)

        if os.environ.get("APP_ENV"):
            self._load_environment(os.environ.get("APP_ENV"), override=override)
        if environment:
            self._load_environment(environment, override=override)

        if "pytest" in sys.modules:
            self._load_environment("testing", override=override)

    def _load_environment(self, environment, override=False):
        """Load the environment depending on the env file.

        Arguments:
            environment {string} -- Name of the environment file to load from

        Keyword Arguments:
            override {bool} -- Whether the environment file should overwrite existing environment keys. (default: {False})
        """
        env_path = str(Path(".") / ".env.{}".format(environment))
        self.env(dotenv_path=env_path, override=override)


def env(value, default="", cast=True):
    env_var = os.getenv(value, default)

    if not cast:
        return env_var

    if env_var == "":
        env_var = default

    if isinstance(env_var, bool):
        return env_var
    elif env_var is None:
        return None
    elif env_var.isnumeric():
        return int(env_var)
    elif env_var in ("false", "False"):
        return False
    elif env_var in ("true", "True"):
        return True
    else:
        return env_var

from fpssh.clients.base_parallel import BaseParallelSSHClient
from gevent.lock import RLock
from fpssh.clients.native.single import SSHClient


class ParallelSSHClient(BaseParallelSSHClient):

    def __init__(self, hosts, user, password, port):
        BaseParallelSSHClient.__init__(self, hosts, user, password, port)
        self._clients_lock = RLock()

    def _run_command(self, host, command):
        self._make_ssh_client(host)
        return self.host_clients[host].run_command(command)

    def _make_ssh_client(self, host):
        # with  acquire release
        with self._clients_lock:
            self.host_clients[host] = SSHClient(host, self.user, self.password, self.port)


if "__main__" == __name__:
    fake_cmd = "echo foo"
    fake_res = "foo\n"
    hosts = ["127.0.0.1", "127.0.0.1"]
    port = 2222
    user = "foo"
    password = "foo"
    client = ParallelSSHClient(hosts, user, password, port)

    def test_run_command():
        outputs = client.run_command(fake_cmd)
        for host, output in outputs.items():
            print(host, list(output[0]))


    test_run_command()
from skimage.draw import polygon
from scipy.spatial import distance
import numpy as np
import cv2


class PicLabeler:
    def __init__(self, model, config):
        self.model = model
        self.slots = config

    def run(self, image):

        self.image = image
        self.image = cv2.cvtColor(self.image, cv2.COLOR_BGR2GRAY)

        self.pts2 = np.float32([[0, 60], [0, 0], [40, 0], [40, 60]])
        slots = []  # list of preprocessed slot images
        ids = []  # list of slot ids

        for index, space in enumerate(self.slots):
            slot, _ = self.process_slot(space)
            ids.append(index + 1)
            slots.append(slot)

        return self.predict(slots, ids)

    def preprocess_coords(self, xs, ys):
        distances = []
        # calculate all side lengths of the quadrilateral
        for i in range(4):
            distances.append(
                distance.euclidean(
                    np.float32([xs[i], ys[i]]),
                    np.float32([xs[(i + 1) % 4], ys[(i + 1) % 4]])))
        # which one is the longest?
        starting_point = np.argmax(np.array(distances))
        # rearrange coordinates cyclically, so that longest side goes first
        new_xs = xs[starting_point:] + xs[:starting_point]
        new_ys = ys[starting_point:] + ys[:starting_point]
        return new_xs, new_ys

    def predict(self, slots, ids):
        answer = {}
        if not slots:
            print("answer empty")
            return answer
        # batch_size = 16
        # Verbosity mode: 1 = progress bar
        pred = self.model.predict(np.array(slots), 16, 1)

        # construct a JSON entity with results
        pred = pred.ravel().tolist()
        for i, one_id in enumerate(ids):
            answer[one_id] = 'Occupied' if pred[i] else 'Empty'
        return answer

    def process_slot(self, space):
        xs = []
        ys = []
        for point in space:
            xs.append(point[0])
            ys.append(point[1])
        # ensure contour is a quadrilateral. This assertion failed once.
        assert len(xs) == 4
        assert len(ys) == 4
        # preprocess and save coordinates
        xs, ys = self.preprocess_coords(xs, ys)
        xs = np.float32(xs)
        ys = np.float32(ys)
        coords = np.vstack((xs, ys)).T
        # get a matrix for perspective transformation
        M = cv2.getPerspectiveTransform(coords, self.pts2)

        # transform a quadrilateral into a solid rectangle
        dst = cv2.warpPerspective(self.image, M, (40, 60))
        # apply the perspective transformation matrix to a slot
        # and return as 40x60x1 NumPy array
        return np.reshape(dst, (40, 60, 1)), coords

import os
import time
import datetime
import collections
import socket
from calendar import timegm
from future.utils import iteritems

import json
try:
    import cPickle as pickle
except ImportError:
    import pickle

try:
    from threading import get_ident
except ImportError:
    from thread import get_ident

from pandaharvester.harvesterconfig import harvester_config
from pandaharvester.harvestercore import core_utils
from pandaharvester.harvestercore.plugin_factory import PluginFactory
from pandaharvester.harvestercore.db_proxy_pool import DBProxyPool as DBProxy
from pandaharvester.harvestercore.db_interface import DBInterface

# attribute list
_attribute_list = ['id', 'item', 'score']

# fifo object spec
FifoObject = collections.namedtuple('FifoObject', _attribute_list, verbose=False, rename=False)

# logger
_logger = core_utils.setup_logger('fifos')

# base class of fifo message queue
class FIFOBase(object):
    # constructor
    def __init__(self, **kwarg):
        for tmpKey, tmpVal in iteritems(kwarg):
            setattr(self, tmpKey, tmpVal)
        self.hostname = socket.gethostname()
        self.os_pid = os.getpid()
        self.dbProxy = DBProxy()
        self.dbInterface = DBInterface()

    # get process identifier
    def get_pid(self):
        thread_id = get_ident()
        if thread_id is None:
            thread_id = 0
        return '{0}_{1}-{2}'.format(self.hostname, self.os_pid, format(get_ident(), 'x'))

    # make logger
    def make_logger(self, base_log, token=None, method_name=None, send_dialog=True):
        if send_dialog and hasattr(self, 'dbInterface'):
            hook = self.dbInterface
        else:
            hook = None
        return core_utils.make_logger(base_log, token=token, method_name=method_name, hook=hook)

    # intialize fifo from harvester configuration
    def _initialize_fifo(self, force_enable=False):
        self.fifoName = '{0}_fifo'.format(self.titleName)
        self.config = getattr(harvester_config, self.titleName)
        if force_enable:
            self.enabled = True
        elif hasattr(self.config, 'fifoEnable') and self.config.fifoEnable:
            self.enabled = True
        else:
            self.enabled = False
            return
        pluginConf = vars(self.config).copy()
        pluginConf.update( {'titleName': self.titleName} )
        if hasattr(self.config, 'fifoModule') and hasattr(self.config, 'fifoClass'):
            pluginConf.update( {'module': self.config.fifoModule,
                                'name': self.config.fifoClass,} )
        else:
            if not hasattr(harvester_config, 'fifo'):
                return
            pluginConf.update( {'module': harvester_config.fifo.fifoModule,
                                'name': harvester_config.fifo.fifoClass,} )
        pluginFactory = PluginFactory()
        self.fifo = pluginFactory.get_plugin(pluginConf)

    # encode
    def encode(self, item):
        item_serialized = pickle.dumps(item, -1)
        return item_serialized

    # decode
    def decode(self, item_serialized):
        item = pickle.loads(item_serialized)
        return item

    # size of queue
    def size(self):
        mainLog = self.make_logger(_logger, 'id={0}-{1}'.format(self.fifoName, self.get_pid()), method_name='size')
        retVal = self.fifo.size()
        mainLog.debug('size={0}'.format(retVal))
        return retVal

    # enqueue
    def put(self, item, score=None, encode_item=True):
        mainLog = self.make_logger(_logger, 'id={0}-{1}'.format(self.fifoName, self.get_pid()), method_name='put')
        if encode_item:
            item_serialized = self.encode(item)
        else:
            item_serialized = item
        if score is None:
            score = time.time()
        retVal = self.fifo.put(item_serialized, score)
        mainLog.debug('score={0}'.format(score))
        return retVal

    # enqueue by id, which is unique
    def putbyid(self, id, item, score=None, encode_item=True):
        mainLog = self.make_logger(_logger, 'id={0}-{1}'.format(self.fifoName, self.get_pid()), method_name='putbyid')
        if encode_item:
            item_serialized = self.encode(item)
        else:
            item_serialized = item
        if score is None:
            score = time.time()
        retVal = self.fifo.putbyid(id, item_serialized, score)
        mainLog.debug('id={0} score={1}'.format(id, score))
        return retVal

    # dequeue to get the first fifo object
    def get(self, timeout=None, protective=False, decode_item=True):
        mainLog = self.make_logger(_logger, 'id={0}-{1}'.format(self.fifoName, self.get_pid()), method_name='get')
        object_tuple = self.fifo.get(timeout, protective)
        if object_tuple is None:
            retVal = None
        else:
            id, item_serialized, score = object_tuple
            if item_serialized is not None and decode_item:
                item = self.decode(item_serialized)
            else:
                item = item_serialized
            retVal = FifoObject(id, item, score)
        mainLog.debug('called. protective={0} decode_item={1}'.format(protective, decode_item))
        return retVal

    # dequeue to get the last fifo object
    def getlast(self, timeout=None, protective=False, decode_item=True):
        mainLog = self.make_logger(_logger, 'id={0}-{1}'.format(self.fifoName, self.get_pid()), method_name='getlast')
        object_tuple = self.fifo.getlast(timeout, protective)
        if object_tuple is None:
            retVal = None
        else:
            id, item_serialized, score = object_tuple
            if item_serialized is not None and decode_item:
                item = self.decode(item_serialized)
            else:
                item = item_serialized
            retVal = FifoObject(id, item, score)
        mainLog.debug('called. protective={0} decode_item={1}'.format(protective, decode_item))
        return retVal

    # dequeue list of objects with some conditions
    def getmany(self, mode='first', minscore=None, maxscore=None, count=None,
                    protective=False, temporary=False, decode_item=True):
        mainLog = self.make_logger(_logger, 'id={0}-{1}'.format(self.fifoName, self.get_pid()), method_name='getmany')
        object_tuple_list = self.fifo.getmany(mode, minscore, maxscore, count, protective, temporary)
        if not object_tuple_list:
            mainLog.debug('empty list')
        ret_list = []
        for object_tuple in object_tuple_list:
            id, item_serialized, score = object_tuple
            if item_serialized is not None and decode_item:
                item = self.decode(item_serialized)
            else:
                item = item_serialized
            val_tuple = FifoObject(id, item, score)
            ret_list.append(val_tuple)
        mainLog.debug('mode={0} minscore={1} maxscore={2} count={3} protective={4} temporary={5} decode_item={6}'.format(
                        mode, minscore, maxscore, count, protective, temporary, decode_item))
        return ret_list

    # get tuple of the first object and its score without dequeuing
    # If item is large un unnecessary to show int peek, set skip_item=True
    def peek(self, skip_item=False):
        mainLog = self.make_logger(_logger, 'id={0}-{1}'.format(self.fifoName, self.get_pid()), method_name='peek')
        object_tuple = self.fifo.peek(skip_item=skip_item)
        if object_tuple is None:
            retVal = None
            mainLog.debug('fifo empty')
        else:
            id, item_serialized, score = object_tuple
            if item_serialized is None and score is None:
                retVal = FifoObject(None, None, None)
            else:
                if score is None:
                    score = time.time()
                retVal = FifoObject(id, item_serialized, score)
            mainLog.debug('score={0}'.format(score))
        return retVal

    # get tuple of the last object and its score without dequeuing
    def peeklast(self, skip_item=False):
        mainLog = self.make_logger(_logger, 'id={0}-{1}'.format(self.fifoName, self.get_pid()), method_name='peeklast')
        object_tuple = self.fifo.peeklast(skip_item=skip_item)
        if object_tuple is None:
            retVal = None
            mainLog.debug('fifo empty')
        else:
            id, item_serialized, score = object_tuple
            if item_serialized is None and score is None:
                retVal = FifoObject(None, None, None)
            else:
                if score is None:
                    score = time.time()
                retVal = FifoObject(id, item_serialized, score)
            mainLog.debug('score={0}'.format(score))
        return retVal

    # get tuple of the object by id without dequeuing
    def peekbyid(self, id, temporary=False, skip_item=False):
        mainLog = self.make_logger(_logger, 'id={0}-{1}'.format(self.fifoName, self.get_pid()), method_name='peekbyid')
        object_tuple = self.fifo.peekbyid(id, temporary, skip_item=skip_item)
        if object_tuple is None:
            retVal = None
            mainLog.debug('fifo empty')
        else:
            id_gotten, item_serialized, score = object_tuple
            if item_serialized is None and score is None:
                retVal = FifoObject(None, None, None)
            else:
                if score is None:
                    score = time.time()
                retVal = FifoObject(id, item_serialized, score)
            mainLog.debug('id={0} score={1} temporary={2}'.format(id, score, temporary))
        return retVal

    # get list of object tuples without dequeuing
    def peekmany(self, mode='first', minscore=None, maxscore=None, count=None, skip_item=False):
        mainLog = self.make_logger(_logger, 'id={0}-{1}'.format(self.fifoName, self.get_pid()), method_name='peekmany')
        object_tuple_list = self.fifo.peekmany(mode, minscore, maxscore, count, skip_item)
        if not object_tuple_list:
            mainLog.debug('empty list')
        ret_list = []
        for object_tuple in object_tuple_list:
            id_gotten, item_serialized, score = object_tuple
            if item_serialized is None and score is None:
                val_tuple = FifoObject(None, None, None)
            else:
                if score is None:
                    score = time.time()
                val_tuple = FifoObject(id, item_serialized, score)
            ret_list.append(val_tuple)
        mainLog.debug('mode={0} minscore={1} maxscore={2} count={3}'.format(mode, minscore, maxscore, count))
        return ret_list

    # delete objects by list of ids from temporary space, return the number of objects successfully deleted
    def delete(self, ids):
        mainLog = self.make_logger(_logger, 'id={0}-{1}'.format(self.fifoName, self.get_pid()), method_name='release')
        retVal = self.fifo.delete(ids)
        mainLog.debug('released {0} objects in {1}'.format(retVal, ids))
        return retVal

    # restore objects by list of ids from temporary space to fifo; ids=None to restore all objects
    def restore(self, ids=None):
        mainLog = self.make_logger(_logger, 'id={0}-{1}'.format(self.fifoName, self.get_pid()), method_name='restore')
        retVal = self.fifo.restore(ids)
        if ids is None:
            mainLog.debug('restored all objects')
        else:
            mainLog.debug('restored objects in {0}'.format(ids))
        return retVal

    # update a object by its id with some conditions
    def update(self, id, item=None, score=None, temporary=None, cond_score='gt'):
        mainLog = self.make_logger(_logger, 'id={0}-{1}'.format(self.fifoName, self.get_pid()), method_name='update')
        retVal = self.fifo.update(id, item, score, temporary, cond_score)
        update_report_list = []
        if item is not None:
            update_report_list.append('item={0}'.format(item))
        if score is not None:
            update_report_list.append('score={0}'.format(score))
        if temporary is not None:
            update_report_list.append('temporary={0}'.format(temporary))
        update_report = ' '.join(update_report_list)
        mainLog.debug('update id={0} cond_score={1}: return={2}, {3}'.format(id, cond_score, retVal, update_report))
        return retVal


# Special fifo base for non havester-agent
class SpecialFIFOBase(FIFOBase):
    # constructor
    def __init__(self, **kwarg):
        FIFOBase.__init__(self, **kwarg)
        self.fifoName = '{0}_fifo'.format(self.titleName)
        pluginConf = {}
        pluginConf.update( {'titleName': self.titleName} )
        pluginConf.update( {'module': harvester_config.fifo.fifoModule,
                            'name': harvester_config.fifo.fifoClass,} )
        pluginFactory = PluginFactory()
        self.fifo = pluginFactory.get_plugin(pluginConf)


# Benchmark fifo
class BenchmarkFIFO(SpecialFIFOBase):
    titleName = 'benchmark'


# monitor fifo
class MonitorFIFO(FIFOBase):
    titleName = 'monitor'

    # constructor
    def __init__(self, **kwarg):
        FIFOBase.__init__(self, **kwarg)
        self._initialize_fifo()

    def populate(self, seconds_ago=0, clear_fifo=False):
        """
        Populate monitor fifo with all active worker chunks and timeNow as score from DB
        with modificationTime earlier than seconds_ago seconds ago
        object in fifo = [(queueName_1, [[worker_1_1], [worker_1_2], ...]), (queueName_2, ...)]
        """
        if clear_fifo:
            self.fifo.clear()
        try:
            fifoMaxWorkersToPopulate = self.config.fifoMaxWorkersToPopulate
        except AttributeError:
            fifoMaxWorkersToPopulate = 2**32
        try:
            fifoMaxWorkersPerChunk = self.config.fifoMaxWorkersPerChunk
        except AttributeError:
            fifoMaxWorkersPerChunk = 500
        workspec_iterator = self.dbProxy.get_active_workers(fifoMaxWorkersToPopulate, seconds_ago)
        last_queueName = None
        workspec_chunk = []
        timeNow_timestamp = time.time()
        score = timeNow_timestamp
        for workspec in workspec_iterator:
            workspec.set_work_params({'lastCheckAt': timeNow_timestamp})
            if last_queueName is None:
                try:
                    score = timegm(workspec.modificationTime.utctimetuple())
                except Exception:
                    pass
                workspec_chunk = [[workspec]]
                last_queueName = workspec.computingSite
            elif workspec.computingSite == last_queueName \
                and len(workspec_chunk) < fifoMaxWorkersPerChunk:
                workspec_chunk.append([workspec])
            else:
                self.put((last_queueName, workspec_chunk), score)
                try:
                    score = timegm(workspec.modificationTime.utctimetuple())
                except Exception:
                    pass
                workspec_chunk = [[workspec]]
                last_queueName = workspec.computingSite
        if len(workspec_chunk) > 0:
            self.put((last_queueName, workspec_chunk), score)

    def to_check_workers(self, check_interval=harvester_config.monitor.checkInterval):
        """
        Justify whether to check any worker by the modificationTime of the first worker in fifo
        retVal True if OK to dequeue to check;
        retVal False otherwise.
        Return retVal, overhead_time
        """
        mainLog = self.make_logger(_logger, 'id={0}-{1}'.format(self.fifoName, self.get_pid()), method_name='to_check_worker')
        retVal = False
        overhead_time = None
        timeNow_timestamp = time.time()
        peeked_tuple = self.peek(skip_item=True)
        if peeked_tuple is not None:
            score = peeked_tuple.score
            overhead_time = timeNow_timestamp - score
            if overhead_time > 0:
                retVal = True
                if score < 0:
                    mainLog.debug('True. Preempting')
                    overhead_time = None
                else:
                    mainLog.debug('True')
                    mainLog.info('Overhead time is {0} sec'.format(overhead_time))
            else:
                mainLog.debug('False. Workers too young to check')
                mainLog.debug('Overhead time is {0} sec'.format(overhead_time))
        else:
            mainLog.debug('False. Got nothing in FIFO')
        return retVal, overhead_time


class MonitorEventFIFO(SpecialFIFOBase):
    titleName = 'monitorEvent'

    # constructor
    def __init__(self, **kwarg):
        self.config = getattr(harvester_config, 'monitor')
        self.enabled = False
        if hasattr(self.config, 'fifoEnable') and self.config.fifoEnable \
            and getattr(self.config, 'eventBasedEnable', False):
            self.enabled = True
        SpecialFIFOBase.__init__(self, **kwarg)

# pylint: disable=g-bad-file-header
# Copyright 2016 The Bazel Authors. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Wrapper script for executing the Microsoft Compiler."""
import os
import sys
import msvc_link
import msvc_tools

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(SCRIPT_DIR)

GCCPATTERNS = [
    ('-I(.+)', ['/I$0']),
    ('-m(32|64)', ['$TARGET_ARCH']),
    ('-Xcompilation-mode=(dbg|fastbuild|opt)', ['$COMPILATION_MODE']),
    ('-msse', ['/arch:SSE']),
    ('-msse2', ['/arch:SSE2']),
    ('-D(.+)', ['/D$0']),
    ('-U(.+)', ['/U$0']),
    ('-E', ['/E']),
    ('-O0', ['/Od']),
    ('-Os', ['/O1']),
    ('-O2', ['/O2']),
    ('-g0', []),
    ('-g', ['$DEBUG_RT']),
    ('-fexceptions', ['/U_HAS_EXCEPTIONS', '/D_HAS_EXCEPTIONS=1', '/EHsc']),
    ('-fomit-frame-pointer', ['/Oy']),
    ('-fno-rtti', ['/GR-']),
    ('-frtti', ['/GR']),
    ('-fPIC', []),

    # This is unneeded for Windows.
    (('-include', '(.+)'), ['/FI$PATH0']),
    ('-w', ['/w']),
    ('-Wall', ['/Wall']),
    ('-Wsign-compare', ['/we4018']),
    ('-Wno-sign-compare', ['/wd4018']),
    ('-Wconversion', ['/we4244', '/we4267']),
    ('-Wno-conversion', ['/wd4244', '/wd4267']),
    ('-Wno-sign-conversion', []),
    ('-Wno-implicit-fallthrough', []),
    ('-Wno-implicit-function-declaration', ['/wd4013']),
    ('-Wimplicit-function-declaration', ['/we4013']),
    ('-Wcovered-switch-default', ['/we4062']),
    ('-Wno-covered-switch-default', ['/wd4062']),
    ('-Wno-error', []),
    ('-Wno-invalid-offsetof', []),
    ('-Wno-overloaded-virtual', []),
    ('-Wno-reorder', []),
    ('-Wno-string-plus-int', []),
    ('-Wl,S', []),  # Stripping is unnecessary since msvc uses pdb files.
    ('-Wl,-rpath(.+)', []),
    ('-B(.+)', []),
    ('-static', []),
    ('-shared', ['/DLL']),
    ('-std=(.+)', []),
]


def _IsLink(args):
  """Determines whether we need to link rather than compile.

  A set of arguments is for linking if they contain -static, -shared, are adding
  adding library search paths through -L, or libraries via -l.

  Args:
    args: List of arguments

  Returns:
    Boolean whether this is a link operation or not.
  """
  for arg in args:
    # Certain flags indicate we are linking.
    if (arg in ['-shared', '-static'] or arg[:2] in ['-l', '-L'] or
        arg[:3] == '-Wl'):
      return True
  return False


class MsvcCompiler(msvc_tools.WindowsRunner):
  """Driver for the Microsoft compiler."""

  def Run(self, argv):
    """Runs the compiler using the passed clang/gcc style argument list.

    Args:
      argv: List of arguments

    Returns:
      The return code of the compilation.

    Raises:
      ValueError: if target architecture isn't specified
    """
    parser = msvc_tools.ArgParser(self, argv, GCCPATTERNS)

    # Select runtime option
    # Find the last runtime option passed
    rt = None
    rt_idx = -1
    for i, opt in enumerate(reversed(parser.options)):
      if opt in ['/MT', '/MTd', '/MD', '/MDd']:
        if opt[-1] == 'd':
          parser.enforce_debug_rt = True
        rt = opt[:3]
        rt_idx = len(parser.options) - i - 1
        break
    rt = rt or '/MT'  # Default to static runtime
    # Add debug if necessary
    if parser.enforce_debug_rt:
      rt += 'd'
    # Include runtime option
    if rt_idx >= 0:
      parser.options[rt_idx] = rt
    else:
      parser.options.append(rt)

    compiler = 'cl'
    if parser.is_cuda_compilation:
      compiler = 'nvcc'
    return self.RunBinary(compiler, parser.options, parser)


def main(argv):
  # If we are supposed to link create a static library.
  if _IsLink(argv[1:]):
    return msvc_link.main(argv)
  else:
    return MsvcCompiler().Run(argv[1:])


if __name__ == '__main__':
  sys.exit(main(sys.argv[1:]))  # need to skip the first argument

from collections import defaultdict
import numpy as np
import matplotlib.pyplot as plt

from hyperopt_synthetic import run_one_exp as hyperopt_synthetic_opt
from xbbo_synthetic import run_one_exp as xbbo_synthetic_opt

max_call = 50
if __name__ == "__main__":
    rng = np.random.RandomState(42)
    result_opts = defaultdict(list)
    for i in range(3):
        seed = rng.randint(1e5)
        # result_opts['hyperopt-rand'].append(hyperopt_synthetic_opt('rand', max_call,seed))
        result_opts['hyperopt-tpe'].append(hyperopt_synthetic_opt('tpe', max_call,seed))
        # result_opts['hyperopt-atpe'].append(hyperopt_synthetic_opt('atpe', max_call,seed))
        # result_opts['hyperopt-mix'].append(hyperopt_synthetic_opt('mix', max_call,seed))
        result_opts['hyperopt-anneal'].append(hyperopt_synthetic_opt('anneal', max_call,seed))
        result_opts['XBBO-tpe'].append(xbbo_synthetic_opt('tpe', max_call,seed))
        result_opts['XBBO-anneal'].append(xbbo_synthetic_opt('anneal',max_call,seed))
    plt.figure()
    for key in result_opts:
        plt.plot(range(1,max_call+1), np.mean(np.minimum.accumulate(np.asarray(result_opts[key]), axis=1),axis=0)[:], label=key)
    plt.ylim([-0.1,1000])
    plt.xlabel('# of Evaluate')
    plt.ylabel('OBJ')
    plt.title('Average of cumulate best on 3 seeds')
    plt.legend()
    plt.savefig('./out/comp_with_hyperopt.png')
    plt.show()


# Generated by Django 3.1.13 on 2021-09-19 08:36

from django.db import migrations, models
import django.db.models.deletion
import taggit.managers


class Migration(migrations.Migration):

    initial = True

    dependencies = [
        ('wagtailimages', '0023_add_choose_permissions'),
        ('taggit', '0003_taggeditem_add_unique_index'),
    ]

    operations = [
        migrations.CreateModel(
            name='Volunteer',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('name', models.CharField(max_length=255, verbose_name='name')),
                ('email', models.EmailField(blank=True, max_length=254, null=True, verbose_name='email')),
                ('bio', models.TextField(verbose_name='biography')),
                ('affiliation', models.CharField(max_length=128, verbose_name='Affiliation')),
                ('website', models.URLField(blank=True, verbose_name='Website')),
                ('twitter_profile', models.URLField(blank=True, null=True, verbose_name='Twitter Profile')),
                ('linkedin_profile', models.URLField(blank=True, null=True, verbose_name='LinkedIn Profile')),
                ('orcid_profile', models.URLField(blank=True, null=True, verbose_name='OrcID Link')),
                ('creation_date', models.DateTimeField(auto_now_add=True)),
                ('last_updated', models.DateTimeField(auto_now_add=True)),
                ('active_since', models.DateTimeField(default=None, null=True)),
                ('areas_expertise', taggit.managers.TaggableManager(help_text='A comma-separated list of tags.', through='taggit.TaggedItem', to='taggit.Tag', verbose_name='areas of expertise')),
                ('picture', models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='+', to='wagtailimages.image')),
            ],
            options={
                'ordering': ['name'],
            },
        ),
    ]

# coding=utf-8
from typing import List

from src.data_structure.data_structure import ListNode


class Solution:
    """
    
    """

    def delete_node(self, node: ListNode) -> None:
        """
        Time: O(1), Space: O(1)
        :param node:
        :return:
        """
        node.val = node.next.val
        node.next = node.next.next


# Generated by Django 3.1.7 on 2021-03-28 01:29

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('survey', '0002_surveycovid_surveyother'),
    ]

    operations = [
        migrations.AddField(
            model_name='surveycovid',
            name='others',
            field=models.TextField(db_column='other_concerns', default=''),
        ),
    ]

from django.conf import settings
from django.contrib.auth.models import Group
from django.contrib.sites.models import Site
from django.db import models
from django.template import Context, Template
from django.utils.translation import ugettext_lazy as _

from rdmo.conditions.models import Condition
from rdmo.core.models import TranslationMixin
from rdmo.core.utils import copy_model, get_pandoc_version, join_url
from rdmo.questions.models import Catalog

from .managers import ViewManager
from .utils import ProjectWrapper


class View(models.Model, TranslationMixin):

    objects = ViewManager()

    uri = models.URLField(
        max_length=640, blank=True,
        verbose_name=_('URI'),
        help_text=_('The Uniform Resource Identifier of this view (auto-generated).')
    )
    uri_prefix = models.URLField(
        max_length=256,
        verbose_name=_('URI Prefix'),
        help_text=_('The prefix for the URI of this view.')
    )
    key = models.SlugField(
        max_length=128, blank=True,
        verbose_name=_('Key'),
        help_text=_('The internal identifier of this view.')
    )
    comment = models.TextField(
        blank=True,
        verbose_name=_('Comment'),
        help_text=_('Additional internal information about this view.')
    )
    locked = models.BooleanField(
        default=False,
        verbose_name=_('Locked'),
        help_text=_('Designates whether this view can be changed.')
    )
    catalogs = models.ManyToManyField(
        Catalog, blank=True,
        verbose_name=_('Catalogs'),
        help_text=_('The catalogs this view can be used with. An empty list implies that this view can be used with every catalog.')
    )
    sites = models.ManyToManyField(
        Site, blank=True,
        verbose_name=_('Sites'),
        help_text=_('The sites this view belongs to (in a multi site setup).')
    )
    groups = models.ManyToManyField(
        Group, blank=True,
        verbose_name=_('Group'),
        help_text=_('The groups for which this view is active.')
    )
    template = models.TextField(
        blank=True,
        verbose_name=_('Template'),
        help_text=_('The template for this view, written in Django template language.')
    )
    title_lang1 = models.CharField(
        max_length=256, blank=True,
        verbose_name=_('Title (primary)'),
        help_text=_('The title for this view in the primary language.')
    )
    title_lang2 = models.CharField(
        max_length=256, blank=True,
        verbose_name=_('Title (secondary)'),
        help_text=_('The title for this view in the secondary language.')
    )
    title_lang3 = models.CharField(
        max_length=256, blank=True,
        verbose_name=_('Title (tertiary)'),
        help_text=_('The title for this view in the tertiary language.')
    )
    title_lang4 = models.CharField(
        max_length=256, blank=True,
        verbose_name=_('Title (quaternary)'),
        help_text=_('The title for this view in the quaternary language.')
    )
    title_lang5 = models.CharField(
        max_length=256, blank=True,
        verbose_name=_('Title (quinary)'),
        help_text=_('The title for this view in the quinary language.')
    )
    help_lang1 = models.TextField(
        blank=True,
        verbose_name=_('Help (primary)'),
        help_text=_('The help text for this view in the primary language.')
    )
    help_lang2 = models.TextField(
        blank=True,
        verbose_name=_('Help (secondary)'),
        help_text=_('The help text for this view in the secondary language.')
    )
    help_lang3 = models.TextField(
        blank=True,
        verbose_name=_('Help (tertiary)'),
        help_text=_('The help text for this view in the tertiary language.')
    )
    help_lang4 = models.TextField(
        blank=True,
        verbose_name=_('Help (quaternary)'),
        help_text=_('The help text for this view in the quaternary language.')
    )
    help_lang5 = models.TextField(
        blank=True,
        verbose_name=_('Help (quinary)'),
        help_text=_('The help text for this view in the quinary language.')
    )
    available = models.BooleanField(
        default=True,
        verbose_name=_('Available'),
        help_text=_('Designates whether this view is generally available for projects.')
    )

    class Meta:
        ordering = ('key', )
        verbose_name = _('View')
        verbose_name_plural = _('Views')

    def __str__(self):
        return self.key

    def save(self, *args, **kwargs):
        self.uri = self.build_uri(self.uri_prefix, self.key)
        super().save(*args, **kwargs)

    def copy(self, uri_prefix, key):
        view = copy_model(self, uri_prefix=uri_prefix, key=key)

        # copy m2m fields
        view.catalogs.set(self.catalogs.all())
        view.sites.set(self.sites.all())
        view.groups.set(self.groups.all())

        return view

    @property
    def title(self):
        return self.trans('title')

    @property
    def help(self):
        return self.trans('help')

    @property
    def is_locked(self):
        return self.locked

    def render(self, project, snapshot=None, export_format=None):
        # render the template to a html string
        # it is important not to use models here

        return Template(self.template).render(Context({
            'project': ProjectWrapper(project, snapshot),
            'conditions': {
                condition.key: condition.resolve(project, snapshot)
                for condition in Condition.objects.select_related('source')
            },
            'format': export_format,
            'pandoc_version': get_pandoc_version()
        }))

    @classmethod
    def build_uri(cls, uri_prefix, key):
        assert key
        return join_url(uri_prefix or settings.DEFAULT_URI_PREFIX, '/views/', key)

#!/usr/bin/env python
# This code is part of the Biopython distribution and governed by its
# license.  Please see the LICENSE file that should have been included
# as part of this package.
#
# Bio.Wise contains modules for running and processing the output of
# some of the models in the Wise2 package by Ewan Birney available from:
# ftp://ftp.ebi.ac.uk/pub/software/unix/wise2/
# http://www.ebi.ac.uk/Wise2/
#
# Bio.Wise.psw is for protein Smith-Waterman alignments
# Bio.Wise.dnal is for Smith-Waterman DNA alignments

from __future__ import print_function

import os
import sys
import tempfile

from Bio import SeqIO


def _build_align_cmdline(cmdline, pair, output_filename, kbyte=None, force_type=None, quiet=False):
    """Helper function to build a command line string (PRIVATE).

    >>> os.environ["WISE_KBYTE"]="300000"
    >>> if os.isatty(sys.stderr.fileno()):
    ...    c = _build_align_cmdline(["dnal"], ("seq1.fna", "seq2.fna"),
    ...                             "/tmp/output", kbyte=100000)
    ...    assert c == 'dnal -kbyte 100000 seq1.fna seq2.fna > /tmp/output', c
    ...    c = _build_align_cmdline(["psw"], ("seq1.faa", "seq2.faa"),
    ...                             "/tmp/output_aa")
    ...    assert c == 'psw -kbyte 300000 seq1.faa seq2.faa > /tmp/output_aa', c
    ... else:
    ...    c = _build_align_cmdline(["dnal"], ("seq1.fna", "seq2.fna"),
    ...                             "/tmp/output", kbyte=100000)
    ...    assert c == 'dnal -kbyte 100000 -quiet seq1.fna seq2.fna > /tmp/output', c
    ...    c = _build_align_cmdline(["psw"], ("seq1.faa", "seq2.faa"),
    ...                             "/tmp/output_aa")
    ...    assert c == 'psw -kbyte 300000 -quiet seq1.faa seq2.faa > /tmp/output_aa', c

    """
    cmdline = cmdline[:]

    ### XXX: force_type ignored

    if kbyte is None:
        try:
            cmdline.extend(("-kbyte", os.environ["WISE_KBYTE"]))
        except KeyError:
            pass
    else:
        cmdline.extend(("-kbyte", str(kbyte)))

    if not os.isatty(sys.stderr.fileno()):
        cmdline.append("-quiet")

    cmdline.extend(pair)
    cmdline.extend((">", output_filename))
    if quiet:
        cmdline.extend(("2>", "/dev/null"))
    cmdline_str = ' '.join(cmdline)

    return cmdline_str


def align(cmdline, pair, kbyte=None, force_type=None, dry_run=False, quiet=False, debug=False):
    """
    Returns a filehandle
    """
    if not pair or len(pair) != 2:
        raise ValueError("Expected pair of filename, not %s" % repr(pair))

    output_file = tempfile.NamedTemporaryFile(mode='r')
    input_files = tempfile.NamedTemporaryFile(mode="w"), tempfile.NamedTemporaryFile(mode="w")

    if dry_run:
        print(_build_align_cmdline(cmdline,
                                   pair,
                                   output_file.name,
                                   kbyte,
                                   force_type,
                                   quiet))
        return

    for filename, input_file in zip(pair, input_files):
        # Pipe the file through Biopython's Fasta parser/writer
        # to make sure it conforms to the Fasta standard (in particular,
        # Wise2 may choke on long lines in the Fasta file)
        records = SeqIO.parse(open(filename), 'fasta')
        SeqIO.write(records, input_file, 'fasta')
        input_file.flush()

    input_file_names = [input_file.name for input_file in input_files]

    cmdline_str = _build_align_cmdline(cmdline,
                                       input_file_names,
                                       output_file.name,
                                       kbyte,
                                       force_type,
                                       quiet)

    if debug:
        sys.stderr.write("%s\n" % cmdline_str)

    status = os.system(cmdline_str) >> 8

    if status > 1:
        if kbyte != 0: # possible memory problem; could be None
            sys.stderr.write("INFO trying again with the linear model\n")
            return align(cmdline, pair, 0, force_type, dry_run, quiet, debug)
        else:
            raise OSError("%s returned %s" % (" ".join(cmdline), status))

    return output_file


def all_pairs(singles):
    """
    Generate pairs list for all-against-all alignments

    >>> all_pairs(range(4))
    [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]
    """
    pairs = []

    singles = list(singles)
    while singles:
        suitor = singles.pop(0) # if sorted, stay sorted
        pairs.extend((suitor, single) for single in singles)

    return pairs


def main():
    pass


def _test(*args, **keywds):
    import doctest
    doctest.testmod(sys.modules[__name__], *args, **keywds)

if __name__ == "__main__":
    if __debug__:
        _test()
    main()

#!/usr/bin/env python

# summary: physical touchscreen buttons listener

# deps: sudo apt-get install python-dev python-rpi.gpio

import RPi.GPIO as GPIO
from time import sleep
import signal, os, subprocess, sys

buttons = [24, 23, 18]


def button_pressed(channel):
    if channel == 23:
        print("[buttons.python] MIDDLE: restart all processes...")
        subprocess.Popen(['/bin/sh', '/home/pi/pdac/AUTOSTART.sh'])
    elif channel == 24:
        print("[buttons.python] BOTTOM: toggle browser...")
        subprocess.Popen(['/bin/sh', '/home/pi/pdac/system/utilityToggleBrowser.sh'])
def unregister_events():
    for pin in buttons:
        GPIO.remove_event_detect(pin)

if __name__ == '__main__':
    signal.signal(signal.SIGINT, unregister_events)
    try:
        GPIO.setmode(GPIO.BCM)
        for pin in buttons:
            GPIO.setup(pin, GPIO.IN, pull_up_down=GPIO.PUD_UP)
            GPIO.add_event_detect(pin, GPIO.RISING, callback=button_pressed, bouncetime=200)
        while True:
            sleep(10)
    except Exception as e:
        print("Caught exception:", e)
        unregister_events()
#!/usr/bin/env python
import json
import os
import sys
from textwrap import dedent


def get_hashed_filenames(static_path):
    json_file = '{}/staticfiles.json'.format(static_path)
    with open(json_file) as jsonf:
        staticfiles = json.load(jsonf)

    return list(staticfiles['paths'].values())


def move_hashed_files(static_path, hashed_path):
    filenames = get_hashed_filenames(static_path)
    moved_count = 0
    for filename in filenames:
        # some filenames in the file are in the form
        # fontawesome/fonts/fontawesome-webfont.f7c2b4b747b1.eot?v=4.3.0
        # we can skip these as they're duplicated
        if '?' in filename:
            continue

        src_fn = os.path.join(static_path, filename)
        dst_fn = os.path.join(hashed_path, filename)
        if not os.path.exists(os.path.dirname(dst_fn)):
            os.makedirs(os.path.dirname(dst_fn))

        os.rename(src_fn, dst_fn)
        moved_count += 1

    return moved_count


def main(static_path, hashed_path):
    moved = move_hashed_files(static_path, hashed_path)
    print('Successfully moved {} files'.format(moved))


if __name__ == '__main__':
    try:
        main(sys.argv[1], sys.argv[2])
    except IndexError:
        sys.exit(dedent("""\
            ERROR: source and destination directory arguments required.

            Usage: move_hashed_staticfiles.py <source_dir> <dest_dir>

            Moves hashed static files from source_dir to dest_dir based on the
            map of staticfiles in `source_dir/staticfiles.json`.
        """))

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import statistics

# Scores for ABPruning Depth 2 on rack size 5
# nodes = [5, 101, 56, 64, 100, 71, 135, 55, 39, 14, 85, 12, 21, 48, 17, 21, 64, 214, 93, 9, 14, 6, 16, 12, 42, 17, 25, 117, 35, 37, 35, 89, 6, 70, 22, 80, 16, 64, 70, 51, 21, 39, 46, 31, 30, 82, 18, 32, 71, 9, 59, 4, 74, 49, 1, 55, 30, 2, 50, 61, 62, 86, 26, 116, 75, 21, 52, 19, 146, 83, 57, 65, 3, 74, 14, 59, 24, 3, 23, 62, 53, 57, 69, 90, 9, 26, 72, 59, 19, 35, 17, 31, 68, 16, 26, 13, 130, 78, 13, 29, 17, 44, 54, 14, 50, 43, 60, 1, 7, 5, 10, 9, 42, 22, 97, 11, 89, 40, 134, 7, 49, 140, 190, 42, 52, 12, 72, 27, 16, 48, 21, 30, 14, 31, 92, 42, 26, 59, 15, 87, 42, 101, 67, 49, 22, 15, 222, 66, 26, 72, 12, 14, 42, 15, 36, 10, 1, 43, 8, 64, 89, 74, 19, 21, 12, 10, 9, 39, 98, 14, 24, 45, 49, 3, 13, 55, 18, 103, 10, 7, 54]
# p1scores = [204, 167, 283, 162, 240, 250, 301, 167, 221, 193, 307, 199, 208, 187, 146, 219, 197, 149, 229, 187, 262, 15, 183, 294, 139, 249, 230, 178, 198, 237, 281, 198, 201, 307, 188, 253, 257, 239, 315, 249, 154, 231, 222, 226, 250, 248, 200, 175, 234, 261, 227, 154, 290, 250, 190, 270, 227, 208, 155, 232, 271, 285, 168, 242, 226, 181, 217, 248, 192, 179, 237, 238, 215, 191, 252, 293, 248, 232, 190, 193, 255, 164, 252, 221, 220, 153, 239, 0, 205, 0, 190, 149, 238, 284, 194, 52, 249, 227, 264, 195, 200, 246, 207, 0, 253, 271, 176, 246, 189, 216, 210, 169, 273, 190, 285, 281, 179, 207, 255, 248, 283, 173, 210, 261, 196, 241, 0, 217, 212, 205, 200, 208, 205, 267, 206, 230, 138, 231, 181, 109, 0, 255, 259, 229, 175, 234, 356, 275, 280, 273, 255, 27, 199, 220, 289, 188, 299, 291, 211, 246, 267, 192, 206, 278, 242, 253, 266, 182, 162, 230, 225, 250, 199, 0, 243, 337, 170, 175, 260, 145, 207, 231, 203, 235, 169, 232, 194, 177, 258, 194, 216, 267, 14, 186, 221, 199, 281, 293, 121, 0]
# p2scores =[219, 293, 237, 328, 267, 275, 235, 305, 265, 237, 276, 165, 247, 262, 154, 174, 163, 288, 199, 288, 206, 49, 362, 195, 262, 265, 265, 215, 206, 195, 262, 240, 310, 173, 194, 256, 251, 166, 226, 231, 295, 309, 226, 260, 201, 259, 266, 252, 305, 164, 293, 211, 269, 177, 274, 180, 161, 242, 264, 142, 229, 276, 226, 281, 196, 169, 185, 202, 249, 329, 208, 178, 225, 183, 265, 185, 292, 224, 284, 223, 201, 331, 158, 296, 118, 318, 239, 0, 208, 0, 197, 283, 237, 277, 229, 81, 271, 274, 206, 281, 138, 209, 290, 0, 255, 205, 180, 181, 271, 227, 201, 268, 151, 283, 232, 202, 165, 165, 213, 234, 227, 267, 181, 213, 226, 240, 0, 278, 214, 260, 185, 286, 235, 236, 223, 278, 235, 210, 156, 215, 0, 249, 309, 249, 155, 215, 197, 205, 211, 263, 240, 18, 303, 301, 247, 282, 206, 191, 246, 284, 195, 191, 246, 300, 216, 220, 201, 364, 183, 223, 273, 212, 183, 0, 275, 179, 294, 241, 138, 91, 202, 188, 273, 211, 299, 207, 282, 252, 238, 153, 221, 176, 21, 330, 221, 198, 233, 256, 67, 0]
# p1endgamescores = [204, 155, 268, 103, 193, 240, 232, 151, 162, 179, 281, 199, 190, 127, 136, 207, 175, 149, 206, 163, 258, None, 153, 251, 139, 204, 185, 168, 180, 237, 239, 184, 165, 293, 155, 193, 230, 199, 236, 196, 139, 204, 207, 215, 185, 186, 189, 163, 212, 220, 215, 136, 278, 234, 190, 254, 209, 208, 131, 232, 234, 271, 151, 174, 174, 161, 193, 177, 178, 129, 187, 196, 184, 179, 174, 248, 224, 222, 165, 171, 210, 153, 210, 188, None, 136, 158, None, 188, None, 190, 137, 193, 256, 182, None, 236, 212, 224, 174, 200, 233, 207, None, 207, 185, 176, 179, 181, 197, 186, 163, 209, 173, 251, 281, 179, 202, 209, 206, 233, 160, 171, 213, 166, 136, None, 160, 167, 185, 182, 164, 157, 198, 153, 170, 131, 231, None, 100, None, 231, 210, 185, None, 192, 284, 259, 223, 222, 203, None, 188, 200, 267, 165, 228, 219, 203, 206, 198, 164, 188, 264, 189, 253, 213, 175, 118, 230, 225, 212, 199, None, 223, 226, 143, 165, 248, None, 202, 190, 162, 224, 160, 191, 186, 166, 173, 192, 164, 203, None, 184, 204, 194, 226, 245, None, None]
# p2endgamescores = [197, 237, 197, 281, 254, 202, 223, 292, 252, 133, 223, 147, 188, 244, 141, 167, 156, 170, 143, 249, 184, None, 279, 177, 262, 235, 242, 156, 195, 195, 219, 164, 160, 168, 176, 204, 210, 148, 217, 201, 229, 258, 197, 216, 185, 234, 194, 205, 205, 149, 233, 166, 255, 171, 253, 170, 134, 203, 211, 142, 224, 185, 140, 247, 174, 152, 170, 197, 208, 304, 197, 156, 171, 123, 244, 172, 220, 171, 262, 216, 173, 283, 128, 219, None, 250, 201, None, 185, None, 180, 240, 188, 189, 157, None, 196, 211, 149, 198, 138, 145, 224, None, 224, 205, 180, 167, 185, 210, 194, 196, 103, 223, 170, 190, 165, 148, 188, 220, 202, 251, 165, 187, 167, 197, None, 235, 181, 190, 178, 232, 188, 222, 180, 242, 223, 147, None, 203, None, 174, 286, 230, None, 181, 158, 181, 161, 243, 180, None, 228, 253, 171, 205, 191, 179, 214, 258, 193, 173, 190, 237, 196, 204, 181, 336, 169, 193, 254, 156, 181, None, 204, 168, 285, 207, 106, None, 195, 180, 259, 168, 258, 186, 193, 200, 233, 146, 187, 148, None, 242, 221, 175, 206, 207, None, None]
# times =[0.392374, 1.087435, 0.7119709999999997, 0.810673, 1.126067, 0.7669739999999994, 1.994154, 0.6895319999999998, 0.6009550000000008, 0.3275289999999984, 0.9863210000000002, 0.2606470000000005, 0.34875400000000134, 0.7456220000000009, 0.32181700000000113, 0.4138010000000012, 0.8067349999999998, 0.15606599999999915, 2.6905530000000013, 0.9456589999999991, 0.31708599999999976, 0.02612799999999993, 0.3128700000000002, 0.18505099999999786, 0.30555900000000236, 0.2620560000000012, 0.6745239999999981, 0.3066670000000009, 0.47260400000000047, 0.17609900000000067, 1.093862999999999, 0.45542199999999866, 0.7054580000000001, 0.5520420000000001, 1.0170379999999994, 0.2309340000000013, 0.733039999999999, 0.3513699999999993, 1.010028000000002, 0.29272900000000135, 0.8606179999999988, 0.7247850000000007, 0.6421530000000004, 0.2927980000000012, 0.7698590000000003, 0.6178729999999995, 0.3698689999999978, 0.43626899999999935, 0.8837449999999976, 0.3141510000000025, 0.4212889999999945, 0.7093589999999992, 0.2769800000000018, 0.7990300000000019, 0.21445099999999684, 0.8646510000000021, 0.6168930000000046, 0.1808240000000012, 0.6278900000000007, 0.4272070000000028, 0.21817000000000064, 0.7489089999999976, 0.6385280000000009, 0.9075630000000032, 0.8847120000000004, 0.5543809999999993, 1.1678890000000024, 0.9268649999999994, 0.32570499999999925, 0.7548699999999968, 0.3216419999999971, 1.9017319999999955, 1.4462270000000004, 0.6572630000000004, 1.060724999999998, 0.20168500000000478, 0.8210849999999965, 0.44043299999999874, 0.7265659999999983, 0.46925099999999986, 0.19774300000000267, 0.4200649999999939, 0.7009690000000006, 0.6478700000000046, 0.11564000000000618, 0.8002919999999989, 0.8857590000000002, 0.00432199999999483, 1.0995959999999982, 0.002073000000002878, 0.251300999999998, 0.39348400000000083, 0.8867929999999973, 0.9020439999999965, 0.33812700000000007, 0.07212599999999725, 0.48321200000000175, 0.3076520000000045, 0.5212119999999985, 0.7779639999999972, 0.3196629999999985, 0.37090200000000095, 0.2648180000000053, 0.0021559999999993806, 1.3325860000000063, 0.9182589999999919, 0.26353699999999947, 0.6415129999999891, 0.30544600000000344, 0.4823680000000081, 0.7752869999999916, 0.28730600000000095, 0.5652749999999997, 0.7506249999999994, 0.8220110000000034, 0.2295310000000086, 0.20372600000000318, 0.1945779999999928, 0.32604299999999853, 0.24623400000000117, 0.5504200000000026, 0.39636099999999885, 1.0773339999999934, 0.30486299999999744, 0.9997989999999959, 0.6598670000000055, 0.00277499999999975, 1.401699999999991, 0.24062299999999937, 0.612363000000002, 1.5137590000000074, 2.308454999999995, 0.5395869999999974, 1.1261659999999978, 0.3013700000000057, 0.8219319999999897, 0.46814299999999776, 0.19157900000000438, 0.15418999999999983, 0.2853580000000022, 0.0024219999999957054, 0.5605540000000104, 0.43711700000000064, 0.47679000000000826, 0.13257300000000782, 0.3474680000000063, 0.45118300000000033, 0.9176939999999973, 0.6302490000000063, 0.44485699999999895, 0.6749839999999949, 0.02986500000000092, 0.3542879999999968, 1.1250870000000077, 0.48420600000000036, 1.1846189999999979, 0.9359439999999921, 0.9131380000000036, 0.35106799999999794, 0.36182700000000523, 2.052135000000007, 0.8193789999999979, 0.3867840000000058, 0.8020479999999992, 0.3208309999999983, 0.351657000000003, 0.6312570000000051, 0.36063900000000615, 0.5858340000000055, 0.3393300000000039, 0.23335600000000056, 0.545345999999995, 0.263898999999995, 0.002562999999994986, 0.7251380000000012, 1.133899999999997, 0.8232259999999911, 0.38127900000000636, 0.40733799999999576, 0.08812200000001269, 0.2706400000000002, 0.29463900000000365, 0.28798100000000204, 0.5042630000000088, 0.9467050000000086, 0.3245540000000062, 0.35645500000001107, 0.4888389999999987, 0.5706250000000068, 0.1804039999999958, 0.40680500000000563, 0.709456000000003, 0.015135000000000787, 0.2789380000000108, 1.0135240000000039, 0.2699709999999982, 0.32936300000000074, 0.7083930000000009, 0.059475999999989426, 0.00197799999999404]

# AB Pruning Depth 3 
# nodes = [86, 5, 16, 5, 24, 1001, 55, 221, 4, 1027, 124, 77, 335, 137, 211, 403, 4, 11, 41, 27, 21, 1486, 654, 65, 49, 772, 22, 63, 216, 48, 364, 11605, 537, 23, 42, 14, 147, 146, 61, 95, 458, 1625, 55, 672, 271, 91, 49, 757, 4, 127, 29, 63, 1719, 71, 100, 127, 103, 76, 103, 12, 127, 64, 1, 160, 18, 97, 8, 143, 127, 292, 704, 367, 13, 52, 8, 63, 148, 8, 225, 107, 61, 18, 226, 4333, 48, 510, 28, 356, 646, 182, 44, 109, 245, 589, 33, 257, 7, 531, 1125, 35, 9, 441, 3, 54, 79, 87, 328, 31, 391, 413, 383, 11, 1058, 18, 183, 69, 56, 74, 74, 1, 22, 711, 8, 196, 374, 45, 106, 2, 480, 32, 821, 675, 55, 1881, 590, 203, 23, 243, 574, 155, 29, 69, 54, 900, 27, 340, 405, 24, 39, 307, 139, 4, 77, 789, 182, 90, 252, 426, 10, 219, 26, 73, 1, 183, 370, 26, 1, 2, 90, 2866, 6, 51, 4, 989, 436, 34, 534, 717, 4, 99] + [3, 30, 28, 62, 14, 32, 13, 577, 1, 396, 41, 59, 59, 447, 36, 117, 31, 219, 40, 11, 5, 21, 38, 2, 92, 477, 56, 177, 158, 1277, 9, 27, 429, 2447, 23, 403, 45, 14, 35, 799, 310, 77, 72, 25, 449, 61, 1, 7, 835, 5, 72, 1, 18, 85, 159, 95, 133, 84, 177, 79, 36, 118, 301, 23, 51, 1, 85, 11, 483, 68, 135, 26, 33, 97, 155, 33, 12, 329, 133, 1, 22, 212, 10, 501, 439, 186, 33, 740, 27, 107, 4, 1151, 333, 23, 14, 25, 109, 168, 209, 83, 41, 239, 301, 305, 65, 69, 183, 597, 717, 69, 291, 250, 48, 17, 176, 53, 65, 72, 1040, 64, 6, 71, 1033, 10, 6, 303, 117, 192, 103, 80, 159, 121, 105, 113, 184, 8, 27, 114, 439, 89, 21, 22, 759, 1073, 10, 41, 21, 301, 5, 653, 9, 2595, 165, 59, 190, 158, 70, 7, 35, 2, 7, 327, 126, 296, 42, 574, 241, 335, 893, 102, 24, 279, 229, 1172, 73, 1457, 116]
# p1scores =[228, 140, 230, 155, 308, 305, 273, 222, 364, 0, 183, 280, 228, 167, 278, 216, 316, 124, 228, 123, 305, 245, 200, 275, 175, 251, 320, 217, 281, 47, 276, 88, 240, 207, 194, 243, 234, 408, 275, 227, 197, 234, 247, 366, 245, 156, 198, 225, 234, 193, 281, 221, 191, 202, 222, 193, 374, 256, 153, 200, 331, 252, 270, 218, 228, 245, 224, 266, 165, 331, 208, 208, 0, 272, 209, 224, 223, 300, 0, 245, 216, 296, 274, 164, 253, 290, 228, 283, 257, 222, 163, 251, 220, 110, 225, 307, 187, 248, 214, 226, 232, 249, 170, 213, 165, 186, 208, 174, 223, 135, 237, 231, 238, 251, 174, 231, 198, 311, 249, 213, 218, 192, 179, 202, 174, 311, 273, 185, 263, 333, 321, 233, 141, 260, 180, 180, 263, 183, 187, 180, 230, 143, 229, 253, 93, 254, 184, 241, 190, 257, 66, 218, 153, 360, 280, 192, 181, 214, 214, 207, 213, 162, 257, 239, 262, 189, 185, 272, 300, 292, 180, 181, 228, 271, 278, 190, 151, 174, 257, 250, 163, 207, 212, 275, 173, 251, 328, 245, 282, 219, 265, 278, 157, 205, 177, 196, 0, 0, 260, 203] + [155, 231, 171, 247, 245, 0, 202, 235, 159, 283, 209, 265, 269, 243, 273, 163, 192, 176, 0, 164, 220, 195, 216, 211, 154, 275, 192, 244, 192, 191, 181, 231, 229, 253, 213, 316, 170, 162, 307, 231, 161, 163, 178, 193, 249, 317, 356, 203, 234, 244, 190, 177, 256, 0, 234, 230, 172, 182, 265, 152, 160, 0, 174, 170, 284, 222, 226, 258, 139, 190, 139, 212, 174, 226, 160, 54, 152, 315, 136, 158, 320, 196, 280, 168, 124, 81, 178, 167, 174, 171, 220, 253, 186, 154, 209, 272, 230, 255, 272, 309, 368, 260, 165, 169, 294, 234, 231, 328, 168, 151, 171, 221, 207, 252, 359, 48, 257, 269, 231, 221, 0, 290, 220, 66, 289, 194, 236, 115, 162, 156, 228, 207, 208, 318, 206, 230, 221, 230, 202, 207, 330, 252, 271, 218, 242, 302, 257, 221, 296, 241, 194, 174, 306, 145, 233, 252, 286, 297, 231, 169, 247, 0, 284, 272, 162, 300, 235, 268, 258, 0, 193, 257, 353, 176, 234, 172, 182, 256, 160, 258, 186, 178, 191, 58, 159, 306, 238, 308, 208, 210, 239, 252, 0, 212, 235, 114, 252, 176, 307, 222]
# p2scores = [234, 234, 251, 241, 175, 253, 229, 235, 204, 0, 268, 228, 319, 282, 181, 195, 207, 133, 238, 87, 156, 223, 217, 170, 276, 186, 191, 183, 209, 41, 150, 195, 267, 207, 228, 301, 275, 176, 273, 290, 255, 221, 233, 218, 208, 173, 274, 295, 219, 246, 197, 212, 202, 119, 243, 282, 114, 229, 221, 193, 235, 228, 193, 182, 319, 199, 221, 242, 197, 150, 211, 166, 0, 179, 225, 238, 193, 182, 0, 276, 270, 192, 202, 191, 241, 184, 217, 115, 167, 220, 230, 226, 180, 223, 252, 209, 345, 137, 230, 222, 280, 248, 327, 226, 309, 207, 192, 264, 247, 252, 246, 222, 244, 170, 283, 249, 233, 171, 241, 253, 245, 217, 243, 289, 202, 143, 173, 200, 239, 266, 245, 293, 239, 151, 236, 151, 185, 190, 227, 225, 223, 206, 171, 283, 67, 238, 219, 267, 265, 210, 99, 283, 214, 146, 256, 328, 268, 358, 218, 252, 179, 116, 301, 260, 239, 282, 314, 197, 234, 222, 209, 124, 223, 190, 211, 224, 224, 235, 197, 142, 206, 178, 262, 228, 183, 185, 212, 280, 155, 233, 249, 252, 348, 315, 217, 205, 0, 0, 307, 225] + [227, 197, 196, 129, 291, 0, 210, 247, 187, 245, 256, 190, 184, 214, 201, 144, 270, 224, 0, 186, 235, 204, 209, 202, 343, 124, 194, 229, 179, 184, 259, 265, 239, 166, 202, 274, 249, 265, 358, 252, 242, 250, 299, 367, 223, 196, 194, 320, 259, 247, 356, 203, 184, 0, 253, 272, 228, 262, 241, 230, 271, 0, 299, 184, 273, 156, 237, 147, 99, 183, 273, 242, 265, 240, 310, 35, 210, 250, 168, 230, 154, 210, 222, 167, 149, 55, 222, 263, 256, 214, 148, 251, 278, 221, 267, 184, 208, 176, 244, 253, 154, 291, 178, 314, 126, 281, 214, 243, 192, 199, 242, 193, 208, 238, 246, 62, 198, 279, 209, 296, 0, 210, 238, 138, 173, 207, 250, 113, 162, 319, 350, 221, 196, 170, 222, 194, 222, 279, 242, 224, 163, 183, 224, 275, 187, 191, 230, 312, 259, 230, 240, 327, 252, 205, 162, 110, 179, 176, 279, 285, 255, 0, 278, 298, 291, 269, 222, 237, 209, 0, 191, 181, 220, 211, 185, 335, 297, 258, 183, 174, 179, 238, 164, 86, 285, 197, 276, 189, 255, 337, 212, 190, 0, 269, 244, 143, 227, 205, 199, 237]
# p1endgamescores = [209, 140, 219, 155, 308, 264, 209, 210, 259, None, 183, 240, 158, 109, 254, 203, 270, None, 211, None, 241, 203, 200, 275, 167, 201, 243, 150, 263, None, 221, None, 177, 160, 181, 188, 188, 333, 259, 173, 173, 222, 186, 321, 239, 147, 174, 158, 162, 193, 254, 208, 182, None, 182, 174, 374, 249, 153, 192, 331, 188, 266, 159, 192, 186, 215, 203, 165, 271, 168, 208, None, 221, 209, 179, 185, 283, None, 192, 169, 224, 220, 164, 208, 238, 228, 217, 203, 177, 121, 236, 210, 110, 178, 244, 183, None, 168, 175, 224, 193, 159, 172, 158, 162, 190, 166, 192, 121, 226, 205, 194, 210, 146, 182, 194, 257, 227, 187, 205, 170, 157, 169, 174, 265, 271, 183, 251, 278, 271, 184, 141, 260, 110, 180, 211, 165, 160, 161, 192, 129, 184, 179, None, 233, 174, 241, 167, 200, None, 203, 147, 304, 203, 173, 137, 201, 156, 143, 207, None, 180, 169, 219, 180, 158, 220, 260, 210, 143, None, 213, 230, 260, 180, 151, 153, 208, 247, 163, 203, 174, 270, 173, 212, 274, 220, 282, 176, 222, 173, 145, 186, 162, 193, None, None, 216, 192] + [155, 196, 171, None, 234, None, 146, 187, 159, 283, 168, 265, 216, 228, 225, None, 156, 157, None, 159, 202, 189, 149, 209, 142, 275, 143, 199, 192, 182, 172, 222, 199, 211, 164, 273, 159, 144, 212, 222, 125, 154, 178, 188, 197, 250, 307, 158, 217, 182, 175, 177, 256, None, 163, 191, 160, 182, 259, None, 144, None, 144, 145, 226, 222, 164, 243, None, 171, 139, 188, 156, 216, 131, None, 152, 289, None, 149, 233, 189, 207, 162, None, None, 164, 137, 144, 148, 220, 199, 170, 154, 179, 228, 230, 196, 203, 281, 288, 248, 165, 156, 252, 225, 202, 300, 168, 141, 160, 212, 180, 193, 291, None, 183, 209, 214, 210, None, 229, 150, None, 232, 130, 214, None, 149, 135, 187, 185, 178, 304, 193, 216, 166, 214, 162, 166, 267, 208, 227, 162, 176, 246, 247, 156, 274, 214, 147, 159, 289, 145, 229, 252, 219, 268, 198, 156, 194, None, 233, 171, 162, 292, 235, 202, 220, None, 173, 257, 293, 164, 230, 158, 166, 214, 160, 246, 186, 178, 177, None, 139, 257, 204, 255, 178, 178, 179, 188, None, 206, 168, 102, 181, 160, 234, 208]
# p2endgamescores = [158, 189, 184, 176, 141, 214, 229, 158, 189, None, 209, 198, 277, 260, 121, 184, 204, None, 183, None, 145, 200, 197, 154, 197, 172, 171, 163, 190, None, 124, None, 230, 181, 159, 272, 233, 168, 175, 230, 180, 176, 187, 194, 202, 163, 227, 268, 210, 246, 187, 197, 187, None, 227, 214, 111, 174, 175, 185, 235, 214, 159, 164, 233, 194, 205, 192, 182, 142, 186, 152, None, 172, 225, 167, 167, 164, None, 266, 233, 157, 185, 191, 221, 149, 194, 86, 146, 191, 182, 159, 151, 223, 219, 190, 305, None, 215, 201, 229, 233, 278, 202, 282, 186, 189, 259, 193, 196, 199, 202, 202, 153, 243, 225, 189, 149, 191, 234, 201, 169, 228, 242, 202, 127, 145, 192, 215, 213, 209, 260, 221, 130, 213, 142, 143, 181, 148, 183, 196, 193, 149, 269, None, 168, 152, 180, 198, 189, None, 275, 212, 142, 226, 275, 232, 312, 182, 205, 157, None, 258, 250, 208, 224, 204, 175, 210, 183, 156, None, 132, 142, 195, 207, 217, 158, 155, 111, 189, 154, 210, 182, 153, 165, 176, 199, 123, 208, 222, 244, 288, 274, 156, 142, None, None, 271, 216] + [216, 159, 196, None, 238, None, 172, 230, 187, 229, 199, 163, 178, 144, 148, None, 187, 209, None, 168, 193, 173, 180, 184, 279, 124, 164, 176, 153, 171, 210, 208, 195, 157, 202, 250, 198, 213, 345, 193, 228, 193, 283, 291, 172, 188, 134, 280, 171, 235, 283, 184, 163, None, 245, 231, 220, 214, 176, None, 222, None, 259, 179, 243, 156, 210, 112, None, 173, 200, 196, 223, 174, 270, None, 196, 210, None, 172, 139, 193, 220, 151, None, None, 174, 251, 227, 149, 132, 218, 262, 195, 188, 132, 208, 169, 229, 193, 132, 225, 148, 269, 96, 228, 165, 174, 175, 195, 238, 193, 166, 213, 246, None, 178, 270, 168, 252, None, 171, 208, None, 150, 200, 176, None, 147, 311, 332, 219, 139, 140, 203, 150, 211, 190, 220, 200, 149, 168, 205, 227, 176, 170, 166, 295, 212, 188, 198, 280, 171, 202, 151, 110, 161, 171, 236, 205, 233, None, 256, 236, 252, 191, 208, 215, 183, None, 176, 116, 197, 165, 176, 262, 188, 228, 161, 124, 179, 217, 157, None, 236, 155, 189, 171, 191, 268, 192, 177, None, 231, 211, 140, 216, 194, 180, 197]
# times = [0.302287, 0.48822600000000005, 0.5163570000000002, 0.16153499999999976, 0.7776640000000001, 0.001535000000000064, 0.29301699999999986, 0.5065280000000003, 0.12990400000000024, 0.32921500000000004, 4.679038, 0.1681329999999992, 3.2305550000000007, 0.5520459999999989, 0.7192220000000002, 0.12025600000000125, 0.6981760000000001, 3.6961950000000012, 0.003185000000001992, 0.48873000000000033, 1.2964649999999978, 0.5333119999999987, 1.792968000000002, 0.6345550000000024, 0.28365699999999805, 0.20615100000000197, 0.3406500000000001, 0.4964759999999977, 0.18752800000000036, 1.0494940000000028, 3.761049, 0.9204159999999995, 1.9074659999999959, 1.5012680000000032, 9.198293, 0.24091200000000157, 0.4045929999999984, 3.628905000000003, 17.396524999999997, 0.3690509999999989, 3.187155000000004, 0.6000769999999989, 0.29019800000000373, 0.48729500000000314, 6.386953000000005, 2.8630199999999917, 0.7748610000000014, 0.7580719999999985, 0.3651010000000099, 3.6552050000000094, 0.7605970000000042, 0.19398900000000197, 0.24668499999999938, 0.0033159999999980982, 6.240561999999997, 0.1838510000000042, 0.8707640000000083, 0.16168700000000058, 0.4301110000000108, 0.16683499999999185, 0.9311519999999973, 0.0026849999999996044, 1.4847460000000012, 1.142407999999989, 1.4954979999999978, 1.0758420000000086, 1.8791650000000004, 0.9196019999999976, 0.10925000000000296, 0.5226569999999953, 0.1593209999999914, 1.157032000000001, 2.5614479999999986, 0.3541659999999922, 0.6173730000000006, 0.059522000000001185, 0.160381000000001, 0.9406049999999908, 0.13094700000000614, 0.25168399999999735, 3.846960999999993, 0.7361259999999987, 1.356601000000012, 0.6552250000000015, 0.11782300000000134, 0.04031500000000676, 0.5775300000000101, 1.1632939999999934, 1.4485340000000093, 0.45620300000000213, 0.27908299999999997, 3.0335510000000028, 1.2548239999999993, 0.17242399999999236, 0.36704799999999693, 2.1102300000000014, 0.29303199999999663, 4.164985999999985, 4.567004999999995, 2.086742000000015, 0.4733879999999999, 5.844267000000002, 0.4267099999999857, 1.209859999999992, 0.2015480000000025, 8.292562000000004, 2.92284699999999, 0.4104629999999929, 0.29586899999998195, 0.46127000000001317, 1.0170789999999954, 1.455286000000001, 1.8845369999999946, 0.8482099999999946, 0.5651830000000189, 0.04415199999999686, 2.0751889999999946, 2.479348000000016, 2.5295759999999916, 0.7797919999999863, 0.0026730000000156906, 0.9183069999999987, 1.6649270000000058, 0.1019019999999955, 4.529155000000003, 5.846455999999989, 0.7741430000000094, 0.08540899999999851, 2.5175160000000005, 2.068481999999989, 0.5640789999999924, 0.428260999999992, 1.7390479999999968, 1.0019539999999836, 0.8226879999999994, 0.9755020000000059, 7.529112999999995, 0.7808420000000069, 0.21134399999999687, 0.7513040000000046, 7.666158999999993, 0.26195699999999533, 0.23442700000001082, 2.399527000000006, 1.2790499999999838, 1.7537809999999752, 1.2455859999999745, 1.0338150000000041, 1.7212069999999926, 1.1864989999999977, 1.219680000000011, 1.1028200000000083, 2.04205300000001, 0.26001099999999155, 0.38013599999999315, 0.15817499999999995, 1.6904879999999878, 4.912279000000012, 0.9157529999999952, 0.3886739999999804, 0.4158909999999878, 0.0027560000000050877, 6.362789000000021, 7.658319000000006, 0.28317899999998986, 0.5404159999999933, 0.47652000000002204, 3.7486769999999865, 0.2283069999999725, 0.0037100000000123146, 4.584168000000034, 0.28847100000001547, 19.02055999999999, 1.7636289999999804, 0.7426980000000185, 2.4264119999999707, 1.9503730000000132, 0.8652650000000222, 0.20177099999995107, 0.5929290000000265, 0.1720860000000357, 0.2188199999999938, 3.2740200000000073, 0.050792999999998756, 1.3619659999999953, 3.1773059999999873, 0.6073489999999993, 4.746735000000001, 2.2632360000000062, 3.4399270000000115, 8.308117999999979, 1.0250589999999988, 0.00169000000005326, 0.39543000000003303, 2.3666280000000484, 1.9741940000000113, 9.370889999999974, 0.8959370000000035, 10.932597000000044, 1.146158000000014] + [0.9818600000000001, 0.18684599999999985, 0.284651, 0.1931339999999997, 0.20349899999999987, 0.37138899999999975, 7.124136999999999, 0.6058829999999986, 2.4041420000000002, 0.003247, 0.1637550000000001, 0.23646800000000034, 7.939729000000002, 1.4012209999999996, 0.877122, 3.031236, 1.5146709999999999, 0.11898400000000109, 1.9030850000000008, 0.12234899999999982, 3.3588829999999987, 0.20842400000000083, 0.2950750000000042, 0.5302830000000043, 0.45972799999999836, 0.3606710000000035, 12.203933, 5.492877, 0.770862000000001, 0.03264899999999926, 0.6282549999999958, 0.17952399999999358, 6.333936999999999, 0.37348300000000023, 0.7375720000000001, 1.793454000000004, 0.5641670000000119, 3.2574770000000086, 74.67717799999998, 4.1415079999999875, 0.4001869999999883, 0.6712049999999863, 0.3119820000000004, 1.284636000000006, 1.3202970000000107, 0.6841909999999984, 1.0290500000000122, 3.697091999999998, 11.638582000000014, 0.6672000000000082, 5.086490999999995, 2.5516369999999995, 0.8975179999999909, 0.1515009999999961, 0.5307719999999847, 5.827793000000014, 0.22040800000002037, 1.4703389999999956, 0.528538999999995, 0.684363999999988, 0.15716900000001033, 11.696953000000008, 0.893558000000013, 0.9925029999999992, 1.231281999999993, 1.0381110000000149, 0.8575850000000003, 1.1173339999999996, 0.29615900000001716, 1.1849320000000034, 0.6783920000000023, 0.1940620000000024, 0.0026720000000182154, 1.5418269999999836, 0.3530099999999834, 1.0412879999999802, 0.26709900000000175, 1.358899000000008, 0.0024579999999900792, 1.183667999999983, 2.0924819999999897, 5.934335000000004, 3.029453999999987, 0.1766280000000222, 0.31619799999998577, 0.5998119999999858, 0.28447099999999637, 0.7415009999999995, 1.376227, 0.248722000000015, 1.985258000000016, 1.0290329999999983, 0.7684800000000109, 0.2983030000000042, 1.9575689999999781, 28.328948000000025, 0.573664000000008, 0.17980899999997746, 4.1246980000000235, 0.5455279999999902, 2.5830419999999776, 5.1338309999999865, 1.8075769999999807, 0.5125810000000115, 1.0577020000000061, 2.2284500000000094, 4.102599999999995, 0.4566859999999906, 2.302823999999987, 0.2938520000000153, 4.157145000000014, 8.024020999999948, 0.5287420000000225, 0.2804760000000215, 3.1981030000000032, 0.22229199999998173, 0.5656019999999558, 1.203959999999995, 1.0350630000000365, 3.2405449999999973, 0.41684399999996913, 2.891042000000027, 3.298088000000007, 3.5374330000000214, 0.2688280000000418, 7.112251000000015, 0.3323740000000157, 1.5135099999999966, 0.675389999999993, 0.7517609999999877, 0.7041050000000268, 0.6652470000000221, 0.19704200000001038, 0.3577319999999986, 5.477946999999972, 0.2305769999999825, 1.974019999999996, 2.7916529999999966, 0.49781999999999016, 1.2191030000000183, 0.17617500000000064, 3.7758499999999913, 0.4338819999999828, 5.4692380000000185, 0.07212999999995873, 5.095369000000005, 0.5802400000000034, 0.20575300000001562, 12.511234000000002, 4.129910999999993, 0.06967699999995602, 1.6817480000000273, 0.39475400000003447, 1.9287759999999707, 5.909084000000007, 1.5200940000000287, 0.4394580000000019, 0.6964069999999651, 0.5813059999999837, 6.31139300000001, 0.40707700000001523, 0.1473760000000084, 2.8344220000000178, 3.150712999999996, 0.4260699999999815, 0.4825100000000475, 2.814973000000009, 1.2284329999999954, 0.19369899999998097, 0.7573419999999942, 6.188214000000016, 0.11435000000000173, 1.5439799999999764, 0.8929660000000013, 2.1043359999999893, 3.371122000000014, 0.27641800000003514, 1.863973000000044, 0.36803000000003294, 0.6705519999999865, 0.15709199999997736, 1.5896569999999883, 2.8591299999999933, 0.36724600000002283, 0.17984000000001288, 0.21805399999999509, 0.9198010000000068, 18.44721400000003, 0.21354800000000296, 0.6144419999999968, 0.17110600000000886, 6.6795529999999985, 3.6269309999999564, 0.42886500000003025, 4.119304, 5.221270000000004, 0.002066000000013446, 0.0019340000000056534, 0.2663430000000062, 0.9184209999999666]

# # Score for ABPruning Depth 4 on rack size 5
# p1scores = [175, 14, 220, 184, 201, 199, 248, 187, 259, 241, 292, 142, 261, 284, 193, 0, 196, 149, 227, 247, 170, 210, 284, 240, 353, 247, 214, 333, 210, 0, 230, 165, 207, 193, 259, 183, 247, 164, 250, 301, 193, 149, 223, 211, 193, 223, 287, 123, 240, 276, 222, 308, 246, 154, 173, 221, 158, 199, 147, 121, 0, 77, 153, 185, 218, 200, 207, 211, 171, 234, 198, 222, 175, 159, 180, 280, 219, 172, 277, 254, 238, 226, 200, 209, 258, 303, 242, 210, 244, 249, 300, 107, 174, 330, 268, 220, 252, 243, 196, 370, 213, 167, 168, 94, 202, 125, 175, 211, 253, 0, 125, 201, 0, 219, 211, 176, 209, 200, 223, 312, 197, 240, 274, 261, 212, 165, 258, 223, 204, 282, 238, 232, 200, 229, 292, 234, 248, 269, 227, 225, 150, 203, 253, 251, 127, 235, 229, 174, 268, 179, 189, 206, 262, 0, 186, 138, 274, 207, 213, 241, 237, 283, 223, 190, 325, 252, 211, 0, 205, 317, 145, 332, 217, 199, 273, 294, 240, 210, 218, 312, 211, 176, 230, 162, 249, 149, 166, 186, 131, 179, 174, 198, 208, 359, 273, 168, 225, 275, 406, 236]
# p2scores = [169, 11, 225, 226, 253, 198, 214, 280, 297, 206, 203, 209, 200, 206, 267, 0, 219, 273, 297, 189, 245, 218, 117, 170, 236, 238, 138, 181, 261, 0, 261, 191, 245, 236, 218, 225, 206, 270, 293, 198, 220, 253, 174, 288, 253, 240, 194, 202, 280, 254, 211, 138, 258, 262, 159, 191, 320, 170, 264, 188, 0, 60, 199, 340, 295, 167, 254, 311, 263, 160, 169, 221, 187, 224, 205, 163, 190, 208, 226, 213, 199, 234, 144, 182, 300, 216, 233, 234, 267, 300, 219, 248, 274, 158, 238, 237, 224, 284, 270, 214, 241, 225, 249, 178, 274, 189, 268, 282, 157, 0, 129, 257, 0, 95, 222, 135, 244, 212, 188, 178, 200, 250, 226, 292, 235, 187, 159, 210, 187, 188, 264, 174, 241, 230, 242, 193, 257, 252, 290, 227, 204, 233, 189, 194, 44, 143, 214, 217, 233, 145, 226, 195, 231, 0, 203, 236, 208, 202, 247, 205, 274, 140, 238, 224, 245, 274, 174, 0, 204, 232, 260, 189, 266, 225, 226, 228, 276, 169, 288, 156, 280, 243, 200, 200, 216, 188, 181, 259, 183, 270, 253, 247, 272, 182, 227, 199, 231, 239, 154, 241]
# p1endgamescores = [166, None, 157, 170, 201, 192, 211, 180, 239, 198, 243, 142, 188, 210, 171, None, 160, 149, 173, 184, 159, 207, 284, 240, 299, 185, 212, 267, 180, None, 182, 165, 141, 139, 194, 174, 247, 152, 221, 240, 193, 142, 223, 211, 181, 212, 248, 123, 228, 208, 222, 255, 191, 154, 173, 154, 150, 199, 124, 121, None, None, 153, 181, 183, 194, 185, 183, 112, 234, 198, 198, 175, 129, 180, 211, 188, 172, 277, 227, 238, 172, 193, 209, 214, 247, 198, 180, 209, 213, 244, None, 138, 330, 187, 180, 237, 178, 187, 304, 157, 155, 152, None, 148, 125, 160, 175, 196, None, None, 187, None, None, 211, None, 174, 200, 184, 255, 159, 157, 229, 247, 212, None, 258, 161, 189, 204, 202, 211, 182, 209, 243, 234, 248, 247, 188, 225, 150, 199, 253, 174, None, 235, 175, 174, 214, None, 189, 206, 249, None, 183, 138, 200, 145, 173, 230, 227, 283, 193, 153, 280, 153, 211, None, 142, 257, None, 278, 191, 199, 228, 249, 179, 200, 204, 258, 178, 152, 170, 148, 197, 149, 151, 171, 124, 146, 162, 186, 208, 301, 223, 168, 164, 257, 340, 190]
# p2endgamescores = [163, None, 196, 209, 207, 177, 192, 192, 191, 180, 147, 209, 193, 194, 205, None, 182, 251, 261, 177, 239, 190, 117, 153, 185, 222, 136, 171, 223, None, 219, 185, 214, 202, 200, 158, 206, 163, 245, 190, 212, 238, 151, 267, 210, 232, 179, 202, 222, 219, 197, 114, 246, 238, 159, 185, 269, 159, 225, 188, None, None, 199, 259, 221, 153, 245, 260, 249, 160, 155, 180, 187, 181, 205, 153, 186, 193, 226, 173, 199, 206, 111, 164, 272, 209, 208, 219, 217, 287, 204, None, 204, 158, 179, 211, 160, 262, 178, 183, 230, 206, 203, None, 258, 172, 193, 235, 143, None, None, 179, None, None, 222, None, 194, 212, 170, 161, 188, 239, 196, 249, 211, None, 159, 180, 187, 181, 195, 160, 184, 183, 213, 166, 206, 180, 241, 206, 204, 218, 119, 178, None, 127, 174, 217, 175, None, 176, 192, 172, None, 193, 236, 187, 180, 194, 193, 131, 140, 189, 152, 232, 260, 174, None, 184, 210, None, 159, 225, 208, 190, 178, 268, 162, 216, 143, 230, 222, 155, 180, 192, 188, 173, 218, 165, 238, 213, 170, 212, 154, 200, 199, 199, 180, 135, 182]
# times = [2.732856, 0.013799000000000117, 0.6212740000000005, 17.461609000000003, 0.15544099999999972, 3.888458, 0.1793350000000018, 0.3636580000000009, 0.7966540000000002, 0.4364050000000006, 0.3475600000000014, 0.4426419999999993, 16.792683999999998, 45.095144999999995, 0.8019570000000016, 0.002502999999990152, 0.7269470000000098, 0.25820200000001137, 1.8612710000000021, 19.421993, 3.292968000000002, 0.6188310000000001, 0.418879000000004, 1.749019000000004, 3.5704350000000034, 7.313006999999999, 0.5017420000000072, 6.908141999999998, 0.8737129999999809, 0.0025660000000016225, 0.5404660000000092, 0.3335399999999993, 5.45119600000001, 0.9017589999999984, 20.26504, 0.703519, 0.4969130000000064, 0.4349410000000091, 1.9195970000000102, 8.028055999999992, 0.28790599999999245, 0.5483049999999992, 0.2805319999999938, 0.29201000000000477, 0.49781300000000783, 4.297392000000002, 2.5112029999999947, 3.0092320000000257, 2.238361999999995, 11.072942000000012, 1.7760710000000017, 0.48068399999999656, 2.9702339999999765, 0.24950999999998658, 0.4291249999999991, 12.07982100000001, 0.5462039999999888, 0.22472299999998313, 0.9820869999999786, 1.1886100000000113, 0.003675000000015416, 0.07956299999997896, 2.8142090000000053, 0.388623999999993, 10.462159000000014, 38.91616700000003, 1.1222400000000334, 4.5918779999999515, 1.1467499999999973, 0.21945599999997967, 0.38036699999997836, 1.5123520000000212, 0.5903450000000134, 0.8667050000000245, 0.48585700000001, 26.411121999999978, 1.29780599999998, 0.2938289999999597, 1.8478240000000028, 0.7368250000000103, 1.1077379999999835, 21.91255000000001, 1.8880800000000022, 2.213452000000018, 3.029463000000021, 3.320968999999991, 0.2863209999999867, 2.8904580000000237, 1.3423369999999863, 7.960946000000035, 6.179254999999955, 0.10425200000003088, 2.6533749999999827, 0.1375290000000291, 0.47113200000001143, 0.993741, 13.20896399999998, 23.886505999999997, 0.38961600000004637, 3.5794860000000313, 3.537679, 5.111327000000001, 53.177969000000004, 0.1302570000000003, 1.8522600000000011, 0.6265039999999971, 4.604827999999998, 0.5654570000000092, 1.4373130000000032, 0.002815999999995711, 0.09244599999999537, 0.40296200000000226, 0.002415999999996643, 0.11714700000000278, 0.24759699999999896, 0.0949499999999972, 1.584856000000002, 2.4273259999999937, 0.18289699999999698, 1.5296809999999965, 14.213572999999997, 4.0379059999999924, 0.2696860000000072, 2.9077939999999955, 2.107904000000005, 0.16219599999999446, 0.5316650000000038, 6.9160169999999965, 0.41877599999999404, 8.157394999999994, 4.251577000000012, 4.231014000000002, 9.600237000000007, 7.453451000000001, 0.5010110000000054, 0.9992290000000139, 0.20955200000000218, 0.9690920000000176, 0.5488789999999995, 0.24547300000000405, 0.6523559999999975, 2.037618999999978, 0.18045399999999745, 18.83436499999999, 0.06877599999998552, 0.38777899999999477, 0.9994989999999859, 3.7605120000000056, 0.8266940000000034, 0.09906100000000606, 0.14770200000000955, 0.14975399999997308, 0.5807059999999922, 0.002221999999989066, 1.9387200000000178, 0.7457660000000033, 11.175792999999999, 0.6206999999999994, 3.6378159999999866, 3.3504609999999957, 4.76576399999999, 1.0963680000000124, 14.936075000000017, 1.338105000000013, 0.45994799999999714, 1.2010209999999972, 0.35091800000000717, 0.002596000000011145, 9.615527000000014, 15.125898999999976, 0.14351299999998446, 8.531500999999992, 11.296021999999994, 0.23752799999999752, 0.43902700000001005, 0.405838000000017, 2.0098749999999654, 9.548711000000026, 1.9105750000000512, 5.05331000000001, 0.6292460000000233, 0.34711999999996124, 0.917568000000017, 2.972601999999995, 0.4131250000000364, 0.9787170000000174, 3.019792999999993, 0.3610109999999622, 1.3573409999999626, 0.3707560000000285, 8.371876000000043, 6.125440000000026, 0.1835360000000037, 19.582659999999976, 9.010859000000039, 0.3678770000000213, 3.2456080000000043, 0.36412599999999884, 3.174400999999989, 1.2992909999999824]


#score for ABPruning Depth 5 on rack size 5

# nodes1 = [2030, 15, 51, 67, 94, 27, 868, 862, 37, 8, 561, 2, 439, 375, 930, 136, 898, 20, 91, 21, 39, 27, 285, 219, 38, 52, 109, 4, 63, 628, 929, 237, 2169, 13800, 151, 1184, 12, 71, 11, 417, 3589, 2811, 225, 32, 14, 31, 11, 95, 15, 46, 29, 67, 71, 2097, 619, 588, 39, 2600, 624, 50, 41, 169, 5, 39, 765, 48, 44, 4, 81, 8, 92, 1, 72, 84, 73, 201, 66, 58, 59, 372, 153, 146, 17, 28, 16, 711, 610, 11]
# p1scores1 = [170, 188, 237, 232, 214, 253, 279, 239, 203, 171, 167, 284, 304, 225, 214, 201, 152, 252, 365, 193, 219, 169, 252, 190, 267, 251, 222, 211, 290, 163, 272, 253, 195, 174, 64, 212, 312, 247, 192, 146, 208, 200, 230, 274, 0, 228, 286, 183, 178, 180, 376, 258, 150, 180, 200, 124, 249, 266, 291, 227, 222, 153, 182, 135, 190, 201, 275, 196, 317, 0, 0, 220, 238, 186, 209, 236, 255, 219, 205, 228, 22, 0, 191, 202, 253, 218, 200, 305, 285, 249, 256, 233, 255, 0, 155, 126, 287, 203, 169, 256]
# p2scores1 =[260, 184, 203, 231, 269, 195, 237, 266, 255, 222, 165, 181, 191, 251, 225, 244, 264, 283, 188, 220, 180, 187, 252, 225, 271, 221, 150, 259, 202, 193, 217, 230, 173, 288, 113, 247, 225, 189, 245, 192, 175, 287, 243, 267, 0, 213, 218, 236, 205, 253, 156, 184, 304, 315, 275, 160, 168, 272, 175, 170, 266, 190, 230, 363, 243, 273, 227, 183, 184, 0, 0, 246, 203, 303, 281, 240, 215, 189, 213, 302, 20, 0, 184, 194, 184, 169, 178, 196, 278, 247, 223, 173, 184, 0, 224, 178, 175, 217, 208, 165]
# p1endgamescores1 = [133, 188, 237, 176, 186, 243, 279, 213, 147, 162, 167, 216, 258, 189, 214, 182, 152, 252, 305, 193, 219, 169, 239, 162, 256, 230, 222, 203, 253, 163, 229, 230, 148, 165, None, 186, 235, 208, 126, 146, 208, 200, 230, 264, None, 166, 227, 166, 178, 180, 322, 258, 150, 160, 200, 124, 203, 242, 200, 168, 204, 153, 182, 114, 169, 166, 214, 196, 280, None, None, 204, 232, 170, 158, 236, 215, 219, 205, 210, None, None, 191, 156, 253, 218, 194, 254, 218, 192, 207, 222, 255, None, 155, 126, 249, 125, 164, 214]
# p2endgamescores1 = [185, 167, 203, 227, 196, 132, 190, 240, 204, 192, 165, 169, 174, 233, 225, 224, 264, 283, 160, 208, 180, 170, 183, 167, 228, 164, 120, 196, 185, 158, 188, 173, 167, 243, None, 179, 195, 143, 202, 192, 163, 287, 189, 235, None, 196, 176, 187, 178, 227, 138, 148, 230, 272, 275, 160, 149, 225, 173, 149, 219, 190, 230, 284, 184, 230, 199, 162, 160, None, None, 196, 203, 206, 232, 178, 198, 189, 213, 229, None, None, 160, 166, 184, 152, 151, 149, 268, 243, 195, 173, 184, None, 224, 178, 143, 177, 182, 138]
# times1  =[16.112638, 0.31204600000000227, 0.6796920000000028, 0.7734100000000019, 0.9705629999999985, 0.46329000000000065, 0.17553400000000252, 8.127403000000001, 7.779152, 0.5737299999999976, 0.23147000000000162, 4.792618999999995, 0.2497279999999975, 5.353186999999998, 3.664079000000001, 8.297562, 1.6148019999999974, 0.2074479999999994, 7.665702000000003, 0.3802950000000038, 1.2186629999999923, 0.39734699999999634, 0.5429369999999949, 0.3928499999999957, 2.7380150000000043, 2.2847989999999925, 0.5711359999999956, 0.6492369999999994, 1.601081999999991, 0.23464300000000549, 0.7281010000000094, 5.839645000000004, 8.009985, 2.2360089999999957, 0.03794600000000514, 18.62865500000001, 106.599591, 1.4785849999999812, 10.312135000000012, 0.2718900000000133, 0.8596039999999903, 0.19486899999998286, 0.24861400000000344, 3.8706320000000005, 0.0023579999999867596, 29.138779999999997, 23.555926999999997, 2.274694000000011, 0.5276250000000005, 0.27002799999996796, 0.41291000000001077, 0.25529799999998204, 0.17174199999999473, 0.9664579999999887, 0.3229390000000194, 0.6177310000000489, 0.3732360000000199, 0.7111439999999902, 0.8822620000000256, 16.29657199999997, 5.067601999999965, 5.20061800000002, 0.5240180000000123, 21.185518000000002, 5.6123850000000175, 0.6041490000000067, 0.5517729999999688, 1.9459439999999972, 0.21777700000001232, 0.0031510000000025684, 0.0023370000000113578, 0.5712520000000154, 6.16274999999996, 0.5592800000000011, 0.4912060000000338, 0.1607930000000124, 0.1942860000000337, 0.847802999999999, 0.2699929999999995, 0.8795339999999783, 0.025317999999970198, 0.0020590000000311193, 0.17303100000003724, 0.7059210000000462, 1.0945910000000367, 0.877018000000021, 1.789291999999989, 0.7205609999999751, 0.6656090000000177, 0.8197440000000142, 3.1443270000000325, 1.449218999999971, 1.6580999999999904, 0.003945999999984906, 0.365415999999982, 0.46404300000000376, 0.31528900000000704, 6.125376000000017, 5.259367999999995, 0.25710000000003674]


# nodes2 = [1, 1, 20, 7433, 21, 10733, 357, 186, 231, 990, 421, 5195, 33, 400, 20, 197, 81, 157, 30, 84, 1, 14, 5147, 838, 289, 80, 361, 699, 8385, 374, 846, 683, 6, 88, 43, 215, 193, 11, 8, 37, 1780]
# p1scores2 =[147, 129, 0, 147, 316, 194, 234, 237, 316, 240, 207, 251, 199, 197, 0, 271, 242, 254, 231, 277, 233, 146, 305, 181, 186, 266, 232, 214, 78, 247, 0, 281, 217, 200, 149, 177, 0, 248, 228, 229, 174, 212, 0, 223, 186, 250, 0, 186, 331, 266]
# p2scores2 =[218, 278, 0, 274, 200, 144, 173, 182, 178, 236, 286, 312, 158, 189, 0, 246, 235, 161, 245, 214, 237, 221, 144, 162, 136, 162, 137, 241, 113, 232, 0, 178, 303, 313, 62, 225, 0, 212, 186, 225, 206, 262, 0, 159, 219, 197, 0, 240, 228, 168]
# p1endgamescores2 =[147, 129, None, 136, 253, 194, 169, 165, 260, 176, 138, 240, None, 138, None, 255, 182, 197, 228, 197, 153, 146, 258, 181, 186, 256, 154, 202, None, 236, None, 211, 198, 166, None, 169, None, 188, 177, 184, 169, 161, None, 177, 145, 194, None, 186, 311, 211]
# p2endgamescores2 =[203, 270, None, 207, 187, 144, 167, 168, 128, 223, 269, 305, None, 171, None, 155, 214, 146, 219, 196, 231, 191, 117, 146, 125, 133, 129, 182, None, 199, None, 150, 230, 251, None, 211, None, 205, 165, 189, 198, 223, None, 137, 174, 165, None, 226, 177, 154]
# times2= [0.27102499999999996, 0.206704, 0.003219000000000083, 0.339874, 73.975264, 0.32704999999999984, 80.912725, 3.3703629999999976, 2.0692709999999863, 2.395210999999989, 10.872302999999988, 4.510770000000008, 0.1395480000000191, 48.527783, 0.0015649999999993724, 0.4957629999999824, 5.709429999999998, 0.3313050000000146, 2.196942000000007, 0.9056879999999978, 1.7203229999999792, 0.46023599999998055, 0.83063199999998, 0.17995000000001937, 0.3007690000000025, 44.54222200000001, 7.979903999999976, 2.9357029999999895, 0.0614140000000134, 0.9921439999999961, 0.0038780000000429027, 3.3685120000000097, 6.164650999999992, 62.014320999999995, 0.0810989999999947, 4.158075999999994, 0.0031409999999709726, 7.932039000000032, 5.6581540000000246, 0.31328999999999496, 0.9587760000000003, 0.5008419999999774, 0.0027319999999804168, 2.3414569999999912, 1.9283489999999688, 0.2215190000000007, 0.002453000000002703, 0.2446790000000192, 0.4796670000000063, 15.450380999999993]

# nodes = nodes1 + nodes2
# p1scores = p1scores1 + p1scores2
# p2scores = p2scores1 + p2scores2
# p1endgamescores = p1endgamescores1 + p1endgamescores2
# p2endgamescores = p2endgamescores1 + p2endgamescores2
# times = times1 + times2


# No max depth for AB pruning on rack size 5
# nodes1 = [2918, 19, 20, 167, 223, 24, 66, 239, 901, 34, 5, 439, 14, 34, 667, 170, 79, 670, 308, 66, 62, 253, 343, 4, 79, 2440, 54, 2283, 92, 206, 433, 3, 43, 938, 54, 10, 197, 2857, 75, 13, 105, 44, 119, 19, 69, 4487, 1236, 25, 10, 2, 199, 981, 96, 12, 815, 53, 726, 61, 3301, 69, 1665, 1018, 2148, 120432, 49, 262, 27, 528, 75, 2508, 65, 43, 60, 109, 32, 1664, 287, 2759, 1428, 246, 128, 90, 898, 20, 371, 56, 13, 8, 1196, 6033, 129, 13, 1399]
# p1scores1 = [180, 215, 196, 195, 195, 235, 184, 220, 231, 195, 197, 212, 263, 30, 187, 161, 293, 218, 268, 273, 173, 280, 203, 243, 235, 305, 255, 197, 191, 255, 170, 228, 213, 234, 163, 189, 355, 214, 212, 167, 225, 214, 206, 158, 145, 264, 239, 259, 224, 182]
# p2scores1 = [261, 276, 286, 203, 278, 182, 178, 177, 198, 209, 189, 200, 187, 22, 112, 229, 206, 188, 268, 220, 194, 171, 158, 260, 179, 224, 253, 256, 186, 186, 288, 267, 323, 274, 202, 230, 211, 268, 208, 178, 171, 255, 161, 184, 53, 144, 197, 220, 262, 251]
# p1endgamescores1 = [104, 170, 196, 181, 184, 190, 184, 210, 220, 195, 190, 158, 263, None, None, 161, 222, 215, 256, 264, 173, 229, 203, 221, 165, 265, 216, 177, 191, 200, 158, 213, 206, 195, 163, 165, 297, 214, 212, 167, 211, 206, 206, 158, None, 264, 234, 196, 170, 156]
# p2endgamescores1 = [230, 258, 228, 160, 222, 149, 178, 147, 189, 200, 150, 187, 156, None, None, 229, 200, 188, 178, 172, 194, 157, 133, 196, 169, 210, 220, 184, 186, 180, 213, 147, 277, 236, 202, 183, 205, 244, 179, 178, 129, 178, 161, 184, None, 123, 180, 172, 252, 232]
# times1 = [28.457569, 0.33411299999999855, 0.3422729999999987, 1.7242899999999999, 1.9751229999999964, 0.34948599999999885, 0.8178440000000009, 2.284306000000001, 8.470917, 0.49538099999999474, 0.2556449999999941, 4.728405000000002, 0.34807999999999595, 0.023735999999999535, 0.12552500000000322, 0.483984999999997, 7.080171, 1.9386399999999995, 0.9097670000000022, 6.730280999999998, 2.9818090000000126, 0.7033690000000092, 0.72840699999999, 2.370058, 4.470228999999989, 0.2786049999999989, 0.8592219999999884, 20.823213999999993, 0.6710889999999949, 17.893317999999994, 0.8816439999999943, 1.7517409999999956, 3.9330260000000123, 0.20288299999999992, 0.6357670000000013, 7.664194999999992, 0.7840810000000147, 0.3089580000000183, 1.8615410000000168, 24.267165000000006, 0.848608999999982, 0.27916300000001115, 1.231020000000001, 0.5714190000000201, 0.04297500000001264, 1.3442199999999787, 0.3977549999999894, 0.8133920000000217, 36.14156200000002, 10.697302999999977, 0.5683670000000001, 0.26924099999999984, 0.17733, 2.02914, 8.453273, 0.8671249999999997, 0.2741490000000013, 8.405273000000001, 0.003088000000001756, 0.7832660000000011, 6.770484, 0.001788000000001233, 0.7870349999999995, 32.759317, 0.8810690000000037, 15.959770999999996, 9.082476, 23.287146000000007, 899.159576, 0.6069360000000188, 2.3854630000000725, 0.4569099999999935, 0.13301100000001043, 4.110847000000035, 0.874349000000052, 20.135546999999974, 0.7187790000000405, 0.4951149999999416, 0.675617000000102, 1.151836000000003, 0.4612999999999374, 13.440222000000176, 0.0017380000001594453, 2.7643929999999273, 22.713453000000072, 11.432960999999978, 2.4568299999998544, 1.368257000000085, 0.9997370000000956, 7.1294339999999465, 0.3538969999999608, 3.9655290000000605, 0.6393659999998818, 0.26812500000005457, 0.23288300000012896, 9.49111599999992, 55.96718499999997, 1.2186229999999796, 0.28220200000009754, 10.691080000000056]

# nodes2 = [499, 1628, 990, 2, 13, 133, 17, 1173, 60, 5, 1573, 333, 12296, 31539, 51, 35, 229, 61, 1644, 129, 52, 55, 2175, 41, 137, 73, 7, 12, 41, 993, 14, 23, 15, 670, 505, 7139, 23, 17, 492, 2, 7498, 17, 4, 859, 4331, 8, 3, 165, 3847]
# p1scores2 = [226, 198, 227, 200, 237, 209, 153, 247, 177, 143, 222, 230, 184, 321, 220, 229, 261, 185, 226, 233, 258, 145, 154, 258, 250, 186, 178, 127, 240, 266, 246, 205, 289, 224, 256, 246, 224, 244, 198, 147, 241, 244, 235, 308, 192, 261, 202, 274, 295, 267]
# p2scores2 = [225, 190, 209, 284, 179, 162, 169, 211, 202, 212, 258, 230, 213, 229, 220, 133, 169, 295, 206, 233, 320, 267, 276, 253, 305, 229, 216, 237, 178, 318, 319, 240, 195, 190, 200, 175, 210, 201, 206, 238, 205, 218, 226, 199, 231, 206, 149, 204, 234, 255]
# p1endgamescores2 = [167, 190, 167, 200, 237, 209, 153, 198, 177, 143, 147, 202, 184, 260, 200, 229, 261, 155, 164, 233, 243, 115, 150, 173, 240, 168, 178, 127, 240, 249, 227, 195, 248, 224, 248, 240, 167, 184, 198, 120, 204, 175, 190, 265, 192, 203, 202, 230, 280, 171]
# p2endgamescores2 = [219, 185, 195, 266, 163, 162, 169, 194, 202, 200, 233, 175, 213, 215, 178, 119, 145, 201, 178, 155, 234, 226, 215, 204, 238, 185, 172, 183, 157, 252, 204, 154, 175, 170, 133, 167, 175, 168, 194, 184, 169, 188, 206, 183, 231, 184, 149, 185, 154, 186]
# times2 = [5.462273, 13.628348999999998, 9.027452999999998, 0.20035800000000137, 0.350676, 1.452566000000001, 0.3063469999999988, 9.485683000000002, 0.7841099999999983, 0.20266799999999563, 17.036028, 3.118584999999996, 102.818501, 242.39040999999997, 0.5522730000000138, 0.5206039999999916, 2.3233899999999608, 0.7285350000000221, 14.949749999999995, 0.17539299999998548, 1.184301000000005, 0.577241000000015, 0.6432050000000231, 20.644333999999958, 0.5160099999999943, 1.323853999999983, 0.8926519999999982, 0.2601789999999937, 0.33121899999997595, 0.4770679999999743, 8.132326000000035, 0.31665900000001557, 0.39193999999997686, 0.3116600000000176, 6.508721000000037, 5.085165000000018, 55.82077000000004, 0.3661329999999907, 0.4054830000000038, 4.384490000000028, 0.16655200000002424, 70.42404600000009, 0.35272199999997156, 0.2003789999999981, 8.768035000000054, 35.55618399999992, 0.24955899999997655, 0.1856559999999945, 1.568037000000004, 29.536572999999976]
# nodes = nodes1 + nodes2
# p1scores = p1scores1 + p1scores2
# p2scores = p2scores1 + p2scores2
# p1endgamescores = p1endgamescores1 + p1endgamescores2
# p2endgamescores = p2endgamescores1 + p2endgamescores2
# times = times1 + times2



# PVS Search
# nodes = [1, 13, 3, 101, 3, 4, 16, 11, 26, 6, 44, 43, 23, 3, 3, 9, 133, 69, 24, 13, 1098, 22, 51, 3, 282, 19, 22, 93, 12, 9, 139, 307, 4, 11, 16, 3, 10, 5, 34, 153, 15, 11, 47, 14, 76, 25, 6, 69, 18, 6, 22, 14, 141, 7, 3, 12, 286, 31, 45, 4, 39, 78, 21, 8, 12, 11, 20, 6, 3, 468, 177, 17, 215, 6, 32, 20, 4, 15, 55, 8, 7, 60, 301, 3, 6, 12, 1542, 111, 11, 19, 8, 3, 269, 44, 82, 88, 5, 11, 1368, 6, 39, 12, 13, 49, 55, 11, 3, 19, 6, 36, 12, 35, 23, 29, 213, 22, 23, 9, 6, 6, 455, 3, 8, 2, 83, 12, 137, 74, 38, 15, 4, 3, 6, 11, 3, 3, 34, 28, 21, 17, 51, 3, 7, 29, 67, 11, 6, 30, 35, 476, 3, 3, 49, 17, 6, 3, 224, 9, 57, 3, 10, 43, 164, 33, 13, 138, 41, 12, 3, 20, 169, 167, 3, 3, 13, 14, 20, 3, 50, 3, 146, 77, 3, 3, 194, 7, 11, 3, 8, 8, 6, 181, 11, 11, 15, 28, 40, 14, 261, 3, 18, 47, 10, 14, 4, 236, 6, 6, 6, 77, 57, 3, 18, 54, 41, 3, 3, 592, 15, 62, 29, 47, 1, 5, 55, 31, 7182, 283, 167, 18, 62, 14, 233, 32, 11, 30, 149, 3, 22849, 141, 130, 6, 24, 31, 11, 83, 262, 204, 5, 9, 71, 74, 3, 12, 61, 41, 38, 4, 49, 15, 21, 4, 123, 14, 3, 4, 15, 284, 49, 553, 15, 41, 5, 37, 29, 92, 11, 13, 39, 5, 57, 129, 21, 104, 25, 66, 13, 81, 3, 3, 8, 3, 192, 28, 93, 17107, 53, 135, 53, 89, 7, 6, 23, 9, 80, 408, 125, 36, 56, 1, 204, 7, 13, 3, 12, 18, 3, 30, 25, 7, 22, 581, 6, 2, 3, 110, 6, 164, 57, 6, 10, 3, 3, 6, 3, 7, 3, 25, 3, 3, 1, 102, 1, 1, 13, 5, 15, 3, 250, 11, 223, 101, 16, 4, 90, 19, 75, 9, 25, 3, 40, 5, 7, 16, 69, 5, 6, 3, 6, 22, 3, 3, 54, 8, 69, 6, 3, 3, 524, 17, 27, 9, 56, 6, 16, 17, 48, 3, 376, 1, 56, 30, 21, 378, 48, 27, 23, 3, 821, 12, 6, 31, 12, 8, 284, 35, 172, 59, 36, 1652, 122, 19, 41, 27, 20, 11, 325, 13, 13, 133, 27, 3, 3, 91, 3, 3, 1, 385, 27, 47, 7, 127, 3, 26, 232, 20, 18, 3, 9, 84, 11, 3, 3, 116, 14, 53, 32, 22, 15]
# p1scores =[218, 166, 280, 234, 259, 315, 0, 261, 282, 258, 224, 198, 246, 0, 185, 204, 144, 153, 214, 235, 188, 108, 223, 227, 161, 260, 138, 220, 321, 287, 240, 73, 250, 203, 260, 191, 225, 182, 188, 228, 240, 171, 183, 216, 235, 180, 273, 165, 229, 226, 209, 137, 213, 299, 254, 241, 205, 320, 230, 227, 227, 214, 299, 187, 181, 251, 309, 140, 181, 195, 286, 139, 201, 281, 224, 212, 230, 152, 174, 267, 165, 115, 208, 156, 0, 262, 279, 202, 267, 197, 220, 263, 246, 118, 216, 157, 150, 337, 172, 49, 276, 193, 145, 217, 200, 276, 227, 219, 290, 189, 243, 251, 200, 39, 286, 318, 326, 268, 214, 169, 197, 235, 249, 338, 176, 168, 226, 212, 248, 257, 255, 0, 0, 172, 278, 192, 244, 182, 243, 200, 189, 191, 268, 215, 270, 259, 156, 304, 196, 271, 194, 255, 309, 286, 121, 229, 293, 226, 176, 165, 305, 376, 150, 227, 235, 112, 31, 202, 219, 152, 337, 269, 195, 240, 16, 241, 237, 163, 257, 169, 210, 298, 257, 232, 268, 290, 316, 256, 202, 174, 181, 244, 165, 284, 238, 193, 194, 264, 207, 233, 280, 187, 257, 212, 239, 258, 189, 235, 229, 226, 270, 208, 0, 0, 278, 229, 278, 184, 215, 263, 209, 188, 222, 166, 272, 0, 286, 223, 343, 325, 171, 246, 313, 164, 381, 311, 242, 238, 214, 141, 284, 196, 310, 297, 202, 195, 226, 249, 243, 163, 224, 263, 172, 161, 288, 205, 179, 156, 380, 260, 269, 223, 208, 0, 252, 189, 0, 257, 0, 190, 313, 234, 186, 187, 164, 272, 154, 168, 148, 60, 187, 0, 250, 194, 31, 325, 174, 261, 0, 232, 227, 279, 233, 166, 225, 284, 346, 196, 234, 0, 209, 180, 258, 207, 175, 215, 172, 233, 189, 260, 190, 174, 248, 256, 255, 126, 228, 229, 164, 260, 0, 288, 279, 44, 190, 218, 226, 135, 273, 229, 237, 250, 233, 254, 203, 288, 188, 171, 191, 280, 258, 196, 238, 160, 294, 203, 231, 265, 180, 223, 265, 108, 321, 0, 270, 144, 212, 272, 230, 290, 197, 249, 300, 149, 198, 191, 221, 280, 200, 224, 198, 168, 146, 207, 0, 241, 9, 191, 144, 290, 293, 213, 269, 215, 199, 195, 254, 202, 20, 204, 259, 235, 153, 161, 202, 264, 208, 280, 142, 250, 141, 142, 99, 190, 232, 190, 218, 329, 201, 185, 35, 181, 257, 218, 192, 213, 262, 242, 160, 149, 268, 214, 182, 166, 275, 205, 289, 216, 256, 249, 202, 204, 277, 307, 233, 273, 200, 294, 0, 198, 220, 123, 181, 147, 257, 251, 178, 250, 203, 205, 136, 0, 230, 289, 221, 197, 212, 207, 274, 293, 152, 178, 323, 202, 134, 241, 180, 191, 170, 222, 149, 249, 242, 204, 251, 309, 301, 322, 136, 224, 176, 204, 291, 279, 203, 238, 252, 283, 285, 238, 323, 241, 260, 194, 210, 248, 0, 238, 247, 185]
# p2scores =[189, 196, 243, 218, 227, 220, 0, 259, 216, 248, 316, 289, 184, 0, 146, 141, 184, 198, 186, 261, 170, 94, 184, 367, 178, 332, 233, 200, 254, 238, 195, 95, 192, 305, 263, 156, 306, 248, 203, 215, 148, 246, 248, 118, 247, 276, 195, 243, 309, 273, 335, 174, 255, 268, 215, 140, 252, 221, 205, 226, 243, 149, 112, 269, 199, 219, 221, 96, 290, 143, 164, 222, 209, 171, 250, 264, 264, 263, 221, 215, 265, 158, 219, 201, 0, 240, 246, 195, 196, 290, 250, 264, 267, 192, 271, 275, 240, 222, 193, 12, 177, 178, 184, 160, 248, 190, 262, 200, 140, 208, 298, 215, 250, 24, 173, 219, 197, 256, 386, 200, 218, 279, 196, 179, 292, 215, 225, 186, 201, 173, 266, 0, 0, 196, 160, 212, 154, 143, 312, 170, 212, 120, 222, 202, 195, 239, 208, 186, 195, 177, 206, 153, 168, 153, 188, 191, 256, 255, 261, 314, 297, 181, 195, 284, 179, 98, 33, 155, 246, 159, 186, 274, 226, 184, 35, 224, 189, 232, 203, 192, 220, 159, 232, 206, 237, 274, 182, 134, 187, 185, 217, 163, 197, 197, 184, 180, 248, 186, 232, 261, 181, 198, 258, 151, 206, 190, 221, 238, 235, 209, 193, 266, 0, 0, 129, 280, 196, 202, 225, 218, 259, 239, 164, 242, 282, 0, 219, 198, 193, 249, 301, 176, 245, 215, 203, 187, 220, 261, 230, 239, 189, 217, 261, 168, 321, 195, 242, 263, 268, 224, 208, 171, 332, 258, 209, 238, 223, 274, 179, 192, 214, 250, 121, 0, 223, 316, 0, 290, 0, 294, 204, 215, 311, 184, 317, 157, 276, 230, 145, 136, 218, 0, 195, 205, 25, 167, 197, 192, 0, 242, 171, 290, 195, 160, 193, 330, 258, 249, 293, 0, 196, 206, 198, 217, 223, 160, 222, 247, 178, 212, 177, 227, 250, 196, 190, 261, 271, 283, 215, 237, 0, 203, 233, 153, 184, 202, 284, 227, 186, 210, 194, 277, 225, 176, 170, 245, 369, 181, 227, 230, 251, 330, 199, 205, 252, 308, 197, 195, 244, 230, 171, 274, 206, 0, 136, 178, 220, 294, 221, 125, 257, 304, 303, 252, 274, 179, 268, 173, 173, 191, 236, 229, 214, 278, 0, 197, 17, 327, 129, 186, 208, 230, 223, 160, 139, 191, 183, 252, 11, 238, 207, 126, 186, 263, 187, 209, 190, 263, 305, 241, 291, 202, 148, 214, 246, 209, 188, 271, 213, 240, 46, 305, 275, 315, 230, 125, 214, 223, 199, 204, 237, 209, 169, 351, 191, 238, 219, 234, 311, 220, 245, 192, 224, 160, 233, 170, 200, 249, 0, 167, 188, 175, 231, 238, 252, 243, 139, 285, 281, 283, 185, 0, 260, 181, 204, 278, 312, 233, 156, 215, 348, 230, 146, 212, 205, 286, 197, 300, 249, 222, 266, 166, 166, 225, 217, 192, 205, 231, 161, 221, 360, 250, 170, 198, 184, 255, 250, 222, 206, 169, 235, 159, 301, 184, 296, 215, 0, 149, 218, 345]
# p1endgamescores =[218, 166, 234, 166, 212, 270, None, 229, 211, 194, 199, 166, 201, None, 185, 204, 144, 153, 214, 178, 133, None, 223, 165, 161, 210, 138, 220, 305, 194, 240, None, 203, 203, 216, 191, 183, 182, 188, 184, 231, 158, 183, None, 202, 168, 222, 165, 219, 152, 201, 137, 206, 203, 225, 241, 156, 250, 230, 146, 188, 214, 299, 158, 181, 180, 269, None, 112, 195, 205, 139, 150, 226, 208, 194, 171, 152, 174, 193, 154, None, 195, 156, None, 262, 230, 158, 241, 189, 206, 189, 181, 118, 204, 157, 136, 329, 172, None, 276, 193, 145, 217, 190, 212, 164, 219, 246, 189, 213, 251, 191, None, 207, 246, 278, 251, 150, 169, 150, 219, 188, 287, 160, 168, 154, 143, 197, 202, 236, None, None, 172, 278, 178, 244, 182, 201, 200, 173, None, 218, 205, 207, 188, 145, 263, 196, 231, 176, 255, 309, 264, 121, 229, 285, 201, 176, 165, 290, 293, 150, 182, 219, None, None, 202, 210, 152, 255, 258, 167, 191, None, 196, 237, 163, 218, 169, 210, 207, 212, 221, 166, 267, 305, 209, 202, 174, 145, 181, 165, 231, 189, 193, 194, 212, 149, 193, 210, 158, 210, 212, 176, 188, 176, 188, 182, 226, 206, 185, None, None, 278, 229, 207, 184, 182, 224, 196, 188, 222, 136, 266, None, 276, 223, 275, 308, 134, 246, 251, 149, 340, 311, 179, 227, 167, 141, 193, 149, 257, 211, 163, 195, 202, 225, 189, 163, 157, 198, 167, 151, 226, 183, 150, 135, 335, 195, 269, 210, 208, None, 192, 189, None, 241, None, 155, 246, 223, 164, 134, 161, 231, 139, 153, 148, None, 187, None, 199, 194, None, 263, 174, 190, None, 164, 227, 236, 168, 166, 160, 239, 303, 160, 218, None, 189, 166, 209, 137, 175, 215, 172, 200, 189, 178, 190, 164, 232, 216, 235, 126, 218, 203, 164, 236, None, 232, 279, None, 190, 173, 142, 135, 273, 214, 163, 162, 187, 254, 203, 271, 177, 171, 175, 222, 211, 154, 238, 160, 241, 195, 206, 221, 180, 223, 223, 94, 296, None, 201, 144, 212, 251, 187, 252, 156, 149, 253, 149, 160, 191, 221, 217, 200, 224, 178, 168, 146, 207, None, 197, None, 181, None, 290, 237, 213, 193, 215, 199, 195, 254, 182, None, 204, 259, 222, 153, 155, 188, 181, 148, 223, 142, 237, 141, 142, None, 144, 218, 178, 206, 305, 184, 161, None, 163, 208, 215, 156, 213, 216, 228, 160, 139, 229, 168, 182, 145, 224, 192, 244, 194, 236, 249, 202, 204, 208, 267, 183, 273, 139, 248, None, 187, 203, 123, 152, 136, 210, 190, 178, 248, 184, 157, 136, None, 192, 210, 215, 183, 186, 174, 223, 279, 145, 168, 258, 202, 134, 227, 180, 191, 170, 222, 134, 201, 242, 183, 187, 262, 301, 251, 136, 197, 143, 195, 291, 207, 203, 217, 163, 239, 237, 189, 263, 241, 218, 194, 157, 202, None, 238, 236, 158]
# p2endgamescores= [183, 196, 222, 205, 206, 191, None, 216, 202, 219, 277, 225, 165, None, 124, 141, 184, 198, 175, 233, 161, None, 184, 354, 178, 293, 233, 200, 184, 227, 177, None, 167, 305, 181, 144, 277, 248, 179, 170, 139, 179, 203, None, 194, 254, 158, 225, 187, 249, 278, 174, 182, 223, 158, 128, 199, 207, 178, 167, 201, 127, 112, 226, 199, 203, 187, None, 278, 143, 146, 205, 187, 151, 223, 224, 237, 263, 221, 175, 212, None, 146, 187, None, 220, 228, 190, 144, 224, 172, 228, 254, 169, 214, 261, 170, 153, 166, None, 157, 178, 184, 160, 207, 176, 233, 200, 114, 197, 240, 170, 166, None, 159, 192, 186, 216, 295, 200, 160, 189, 146, 134, 230, 188, 208, 182, 189, 166, 207, None, None, 177, 160, 141, 154, 136, 270, 164, 167, None, 217, 162, 169, 219, 197, 176, 162, 162, 157, 135, 155, 147, 188, 153, 196, 213, 261, 282, 234, 158, 195, 259, 121, None, None, 155, 221, 139, 151, 259, 172, 170, None, 199, 189, 215, 156, 169, 187, 149, 183, 189, 230, 227, 167, 123, 171, 169, 193, 135, 197, 179, 155, 180, 248, 170, 187, 239, 144, 193, 239, 141, 164, 138, 183, 219, 235, 209, 193, 223, None, None, 129, 256, 177, 202, 178, 174, 259, 239, 164, 157, 212, None, 151, 185, 169, 196, 255, 176, 226, 162, 181, 187, 210, 199, 203, 233, 180, 193, 230, 153, 278, 189, 186, 187, 211, 224, 195, 158, 271, 215, 189, 223, 173, 201, 132, 163, 200, 182, 109, None, 201, 257, None, 222, None, 237, 155, 179, 266, 168, 305, 132, 212, 203, 145, None, 192, None, 155, 205, None, 144, 184, 192, None, 230, 171, 263, 166, 151, 135, 305, 196, 205, 245, None, 128, 199, 131, 205, 190, 160, 183, 204, 158, 199, 177, 125, 169, 134, 156, 261, 191, 187, 194, 195, None, 180, 233, None, 184, 167, 257, 227, 186, 199, 187, 261, 187, 176, 166, 206, 261, 181, 153, 223, 228, 289, 154, 205, 223, 269, 134, 161, 219, 198, 151, 223, 157, None, 111, 161, 170, 249, 194, 115, 209, 268, 249, 252, 223, 160, 164, 162, 157, 155, 162, 214, 214, 278, None, 176, None, 276, None, 153, 172, 224, 190, 160, 134, 191, 167, 215, None, 238, 195, 79, 186, 198, 144, 199, 190, 221, 286, 214, 291, 202, None, 188, 200, 147, 150, 188, 175, 164, None, 256, 208, 253, 183, 125, 190, 141, 199, 199, 211, 173, 157, 304, 169, 189, 177, 226, 268, 173, 229, 192, 195, 149, 175, 143, 193, 221, None, 130, 178, 175, 180, 225, 225, 208, 139, 206, 209, 271, 165, None, 208, 161, 197, 237, 238, 188, 123, 208, 254, 168, 137, 212, 168, 231, 175, 269, 249, 222, 223, 146, 159, 175, 201, 165, 192, 225, 161, 160, 317, 223, 170, 183, 169, 211, 205, 195, 181, 140, 209, 148, 280, 184, 254, 178, None, 149, 155, 296]
# times= [0.2928059999999999, 0.3718410000000001, 0.23194399999999993, 1.0388579999999998, 0.22879199999999988, 0.22002699999999997, 0.0020039999999998948, 0.4194810000000002, 0.3165870000000002, 0.37523799999999996, 0.21161799999999964, 0.8742600000000005, 0.6539440000000001, 0.0025639999999995666, 0.33535300000000046, 0.19672800000000024, 0.19727399999999928, 0.21439200000000014, 1.375591000000001, 0.8091880000000007, 0.38516199999999934, 0.09082000000000079, 0.2991969999999995, 9.217161, 0.37808399999999764, 0.8026850000000003, 0.18621900000000124, 2.9802820000000025, 0.3490499999999983, 0.43283699999999925, 1.1114730000000002, 0.08380700000000019, 0.3125120000000017, 0.25763999999999854, 1.3630969999999998, 2.764902000000003, 0.23528799999999706, 0.3022879999999972, 0.4018209999999982, 0.1808000000000014, 0.26575300000000013, 0.24835099999999954, 0.1446050000000021, 0.15552600000000183, 0.5358790000000013, 1.543016999999999, 0.3358879999999971, 0.3114440000000016, 0.6193889999999982, 0.3618939999999995, 0.7966959999999972, 0.390703000000002, 0.25773399999999924, 0.6757259999999974, 0.35827700000000107, 0.22720100000000087, 0.3796840000000046, 0.43254400000000004, 1.4931490000000025, 0.2548719999999989, 0.23152199999999823, 0.2505860000000055, 0.1444189999999992, 2.4351270000000014, 0.5021939999999958, 0.6229390000000024, 0.2047719999999984, 0.07404299999999608, 0.473507000000005, 0.9960200000000015, 0.37344699999999875, 0.2536440000000013, 0.32034300000000115, 0.29288700000000034, 0.4103250000000003, 0.25149100000000146, 0.1767630000000011, 4.083317000000001, 1.9761140000000026, 0.35448799999999636, 1.6717980000000026, 0.09816299999999956, 0.23800300000000618, 0.4695199999999957, 0.003132999999998276, 0.437342000000001, 0.26709399999999306, 0.3666509999999974, 0.5937780000000004, 0.23120199999999613, 0.24429899999999805, 0.7964970000000022, 3.20188499999999, 0.25137300000000096, 0.2140629999999959, 0.2290739999999971, 12.218799000000004, 0.989570999999998, 0.2993850000000009, 0.03139299999999423, 0.34039199999999425, 0.2431930000000051, 0.2137830000000065, 0.13565599999999733, 2.187706999999989, 0.6617399999999947, 1.391360000000006, 0.9645450000000011, 0.20681100000000185, 0.33145700000000033, 9.58070099999999, 0.30060399999999277, 0.512715, 0.021703999999999724, 0.3306620000000038, 0.3271529999999956, 0.614806999999999, 0.6048430000000025, 0.3124660000000006, 0.22258399999999767, 0.3527690000000092, 0.21717099999999334, 0.5492429999999899, 0.3776319999999913, 0.4154920000000004, 0.42213999999999885, 0.4515840000000111, 2.5750829999999922, 0.401961, 0.43810599999999056, 0.22623299999999347, 0.002532000000002199, 0.0018150000000076716, 0.22750400000001036, 0.23555100000000095, 0.2394079999999974, 4.218062000000003, 0.18922299999999836, 0.2122399999999942, 0.19388100000000463, 0.7154629999999997, 0.11671599999999671, 0.31569100000000105, 1.2622329999999948, 0.7974519999999927, 0.6517769999999956, 0.41394299999998907, 0.20967999999999165, 0.234185999999994, 0.251410000000007, 0.2803170000000108, 0.18839300000000492, 0.2053939999999983, 0.534396000000001, 0.4496340000000032, 0.37912299999999277, 0.3112829999999889, 0.6531530000000032, 0.29095900000000086, 0.2679389999999984, 0.5020150000000001, 0.8538089999999983, 0.2451690000000042, 0.24101299999999526, 0.40490699999999435, 0.08170499999999947, 0.03884700000000407, 0.5336039999999969, 5.059089999999998, 0.18274100000000715, 0.2251499999999993, 0.8296859999999953, 0.31562900000000127, 0.21862600000000043, 0.026416000000011763, 0.21508499999998776, 2.6393170000000055, 0.2953679999999963, 0.669667000000004, 0.19704600000000028, 0.2758139999999969, 0.6253249999999753, 1.6880829999999776, 0.5045389999999941, 0.41301500000000146, 1.3947259999999915, 0.5800529999999924, 0.27186100000000124, 0.16846100000000774, 0.36898900000002754, 1.8655620000000113, 1.9216669999999851, 0.22315499999999133, 0.22424100000000635, 0.2909649999999999, 0.3366570000000024, 0.37276799999997934, 0.20635099999998374, 0.6916360000000168, 0.22340499999998542, 1.4546670000000006, 0.9543570000000159, 0.2264419999999916, 0.23100800000000277, 1.7468560000000082, 0.2247030000000052, 0.38132199999998306, 0.208044000000001, 0.22038499999999317, 0.192748000000023, 0.2522610000000043, 2.0404780000000073, 0.0028300000000172076, 0.0027790000000038617, 0.24784099999999398, 0.3263469999999984, 0.3013249999999914, 0.4446060000000216, 0.41555499999998347, 0.3310480000000098, 2.386485999999991, 0.1714550000000088, 0.3242869999999982, 0.7600260000000105, 0.27437799999998447, 0.002240000000000464, 0.2850119999999947, 0.17968399999998041, 2.786428000000001, 0.20285200000000714, 0.20589999999998554, 0.1930720000000008, 0.8937139999999886, 0.8421769999999924, 0.2545039999999972, 0.40410099999999716, 0.8030490000000157, 0.5610989999999845, 0.21402100000000246, 0.2232050000000072, 5.460230999999993, 0.28734200000002375, 0.8302300000000002, 0.45365300000000275, 0.554003999999992, 0.17290600000001177, 0.22391000000001782, 0.7349100000000135, 0.4988390000000038, 57.867542000000014, 2.9327009999999802, 1.8384949999999947, 0.34806199999999876, 0.6671989999999823, 0.3419749999999908, 2.6556659999999965, 0.40598099999999704, 0.3016930000000002, 0.7211619999999925, 1.6560149999999965, 0.2520900000000097, 172.473804, 1.5781190000000151, 0.003030000000023847, 1.5507789999999773, 0.1987359999999967, 0.0020370000000298205, 0.2470360000000369, 0.0019209999999816318, 0.46808499999997366, 0.5146919999999682, 0.30661500000002206, 1.0912050000000022, 3.103079999999977, 2.1348110000000133, 0.20253500000001168, 0.22418199999998478, 0.8674279999999612, 0.8155019999999809, 0.04360199999996439, 0.16060300000003735, 0.0019020000000296022, 0.3572400000000471, 0.6463150000000155, 0.02232800000001589, 0.5644740000000183, 0.5229150000000118, 0.17267799999996214, 0.0018010000000003856, 0.6582479999999578, 0.3232359999999517, 0.42014299999999594, 0.17136499999998023, 1.1740439999999808, 0.3454179999999951, 0.24046300000003384, 0.22762199999999666, 0.3469519999999875, 2.7941959999999995, 0.0023160000000075343, 0.5626589999999965, 5.594690000000014, 0.3335419999999658, 0.5099799999999846, 0.20556300000004057, 0.5425690000000145, 0.4355209999999943, 0.8707939999999894, 0.24418299999996407, 0.380628999999999, 0.5869670000000156, 0.21521200000000817, 0.6054320000000075, 1.3143350000000282, 0.42159899999995787, 1.293295999999998, 0.38424699999995937, 0.6967149999999833, 0.300407000000007, 0.8768420000000106, 0.0034870000000069012, 0.16064799999998058, 0.2565319999999929, 0.07622500000002219, 0.2552289999999857, 0.16474999999996953, 1.8532679999999573, 0.4848479999999995, 1.3019820000000095, 117.35243199999996, 0.7084909999999809, 1.5091749999999138, 0.5793330000000196, 1.1138200000000325, 0.24734999999998308, 0.2551379999999881, 0.41855899999995927, 0.23927900000001046, 0.8112929999999778, 4.130955999999969, 1.4083500000000413, 0.5017380000000458, 0.6638189999999895, 0.13171699999998054, 2.0982510000000048, 0.23331199999995533, 0.3367120000000341, 0.18857300000001942, 0.22771699999998418, 0.36383999999998196, 0.2500719999999319, 0.33536500000002434, 0.5049139999999852, 0.0017800000000534055, 0.2386900000000196, 0.3896989999999505, 0.16270300000007865, 5.211135000000013, 0.22361899999998514, 0.17212899999992715, 0.1858239999999114, 1.361674999999991, 0.21281499999997777, 1.8029099999999971, 0.6559100000000626, 0.2045899999999392, 0.1893720000000485, 0.26809000000002925, 0.18401700000003984, 0.20749799999998686, 0.20333000000005086, 0.19717500000001564, 0.2518370000000232, 0.17771000000004733, 0.004097000000001572, 0.20062699999994038, 0.016684999999938555, 0.3801069999999527, 0.12811999999996715, 0.23352599999998347, 0.23732299999994666, 0.17410899999993035, 1.1341130000000703, 0.15998000000001866, 0.16210699999999179, 0.17891899999995076, 0.3379569999999603, 0.26499699999999393, 0.01535799999999199, 0.314029000000005, 0.19228199999997742, 2.368088000000057, 0.2382149999999683, 2.0576879999999846, 1.0398030000000063, 0.37048200000003817, 0.2978219999999965, 0.8121879999999919, 0.40711000000010245, 0.9477319999999736, 0.2424389999999903, 0.4584310000000187, 0.07188800000005813, 0.19823200000007546, 0.4595449999999346, 0.2064699999999675, 0.24890899999991234, 0.31812999999999647, 0.7636250000000473, 0.20206900000005135, 0.028891999999927975, 0.18949299999997038, 0.27459399999997913, 0.2852930000000242, 0.3780209999999897, 0.18278799999995954, 0.22277599999995346, 0.5774390000000267, 0.23634100000003855, 0.9100349999999935, 0.26783899999998084, 0.1856520000000046, 0.1749169999999367, 4.234426999999982, 0.375171000000023, 0.4404510000000528, 0.30652699999996, 0.8563239999999723, 0.26012500000001637, 0.15538699999990513, 0.3218699999999899, 0.3836069999999836, 0.7139559999999392, 0.16673899999989317, 3.1698390000000245, 0.177866999999992, 0.773089999999911, 0.47497200000009343, 0.0037820000000010623, 0.37524600000006103, 4.157304999999951, 0.6040100000000166, 0.36641099999997095, 0.3943050000000312, 0.23496799999998075, 7.371562999999924, 0.28099299999996674, 0.23972500000002128, 0.41920299999992494, 0.26016200000003664, 0.2959040000000641, 0.0020480000000588916, 2.8170840000000226, 0.5177230000000463, 1.8271009999999706, 0.6777199999999084, 0.5227209999999332, 11.963899999999967, 1.501141999999959, 0.3560229999999365, 0.4501699999999573, 0.3957470000000285, 0.3981499999999869, 0.17331400000000485, 0.26129900000000816, 2.6427649999999403, 0.2815819999999576, 0.29509600000005776, 0.1560309999999845, 1.2763760000000275, 0.3734460000000581, 0.16770899999994526, 0.21281999999996515, 0.9043809999999439, 0.1593480000000227, 0.18587000000002263, 0.2203460000000632, 4.136911999999938, 0.48137499999995725, 0.7381710000000794, 0.2286619999999857, 1.283792999999946, 0.19774400000005699, 0.4337709999999788, 2.3513019999999187, 0.35244299999999384, 0.38877899999999954, 0.25251300000002175, 0.30437899999992624, 0.9129359999999451, 0.33933600000000297, 0.18143799999995736, 0.23576900000000478, 1.2049879999999575, 0.3232110000000148, 0.6946389999999383, 0.002425000000016553, 0.4352480000000014, 0.36690199999998185, 0.30720100000007733]

# nodes = [584, 137, 33, 106, 5, 25, 6, 4, 255, 15, 54, 455, 27, 332, 735, 55, 1233, 278, 33, 8, 4, 4, 6, 3, 5, 13, 159, 39, 3, 33, 39, 335, 57, 6, 779, 146, 9, 328, 9, 11, 47, 367, 6, 1592, 12, 11, 634, 42, 128, 3, 21, 7, 22, 3, 33, 248, 58, 10, 5, 13, 23, 6, 2, 305, 24, 615, 3, 54, 27, 51, 1, 71, 4, 222, 6, 191, 203, 5, 11, 33, 105, 5, 90, 65, 3, 10, 15, 256, 70, 1118, 31, 4, 23, 20, 12, 63, 42, 3, 7, 13, 28, 3, 61, 77, 18, 83, 22, 45, 7894, 128, 247, 18, 313, 6, 9, 299, 11, 303, 18, 4, 31, 32, 3, 65, 3, 15, 18, 16, 3, 6, 15, 69, 27, 129, 539, 2, 1939, 17, 3, 3, 37, 12, 193, 3, 14, 52, 21, 53, 421, 1, 6, 263, 535, 20, 5, 347, 676, 3, 53, 3, 3, 16, 4, 14, 121, 152, 10, 9, 12, 29, 11, 24, 14, 7, 34, 1, 14, 49, 397, 16, 3, 8, 178, 13, 18, 256, 472, 11, 3, 262, 3, 11, 103, 123, 4, 15, 3, 49, 8, 252, 16, 30, 10, 315, 12, 211, 12, 589, 47, 5, 210, 6, 4, 6, 33, 6, 174, 180, 5, 4, 29, 25, 48, 11, 1, 69, 6, 47, 3, 25, 14, 3, 6, 6, 9, 3, 3, 3, 13883, 8, 31, 20, 40, 9, 6, 29, 965, 4, 155, 6, 16, 6, 11, 656, 166, 238, 5, 20, 9, 35, 118, 47, 5, 2, 139, 17, 17, 49, 830, 3, 5, 12, 34, 39, 247, 9, 408, 14, 9, 52, 14, 7, 3, 2370, 3, 133, 6, 237, 6, 73, 6, 1373, 163, 9, 1, 11, 1, 13, 35, 14, 11, 96, 3, 73, 71, 21, 230, 35, 15, 13, 61, 260, 5, 249, 3, 55, 41, 18, 14, 9, 1, 94, 11, 22, 28, 1, 29, 95, 3, 3, 40, 27, 5, 106, 13, 41, 83, 25, 19, 5, 11, 1, 6, 14, 39, 36, 3, 51, 11, 6, 18, 3360, 43, 419, 5, 24, 60, 20, 70, 325, 6, 10, 6, 159, 19, 25, 12, 32, 5, 3, 4, 11, 8, 119, 1, 8, 3, 11, 77, 3, 10, 11, 11, 244, 8, 18, 29, 26, 19, 69, 3, 100, 21, 3, 45, 177, 10, 6, 25, 277, 1185, 19, 25, 3, 3, 10, 6, 6, 130, 71, 9, 81, 282, 4, 4, 15, 6, 126, 327, 5, 21, 23, 3, 7, 6, 116, 147, 14, 3, 19, 22, 65, 12, 30, 23, 85, 38, 6195, 349, 37, 276, 27, 3, 168]
# p1scores = [184, 236, 90, 177, 291, 288, 291, 159, 235, 211, 289, 325, 182, 0, 264, 136, 172, 180, 304, 191, 259, 316, 187, 194, 321, 219, 186, 190, 0, 171, 251, 287, 136, 192, 124, 0, 317, 216, 135, 161, 269, 183, 0, 155, 188, 271, 242, 260, 284, 223, 183, 215, 255, 239, 259, 167, 227, 243, 186, 246, 186, 0, 199, 216, 215, 180, 254, 189, 275, 179, 192, 134, 230, 186, 346, 208, 247, 312, 216, 173, 248, 137, 237, 201, 204, 299, 282, 150, 246, 256, 189, 140, 193, 255, 240, 0, 205, 217, 250, 301, 133, 214, 0, 284, 182, 202, 218, 168, 0, 302, 168, 201, 230, 196, 73, 327, 215, 173, 228, 275, 226, 128, 232, 228, 229, 282, 298, 192, 296, 214, 140, 285, 226, 249, 247, 256, 304, 0, 230, 279, 264, 222, 158, 321, 282, 200, 209, 199, 253, 343, 199, 223, 177, 239, 0, 244, 161, 0, 215, 236, 268, 211, 279, 180, 223, 308, 262, 140, 160, 207, 172, 211, 189, 236, 265, 104, 167, 288, 216, 281, 270, 256, 178, 240, 134, 261, 260, 180, 222, 0, 215, 203, 327, 251, 174, 256, 109, 235, 181, 214, 258, 215, 0, 297, 220, 149, 292, 241, 193, 178, 343, 271, 208, 195, 2, 299, 221, 192, 312, 343, 220, 190, 293, 257, 0, 154, 201, 166, 211, 165, 121, 262, 312, 229, 221, 296, 283, 294, 251, 200, 227, 210, 173, 151, 299, 165, 287, 126, 208, 158, 268, 120, 132, 205, 200, 225, 163, 193, 153, 245, 231, 150, 241, 264, 182, 261, 277, 216, 189, 261, 0, 294, 217, 262, 229, 203, 228, 233, 258, 185, 231, 163, 325, 164, 342, 184, 243, 223, 228, 170, 203, 230, 145, 242, 90, 287, 199, 288, 267, 222, 272, 240, 181, 0, 300, 0, 366, 286, 180, 161, 188, 203, 270, 181, 261, 151, 185, 214, 181, 186, 212, 221, 192, 270, 208, 270, 173, 222, 223, 193, 192, 190, 304, 187, 252, 0, 269, 297, 202, 239, 172, 0, 243, 193, 34, 225, 241, 251, 304, 265, 280, 225, 0, 315, 185, 196, 265, 232, 0, 221, 121, 176, 211, 132, 0, 218, 102, 198, 44, 233, 282, 169, 263, 242, 263, 254, 246, 241, 218, 224, 272, 297, 169, 306, 0, 347, 167, 261, 0, 296, 365, 183, 246, 195, 206, 202, 139, 279, 153, 211, 280, 155, 190, 203, 154, 216, 254, 190, 197, 218, 210, 187, 226, 234, 178, 228, 139, 192, 147, 246, 195, 251, 206, 209, 9, 291, 222, 61, 280, 228, 186, 180, 217, 195, 201, 174, 207, 198, 282, 198, 246, 169, 225, 190, 188, 184, 261, 215, 206, 148, 217, 253, 290, 316, 233, 243, 147, 189, 171, 184, 233, 200, 170, 180, 123, 257, 232, 247, 358, 202, 145, 0, 224, 0, 211, 257, 271, 216, 262, 200, 192, 244, 241, 286, 364, 213, 245, 195, 213, 220, 173, 201, 331, 164, 223, 271, 195, 266, 206, 130]
# p2scores = [222, 201, 56, 162, 279, 200, 212, 309, 234, 203, 169, 255, 147, 0, 237, 187, 287, 282, 225, 363, 251, 177, 185, 243, 210, 255, 217, 151, 0, 193, 216, 218, 194, 212, 57, 0, 208, 174, 229, 246, 242, 212, 0, 210, 257, 218, 222, 218, 257, 123, 216, 195, 199, 188, 279, 191, 188, 260, 295, 265, 223, 0, 190, 203, 178, 162, 229, 283, 260, 252, 134, 226, 217, 293, 190, 176, 178, 267, 297, 265, 202, 85, 178, 254, 183, 273, 242, 200, 197, 238, 203, 71, 169, 207, 232, 0, 153, 173, 276, 210, 305, 216, 0, 225, 283, 168, 279, 188, 0, 235, 338, 261, 205, 395, 131, 210, 181, 205, 299, 258, 217, 208, 303, 193, 173, 255, 178, 182, 215, 212, 276, 180, 258, 225, 213, 194, 209, 0, 213, 194, 243, 272, 212, 256, 292, 249, 207, 213, 236, 198, 216, 310, 241, 147, 0, 282, 258, 0, 229, 238, 287, 203, 187, 245, 285, 267, 223, 274, 321, 241, 171, 270, 169, 315, 289, 212, 294, 230, 259, 195, 173, 276, 184, 205, 225, 203, 349, 255, 76, 0, 313, 174, 190, 235, 184, 268, 129, 244, 261, 246, 230, 172, 0, 161, 323, 181, 188, 192, 251, 194, 236, 193, 258, 135, 11, 206, 285, 289, 206, 214, 270, 236, 221, 206, 0, 254, 215, 253, 184, 144, 151, 159, 200, 232, 204, 222, 228, 267, 204, 188, 292, 221, 240, 195, 205, 150, 164, 216, 167, 196, 223, 224, 205, 202, 160, 257, 211, 259, 139, 253, 199, 241, 268, 188, 225, 201, 193, 181, 211, 211, 0, 203, 217, 197, 178, 281, 242, 169, 211, 168, 155, 246, 166, 313, 195, 254, 191, 218, 224, 189, 168, 201, 178, 207, 70, 179, 142, 241, 226, 212, 254, 243, 298, 0, 118, 0, 211, 223, 194, 214, 270, 212, 213, 328, 268, 227, 177, 292, 245, 197, 229, 226, 219, 162, 281, 167, 269, 225, 234, 243, 225, 198, 196, 272, 170, 0, 207, 263, 221, 205, 240, 0, 304, 174, 50, 281, 196, 206, 204, 183, 175, 192, 0, 221, 258, 172, 212, 237, 0, 215, 223, 206, 232, 315, 0, 285, 161, 208, 72, 247, 305, 260, 314, 174, 152, 182, 225, 201, 186, 168, 262, 158, 219, 197, 0, 165, 300, 221, 0, 180, 213, 257, 278, 163, 166, 230, 214, 182, 207, 246, 186, 215, 233, 281, 184, 329, 325, 280, 240, 207, 246, 260, 198, 217, 185, 287, 248, 213, 173, 198, 241, 160, 179, 243, 14, 225, 257, 88, 203, 299, 262, 199, 263, 340, 195, 166, 268, 274, 228, 177, 243, 160, 245, 213, 232, 113, 210, 241, 251, 208, 219, 242, 259, 211, 163, 206, 200, 259, 196, 217, 312, 150, 336, 224, 164, 170, 156, 265, 182, 240, 278, 0, 198, 0, 175, 208, 173, 172, 267, 241, 281, 261, 234, 196, 174, 139, 188, 252, 142, 263, 304, 292, 209, 142, 172, 296, 249, 137, 188, 282]
# p1endgamescores= [184, 174, None, 177, 266, 267, 273, 146, 190, 171, 280, 303, None, None, 252, 136, 161, 172, 234, 178, 179, 256, 187, 182, 283, 219, 186, 190, None, 171, 204, 201, None, 192, None, None, 317, 216, 135, 149, 217, 183, None, 155, 126, 254, 212, 178, 267, 223, 183, 215, 248, 173, 180, 167, 164, 176, 186, 207, 167, None, 199, 205, 215, 180, 232, 174, 213, 173, 192, 134, 210, 147, 281, 149, 223, 272, 216, 146, 197, None, 237, 153, 204, 253, 282, 150, 231, 244, 189, None, None, 233, 173, None, 205, 169, 236, 258, 133, 214, None, 258, 172, 202, 185, 168, None, 250, 157, 174, 230, 181, None, 253, 204, 173, 224, 232, 219, 128, 221, 183, 229, 235, 298, 180, 238, 214, None, 285, 150, 249, 239, 179, 258, None, 163, 189, 214, 172, 127, 277, 240, 200, 206, 181, 193, 293, 199, 178, 172, 239, None, 182, 135, None, 215, 219, 203, 173, 279, 157, 205, 234, 198, 135, 145, 207, 172, 211, 189, 223, 182, None, 167, 244, 210, 219, 206, 189, 170, 197, 134, 211, 212, 180, None, None, 160, None, 274, 241, 167, 186, None, 224, 181, 171, 215, 215, None, 242, 213, 149, 240, 235, 182, 167, 301, 265, 185, 195, None, 242, 206, 185, 235, 299, 136, 153, 293, 257, None, 146, 153, 148, 211, 165, None, 221, 237, 207, 166, 239, 260, 252, 171, 200, 166, 152, 152, 129, 299, None, 241, 126, 208, 158, 213, 120, 132, 205, 190, 185, 150, 134, 153, 166, 192, 143, 198, 264, 174, 261, 227, 216, 189, 208, None, 249, 217, 181, 229, 182, 219, 233, 203, 185, 231, 149, 258, 147, 301, 184, 192, 206, 191, 163, 186, 230, 145, 242, None, 199, 199, 223, 214, 183, 214, 228, 175, None, 300, None, 335, 241, 180, 161, 165, 203, 217, 163, 172, 151, 185, 178, 157, 186, 212, 192, 192, 254, 186, 218, 165, 156, 203, 193, 178, 190, 304, 129, 252, None, 269, 285, 202, 233, 158, None, 202, 193, None, 214, 180, 171, 266, 216, 280, 225, None, 253, 179, 196, 224, 161, None, 206, 121, 176, 157, 132, None, 207, None, 127, None, 200, 194, 169, 260, 200, 205, 212, 179, 185, 218, 224, 217, 212, 169, 253, None, 281, 157, 249, None, 296, 311, 183, 153, 182, 165, 202, 139, 240, 153, 176, 194, 155, 190, 197, 154, 201, 214, 170, 169, 218, 196, 167, 210, 185, 178, 187, 129, 192, 147, 239, 189, 181, 206, 209, None, 235, 172, None, 216, 174, 162, 164, 217, 147, 152, 174, 168, 186, 183, 198, 189, None, 169, 143, 188, 184, 206, 215, 206, 145, 151, 253, 262, 249, 181, 243, 147, 135, 171, 178, 203, 200, 170, 180, 123, 196, 184, 186, 307, 147, 137, None, 164, None, 199, 233, 217, 216, 244, 190, 144, 187, 167, 239, 310, 202, 209, 195, 213, 178, 160, 143, 302, None, 223, 250, 184, 248, 164, 130]
# p2endgamescores = [222, 184, None, 162, 236, 135, 150, 240, 195, 175, 155, 151, None, None, 149, 187, 220, 230, 207, 310, 213, 143, 185, 176, 156, 181, 210, 138, None, 193, 186, 205, None, 189, None, None, 190, 174, 213, 205, 202, 212, None, 210, 239, 161, 151, 193, 184, 123, 165, 195, 154, 172, 279, 158, 174, 215, 275, 238, 209, None, 190, 177, 157, 162, 181, 227, 233, 196, 134, 226, 147, 185, 174, 162, 168, 251, 297, 216, 158, None, 151, 176, 183, 257, 209, 200, 192, 188, 203, None, None, 164, 182, None, 153, 149, 230, 171, 283, 216, None, 206, 226, 168, 236, 149, None, 210, 275, 183, 191, 338, None, 186, 147, 205, 264, 231, 190, 208, 209, 168, 173, 193, 178, 161, 188, 196, None, 180, 165, 209, 153, 186, 187, None, 186, 165, 222, 241, 151, 196, 233, 228, 182, 150, 204, 185, 199, 261, 204, 135, None, 243, 211, None, 210, 174, 270, 183, 180, 199, 237, 241, 223, 243, 264, 221, 166, 246, 169, 243, 270, None, 294, 187, 174, 180, 153, 239, 156, 145, 225, 197, 328, 255, None, None, 286, None, 168, 150, 160, 225, None, 201, 261, 229, 214, 155, None, 140, 277, 173, 165, 120, 242, 175, 221, 138, 170, 135, None, 180, 202, 236, 181, 193, 263, 206, 172, 206, None, 184, 173, 209, 178, 144, None, 133, 179, 203, 183, 203, 170, 211, 184, 159, 273, 170, 171, 174, 184, None, 148, 216, 143, 196, 207, 224, 190, 178, 152, 213, 141, 204, 137, 221, 163, 226, 225, 188, 207, 189, 193, 174, 199, 211, None, 189, 205, 171, 161, 238, 168, 154, 158, 156, 155, 204, 143, 236, 179, 226, 173, 168, 204, 142, 151, 174, 178, 196, None, 168, 142, 226, 193, 196, 230, 237, 227, None, 118, None, 154, 193, 194, 190, 225, 212, 207, 231, 252, 227, 177, 208, 181, 197, 208, 137, 219, 145, 234, 150, 211, 212, 180, 243, 172, 180, 154, 249, 159, None, 175, 190, 221, 152, 193, None, 235, 138, None, 192, 178, 145, 177, 144, 175, 192, None, 165, 245, 172, 155, 212, None, 193, 223, 206, 212, 299, None, 198, None, 197, None, 173, 292, 255, 239, 174, 152, 156, 169, 196, 176, 168, 255, 155, 219, 194, None, 144, 259, 146, None, 163, 170, 257, 260, 143, 152, 230, 214, 127, 183, 194, 150, 215, 217, 194, 184, 229, 272, 237, 212, 207, 196, 244, 171, 191, 185, 263, 201, 213, 173, 141, 176, 116, 141, 182, None, 209, 230, None, 184, 299, 216, 152, 263, 277, 177, 166, 226, 262, 197, 177, 207, None, 210, 168, 194, 113, 189, 226, 226, 176, 169, 185, 204, 171, 153, 164, 200, 245, 159, 202, 227, 150, 287, 224, 164, 151, 141, 246, 154, 208, 234, None, 183, None, 150, 200, 151, 172, 188, 201, 193, 194, 201, 180, 151, 127, 149, 252, 142, 210, 223, 273, 160, None, 172, 283, 198, 135, 165, 282]
# times =[0.2743589999999999, 5.318681, 0.07979300000000045, 1.5507600000000004, 0.5014630000000011, 1.1186679999999996, 0.27624600000000044, 0.34914800000000135, 0.20126000000000133, 0.19063099999999977, 2.79533, 0.3795719999999996, 0.11689600000000056, 0.0023640000000000327, 0.6355559999999993, 4.5787379999999995, 0.3549609999999994, 3.3291059999999995, 6.6117750000000015, 0.6707800000000006, 11.804835, 3.248521999999994, 0.5565570000000051, 0.26883400000000535, 0.20992000000000388, 0.2048680000000047, 0.21727899999999778, 0.20856299999999806, 0.0026269999999968263, 0.20617599999999925, 0.2120870000000039, 0.33939900000000023, 0.17759799999999615, 1.487210999999995, 0.0785849999999968, 0.002582000000003859, 0.6452020000000047, 0.1866330000000005, 0.47352700000000425, 0.5345780000000033, 3.126418000000001, 0.7590050000000019, 0.0023320000000026653, 0.26359099999999813, 7.4764960000000045, 1.4632659999999973, 0.2262970000000024, 4.835604999999987, 0.27355199999999513, 0.30192799999998954, 0.5614940000000104, 4.153507000000005, 0.25050199999999734, 13.877615999999989, 0.30518599999999196, 0.27388400000000956, 6.8312329999999974, 0.6253849999999943, 1.385463999999999, 0.2206780000000066, 0.39123200000000224, 0.0026289999999988822, 0.2288730000000072, 0.37665399999998783, 0.25503500000000656, 0.5221849999999932, 2.5710949999999997, 0.7062860000000057, 0.30631700000000706, 0.2537200000000013, 0.27305599999999686, 0.36915000000000475, 0.19596199999999442, 0.1919990000000098, 3.1551429999999954, 0.716202999999993, 5.582977999999997, 0.17267300000000319, 0.7769669999999991, 0.37520699999998897, 0.5890309999999914, 0.12458099999999206, 0.15832800000001157, 0.8142780000000016, 0.18010799999998994, 2.2336070000000063, 0.2858439999999973, 2.0616249999999923, 2.2583609999999936, 0.21343000000000245, 0.29611099999999624, 0.07873299999999972, 0.10656600000000083, 0.5221829999999983, 1.1246090000000066, 0.0034959999999983893, 0.19683299999999804, 0.9667299999999841, 0.6986190000000079, 0.23166599999998994, 0.28642899999999827, 0.2941720000000032, 0.0021700000000066666, 3.1485469999999793, 0.7726910000000089, 10.74784600000001, 0.47506500000000074, 0.2107340000000022, 0.004694000000000642, 0.4216910000000098, 0.384254999999996, 0.9270379999999818, 0.8823179999999979, 0.6307809999999847, 0.07757700000001932, 0.25545099999999366, 0.23527200000000903, 0.27938999999997804, 0.5966109999999958, 0.23215700000000083, 0.7871729999999957, 0.9395379999999989, 0.31039200000000733, 0.8570550000000026, 0.3962099999999964, 0.6121700000000203, 74.240038, 1.2719389999999748, 2.398614999999978, 0.4113229999999817, 0.130693999999977, 0.16336599999999635, 3.3271819999999934, 0.21936999999999784, 0.2399300000000153, 3.271817999999996, 0.3502979999999809, 0.0021349999999813463, 2.8564560000000085, 0.4366709999999898, 0.21518299999999613, 0.47966900000000123, 0.511070999999987, 0.21172500000000127, 0.7042129999999815, 0.22021899999998595, 0.3094350000000077, 0.35768600000000106, 0.3170769999999834, 0.20976599999997347, 0.2586960000000147, 0.3194780000000037, 0.7840920000000153, 0.4228789999999947, 0.0018149999999934607, 1.4951489999999978, 4.571793999999983, 0.0028519999999900847, 0.19428400000001034, 16.458572000000004, 0.36880500000000893, 0.2092370000000301, 0.17441700000000537, 0.5177160000000072, 0.3180580000000077, 1.9738370000000032, 0.14592999999996437, 0.3311919999999873, 0.5329450000000406, 0.3922040000000493, 0.7291119999999864, 3.838661000000002, 0.18346299999996063, 0.30108599999999797, 2.617703000000006, 0.11662799999999152, 5.350319999999954, 0.4023660000000291, 0.20773600000001124, 3.0374439999999936, 6.259878000000015, 0.20349399999997786, 0.7443569999999795, 0.20455600000002505, 0.20005799999995588, 0.33098400000000083, 0.28242800000003854, 0.1911269999999945, 0.16996500000004744, 0.00200700000004872, 0.35800599999998894, 0.1151039999999739, 1.2366170000000238, 1.463862000000006, 0.24819899999999961, 0.28080200000005107, 0.1446510000000103, 0.2656059999999911, 0.49560800000000427, 0.3013970000000086, 0.41183500000005324, 0.32828000000000657, 0.0023799999999596366, 0.18572399999999334, 0.4779570000000035, 0.16291100000000824, 0.3611760000000004, 0.48043000000001257, 3.6279910000000086, 0.34387399999997115, 0.21615400000001728, 0.2522140000000377, 1.64122100000003, 0.3578430000000026, 0.012416999999970812, 0.3818390000000136, 2.794548999999961, 4.532386000000031, 0.25787900000000263, 0.28167300000001205, 3.2173520000000053, 0.16719399999999496, 0.26954000000000633, 1.2589529999999627, 0.0030679999999847496, 1.2137920000000122, 0.18453900000002932, 0.26699800000000096, 0.1593520000000126, 0.6906700000000114, 0.11297799999999825, 0.23153299999995625, 2.6802209999999604, 0.332396000000017, 0.4681940000000395, 0.3165329999999926, 3.07279299999999, 0.24047500000000355, 2.0580280000000357, 0.3123619999999505, 5.243685999999968, 0.7146920000000136, 0.21976000000000795, 3.076519000000019, 0.25107799999994995, 0.12842000000000553, 0.24390399999998635, 0.22460699999999179, 0.4590610000000197, 0.2646139999999946, 1.7169870000000174, 1.5391970000000015, 0.1926499999999578, 0.18852800000001935, 0.42942499999998063, 0.46656999999999016, 0.5425439999999639, 0.2884779999999978, 0.16632099999998218, 0.7489209999999957, 0.25986500000004753, 0.6344950000000154, 0.17514399999998886, 0.3713470000000143, 0.43203099999999495, 0.22101200000003018, 0.24162699999999404, 0.28059899999999516, 0.26426200000003064, 0.18813199999999597, 0.002047000000004573, 0.22939300000001595, 0.22065400000002455, 112.07626800000003, 0.25548199999997223, 0.5102570000000242, 0.3294389999999794, 0.48925700000000916, 0.2741060000000175, 0.22553599999997687, 0.4796089999999822, 9.269835999999998, 0.21005400000001373, 1.567997000000048, 0.24786600000004455, 0.37319400000001224, 0.2031189999999583, 0.29812600000002476, 6.961295000000007, 1.6417099999999891, 2.217209999999966, 0.22646800000001122, 0.34776600000003555, 0.27075999999999567, 0.04555599999997639, 0.559359000000029, 1.4431720000000041, 0.6163859999999772, 0.1976860000000329, 0.1663120000000049, 1.4424380000000383, 0.4179849999999874, 0.3047669999999698, 0.0022060000000010405, 0.6744509999999764, 0.0025359999999636784, 7.280429999999967, 0.19869099999993978, 0.2118520000000217, 0.3106629999999768, 0.4921759999999722, 0.5397970000000214, 2.563126000000011, 0.23918699999990167, 4.168202999999949, 0.25603000000000975, 0.25373100000001614, 0.5347769999999628, 0.4362109999999575, 0.248826999999892, 0.2005619999999908, 20.58092499999998, 0.16772800000001098, 1.2809089999999514, 0.23856100000000424, 2.6518559999999525, 0.2241759999999431, 0.807495999999901, 0.25628199999994195, 15.08129299999996, 1.718539000000078, 0.2800760000000082, 0.15305699999998978, 0.24920399999996334, 0.19556299999999283, 0.002608999999893058, 0.3430789999999888, 0.5016770000000861, 0.27557999999999083, 0.24524400000007063, 1.1488299999999754, 0.001960999999937485, 0.18066000000010263, 1.0189860000000408, 0.046310999999946034, 0.8147239999999556, 0.4319380000000592, 2.463842999999997, 0.5028310000000147, 0.28361099999995076, 0.3403039999999464, 0.7582350000000133, 0.002641999999923428, 2.6517499999999927, 0.20867100000009486, 2.8328109999999924, 0.21719999999993433, 0.8316340000000082, 0.004023999999958505, 0.862070000000017, 0.3754100000001017, 0.3171740000000227, 0.28037300000005416, 0.15669800000000578, 0.0025540000000319196, 1.0522500000000719, 0.13599099999998998, 0.3395850000000564, 0.03323299999999563, 0.3468769999999495, 0.5886510000000271, 0.15522699999996803, 0.4459910000000491, 0.9823129999999765, 0.16250299999990148, 0.2060279999999466, 1.0837969999998904, 0.4193809999999303, 0.26057700000001205, 1.1498530000000073, 0.2932789999999841, 0.5540510000000722, 1.00759400000004, 0.5449180000000524, 0.001852999999982785, 0.3948650000000953, 0.19931499999995594, 0.24875999999994747, 0.0020850000000791624, 0.19165899999995872, 0.2945310000000063, 0.2972780000000057, 0.5189850000000433, 0.6321240000000898, 0.20359499999995023, 0.7282339999999294, 0.2336639999999761, 0.22433399999999892, 0.3277249999999867, 25.414679999999976, 1.1345509999999877, 3.627480999999989, 0.17990399999996498, 0.3493879999999763, 0.8598930000000564, 0.4006809999999632, 0.7420379999999795, 3.5034540000000334, 0.34112900000002355, 0.2840160000000651, 0.3063030000000708, 1.7751930000000584, 0.33825100000001385, 0.4618129999998928, 0.3032819999999674, 0.4490490000000591, 0.1971369999999979, 0.18943799999999555, 0.1644260000000486, 0.2644780000000537, 0.25672099999997045, 1.1970969999999852, 0.17178499999999985, 0.18757900000002792, 0.011530999999990854, 0.18386999999995624, 0.2971830000000182, 0.061786000000097374, 0.8652479999999514, 0.18300199999998767, 0.23523599999998623, 0.2565879999999652, 0.28473799999994753, 2.536852000000067, 0.25889299999994364, 0.37905799999998635, 0.38067899999998644, 0.481068999999934, 0.32650599999999486, 0.8703060000000278, 0.2168890000000374, 0.1374399999999696, 1.190992999999935, 0.3806890000000749, 0.23123199999997723, 0.6082290000000512, 1.8757859999999482, 0.3063209999999117, 0.28667799999993804, 0.3929840000000695, 2.716554999999971, 0.18356700000003912, 9.648217000000045, 0.3845440000000053, 0.46393799999998464, 0.17559099999994032, 0.1838070000000016, 0.29988399999990634, 0.20619599999997718, 0.24680100000000493, 1.222122000000013, 0.9158440000001065, 0.15998100000001614, 0.24987899999996444, 0.8780479999999216, 2.5886379999999463, 0.1621099999999842, 0.20714399999997113, 0.3633859999999913, 0.2887719999999945, 1.257669000000078, 0.002312999999958265, 3.205654999999979, 0.002686000000039712, 0.1995879999999488, 0.4251120000000128, 0.4229799999999386, 0.18300399999998263, 0.1944580000000542, 0.21475999999995565, 1.0064419999999927, 1.523727000000008, 0.35039299999994, 0.21240199999999732, 0.36000000000001364, 0.3293019999999842, 0.5997019999999793, 0.34401500000001306, 0.45804899999995996, 0.3713300000000572, 0.9313150000000405, 0.49069199999996727, 46.48023999999998, 0.09830699999997705, 3.6500310000000127, 0.5836429999999382, 2.2442889999999807, 0.5207999999998947, 0.22033799999996972, 2.03454099999999]




""""""""""""""""" Data for evaluation functions """""""""""""""

# Rack Eval Depth 4
# p1scores =[146, 184, 274, 288, 0, 222, 223, 304, 157, 347, 0, 300, 173, 201, 182, 222, 170, 160, 189, 230, 223, 181, 117, 247, 221, 229, 284, 169, 268, 241, 0, 235, 215, 178, 197, 222, 145, 197, 215, 258, 0, 216, 0, 194, 213, 241, 196, 217, 272, 177, 288, 269, 232, 0, 277, 212, 336, 201, 249, 144, 321, 121, 236, 265, 191, 192, 222, 204, 242, 197, 317, 0, 220, 144, 182, 225, 178, 248, 272, 221, 116, 7, 219, 148, 220, 226, 166, 280, 236, 215, 263, 236, 250, 154, 227, 260, 296, 226, 219, 216]
# p2scores =[212, 230, 180, 226, 0, 231, 220, 316, 233, 206, 0, 146, 239, 207, 180, 173, 191, 159, 233, 175, 228, 211, 165, 178, 196, 243, 221, 218, 258, 235, 0, 291, 151, 228, 185, 179, 219, 286, 203, 231, 0, 194, 0, 213, 233, 202, 232, 309, 155, 161, 245, 213, 205, 0, 272, 305, 169, 217, 201, 136, 178, 190, 221, 195, 244, 236, 178, 238, 238, 284, 189, 0, 239, 193, 180, 189, 304, 250, 221, 212, 147, 16, 198, 255, 299, 237, 313, 206, 258, 243, 201, 242, 263, 218, 206, 255, 172, 290, 300, 176]
# p1endgamescores= [146, 141, 232, 229, None, 217, 178, 291, 157, 269, None, 257, 161, 201, 182, 171, 170, 160, 180, 187, 161, 158, None, 244, 204, 181, 230, 169, 218, 183, None, 198, None, 178, 150, 181, 135, 197, 209, 200, None, 216, None, 194, 193, 192, 190, 164, 272, 177, 214, 257, 184, None, 249, 195, 267, 183, 186, 141, 259, None, 172, 207, 191, 176, 172, 139, 171, 178, 267, None, 178, 144, 125, 157, 146, 200, 219, 221, None, None, 173, 148, 207, 215, 137, 205, 193, 170, 218, 177, 218, 154, 168, 185, 252, 182, 211, 190]
# p2endgamescores =[168, 200, 168, 184, None, 171, 216, 242, 214, 183, None, 133, 217, 195, 180, 169, 191, 137, 171, 151, 220, 151, None, 178, 185, 220, 193, 197, 215, 221, None, 252, None, 191, 154, 146, 134, 230, 172, 214, None, 177, None, 213, 182, 179, 205, 289, 123, 161, 224, 152, 182, None, 204, 240, 155, 202, 176, 136, 147, None, 213, 165, 244, 183, 157, 228, 223, 211, 153, None, 215, 165, 157, 173, 222, 175, 200, 190, None, None, 171, 255, 259, 170, 239, 187, 208, 196, 173, 242, 192, 218, 195, 249, 156, 211, 284, 169]
# times =[0.3045979999999999, 0.31411900000000004, 1.028207, 0.7278799999999999, 0.002391000000000254, 0.8223549999999995, 0.5361419999999999, 0.30885799999999985, 0.4713649999999996, 8.040690000000001, 0.0017460000000006914, 4.453520000000001, 3.8119639999999997, 0.15258000000000038, 2.6539730000000006, 4.876311000000001, 1.587686999999999, 0.22876500000000277, 0.37653800000000004, 1.014126000000001, 15.079270000000001, 2.490597000000001, 0.09578199999999981, 0.58981, 5.331534000000005, 14.039788000000009, 0.8188509999999951, 0.9263629999999949, 1.0293779999999941, 30.377004, 0.0019709999999975025, 0.21263400000000843, 0.13112600000000896, 0.6971180000000032, 1.796717000000001, 0.29961399999999117, 0.2729439999999954, 0.17386500000000638, 1.2968099999999936, 21.760698000000005, 0.002534999999994625, 0.3060159999999996, 0.002262999999999238, 4.769763999999981, 13.430371000000008, 7.867820000000023, 0.4319200000000194, 10.286172999999991, 0.5875459999999748, 0.8725009999999997, 17.543420999999995, 0.3250749999999982, 0.3641499999999951, 0.003634000000005244, 14.963843999999995, 2.745355999999987, 4.397753000000023, 1.3617279999999994, 5.664902999999981, 0.8148960000000045, 6.911541, 0.1422369999999944, 34.491161999999974, 1.352770000000021, 0.2054809999999634, 1.0123800000000074, 0.3612370000000169, 1.0711959999999863, 12.283149999999978, 0.9499319999999898, 0.8136440000000107, 0.0021970000000237633, 0.24760899999995445, 0.20240000000001146, 10.06759199999999, 17.460323999999957, 7.640714000000003, 0.28121299999997973, 5.4325359999999705, 1.2148369999999886, 0.07670100000001412, 0.013754000000005817, 0.3093770000000404, 0.8976590000000328, 0.6152389999999741, 0.2235469999999964, 8.328934000000004, 5.629072000000008, 7.482176999999979, 1.0071859999999901, 0.3191290000000322, 1.495381000000009, 1.3220549999999776, 3.6810449999999832, 3.060097999999982, 27.271862, 0.2244190000000117, 0.594159999999988, 1.149567999999988, 15.241562999999985]

# 2x Eval Function Depth 4
# p1scores= [202, 223, 274, 184, 263, 204, 215, 187, 133, 205, 278, 296, 241, 192, 181, 31, 161, 250, 159, 325, 347, 295, 329, 174, 254, 263, 266, 189, 341, 288, 163, 204, 201, 228, 207, 301, 186, 150, 166, 0, 0, 210, 212, 219, 161, 139, 269, 253, 73, 271, 237, 243, 226, 293, 157, 268, 215, 150, 223, 184, 239, 248, 121, 253, 0, 225, 297, 289, 182, 219, 190, 116, 212, 0, 265, 175, 307, 257, 215, 0, 334, 224, 182, 202, 149, 246, 150, 190, 268, 177, 201, 259, 203, 246, 166, 253, 241, 233, 171, 187]
# p2scores= [163, 261, 187, 200, 144, 141, 188, 224, 266, 270, 219, 179, 310, 287, 241, 34, 164, 185, 249, 188, 210, 270, 223, 250, 203, 260, 291, 273, 200, 206, 323, 194, 227, 238, 206, 181, 337, 237, 259, 0, 0, 221, 168, 272, 325, 133, 230, 219, 72, 211, 175, 281, 236, 181, 319, 228, 219, 240, 222, 204, 267, 222, 242, 246, 0, 241, 146, 225, 210, 178, 205, 207, 260, 0, 236, 173, 175, 253, 250, 0, 203, 220, 359, 327, 280, 184, 232, 236, 163, 154, 210, 184, 173, 221, 242, 195, 218, 275, 222, 175]
# p1endgamescores =[202, 209, 226, 184, 263, 204, 163, 187, 128, 192, 229, 220, 208, 184, 181, None, 135, 188, 128, 311, 302, 213, 257, 150, 187, 182, 174, 175, 286, 262, 138, 204, 201, 228, 201, 240, 156, 138, 154, None, None, 189, 191, 197, 143, None, 228, 246, None, 200, 228, 228, 176, 252, 157, 220, 204, 150, 166, 137, 190, 178, 121, 243, None, 164, 229, 197, 182, 203, 190, 116, 196, None, 199, 175, 254, 225, 139, None, 253, 224, 125, 172, 149, 192, 150, 190, 238, 172, 185, 193, 201, 202, 144, 192, 179, 220, 171, 187]
# p2endgamescores = [163, 201, 187, 177, 144, 141, 158, 196, 254, 190, 184, 152, 268, 239, 224, None, 154, 161, 196, 171, 189, 253, 173, 227, 185, 254, 267, 205, 173, 162, 258, 194, 227, 206, 177, 178, 297, 229, 195, None, None, 141, 126, 228, 288, None, 199, 185, None, 198, 155, 193, 209, 154, 304, 194, 159, 183, 222, 188, 238, 204, 242, 183, None, 233, 134, 198, 210, 138, 205, 207, 200, None, 199, 173, 158, 206, 221, None, 188, 220, 339, 288, 260, 146, 232, 236, 151, 106, 161, 153, 161, 209, 188, 160, 198, 206, 222, 152]
# times = [1.6513579999999999, 0.6924669999999997, 1.2346110000000001, 0.44587600000000016, 0.2932700000000006, 0.21223100000000006, 2.0563260000000003, 0.3351480000000002, 0.4891210000000008, 1.4405210000000004, 0.2486669999999993, 1.874815, 1.0169619999999995, 0.8398979999999998, 0.526726, 0.03356199999999987, 2.181731000000001, 1.1180710000000005, 1.1629669999999983, 10.324406, 41.16270399999999, 14.29225000000001, 2.9749949999999927, 3.5038200000000046, 13.185907999999998, 1.687308999999999, 17.269787000000008, 0.6310780000000022, 0.33241999999999905, 1.520972999999998, 1.9234519999999975, 2.1628419999999977, 0.2926080000000013, 0.3611009999999908, 0.35834699999998065, 9.21249400000002, 1.1236370000000022, 1.434044, 0.9033249999999953, 0.002450999999979331, 0.0024689999999907286, 2.669511, 1.0052470000000255, 0.4205469999999991, 0.5798229999999762, 0.1314869999999928, 1.144127999999995, 1.239783000000017, 0.06410700000000702, 0.3739930000000129, 12.927264000000008, 1.9154200000000117, 1.834779999999995, 0.47640100000000984, 0.36341600000000085, 0.4595600000000104, 0.3548640000000205, 0.26604800000001205, 13.487632999999988, 10.89629099999999, 1.0695749999999862, 34.55716100000001, 0.8232450000000142, 0.3374949999999899, 0.002517000000011649, 13.29813900000002, 4.015976999999992, 1.9313290000000052, 10.20299399999999, 0.31741999999997006, 0.3866659999999911, 0.40405199999997876, 0.3988350000000196, 0.0027299999999854663, 0.47522499999996626, 0.7034429999999929, 1.440870000000018, 2.0513310000000047, 5.061823000000004, 0.001909000000011929, 17.014077000000043, 0.5752730000000383, 5.345965000000035, 0.9250749999999925, 0.20375300000000607, 0.3181369999999788, 0.5623110000000224, 1.655349000000001, 5.229088000000047, 1.2716040000000248, 1.0718870000000038, 1.7607150000000047, 0.3929019999999923, 0.7842210000000023, 27.87901800000003, 4.229108999999994, 9.242232999999999, 0.567216999999971, 1.0133329999999887, 0.3096429999999941]


#Baseline Eval Depth 4
# nodes = [52, 288, 52, 249, 966, 77, 34, 18, 111, 26, 41, 197, 34, 84, 4303, 108, 580, 27, 1, 36, 6, 515, 915, 266, 37, 2, 108, 594, 126, 803, 4793, 365, 1045, 100, 17, 403, 8, 35, 45, 5, 225, 113, 10, 111, 92, 49, 17, 3275, 175, 16, 1156, 214, 169, 25, 3, 507, 1970, 163, 234, 1, 205, 149, 55, 87, 986, 2, 176, 202, 3, 8, 77, 10, 1236, 1, 311, 262, 2700, 3879, 22, 77, 462, 472, 63, 59, 212, 3, 1683, 591, 1113, 3615, 2494] + [43, 8, 38, 30, 4, 64, 487, 506, 1472, 87, 13, 88, 5273, 50, 725, 63, 269, 33, 77, 64, 13, 93, 1293, 2, 106, 21, 154, 33, 33, 6, 3622, 11, 3483, 67, 657, 159, 276, 167, 231, 958, 50, 71, 200, 236, 1754, 1371, 8, 6, 82, 23, 89, 108, 248, 589, 3, 46, 2188, 41, 7, 418, 82, 326, 307, 23, 19, 61, 1318, 83, 1364, 505, 634, 43, 295, 218, 41, 79, 371, 213, 607, 37, 39, 636, 1507, 25, 109, 571, 1236, 1144, 719, 23, 581, 2407, 10, 697, 58, 171, 135, 905, 56]
# p1scores =[292, 202, 285, 193, 238, 152, 255, 227, 209, 192, 143, 196, 231, 182, 220, 215, 305, 237, 213, 308, 279, 206, 280, 202, 312, 196, 269, 179, 142, 202, 277, 221, 326, 229, 272, 248, 267, 278, 232, 202, 154, 347, 267, 260, 293, 347, 156, 184, 258, 190, 214, 179, 195, 261, 238, 212, 201, 246, 215, 215, 180, 227, 209, 207, 188, 200, 218, 133, 245, 292, 272, 149, 147, 229, 175, 187, 165, 238, 159, 214, 267, 208, 269, 241, 213, 273, 215, 233, 224, 156, 289, 369, 157, 262, 261, 307, 193, 0, 291, 199] + [182, 169, 192, 207, 199, 200, 224, 217, 223, 0, 157, 237, 216, 214, 168, 266, 149, 209, 192, 193, 148, 223, 279, 248, 228, 218, 233, 210, 199, 241, 292, 0, 202, 214, 249, 213, 218, 0, 268, 169, 186, 179, 150, 261, 255, 220, 0, 274, 265, 294, 230, 151, 188, 288, 268, 238, 239, 265, 207, 160, 293, 225, 254, 199, 11, 257, 261, 193, 264, 0, 251, 166, 213, 146, 189, 185, 278, 223, 199, 197, 276, 188, 234, 216, 257, 246, 237, 225, 169, 177, 253, 280, 160, 164, 239, 0, 388, 239, 226, 186]
# p2scores =[159, 255, 300, 211, 183, 199, 214, 194, 294, 224, 197, 259, 196, 321, 229, 250, 200, 147, 281, 179, 182, 238, 152, 254, 151, 269, 214, 206, 176, 172, 205, 218, 287, 227, 246, 217, 197, 233, 222, 197, 263, 259, 267, 241, 173, 142, 150, 235, 224, 282, 228, 308, 213, 298, 134, 286, 262, 211, 266, 217, 194, 207, 196, 236, 266, 292, 314, 303, 219, 244, 225, 187, 275, 204, 227, 333, 157, 185, 197, 242, 246, 259, 203, 234, 286, 212, 213, 201, 150, 207, 247, 162, 254, 183, 162, 228, 364, 0, 234, 168, 178, 231, 241, 145, 247, 277, 314, 149, 223, 0, 259, 203, 205, 240, 104, 251, 243, 230, 249, 247, 235, 228, 180, 212, 274, 280, 225, 166, 281, 230, 160, 0, 277, 247, 259, 259, 261, 0, 185, 312, 229, 196, 198, 178, 251, 274, 0, 188, 218, 288, 222, 239, 219, 179, 230, 242, 201, 240, 234, 221, 234, 186, 201, 154, 21, 211, 180, 234, 200, 0, 227, 265, 212, 213, 220, 210, 181, 219, 272, 176, 210, 128, 267, 215, 192, 219, 228, 261, 215, 221, 189, 182, 234, 219, 177, 0, 134, 243, 269, 283]
# p1endgamescores= [241, 160, 263, 193, 193, 152, 228, 165, 170, 174, 143, 178, 178, 166, 156, 197, 250, 233, 196, 259, 234, 158, 222, 156, 312, 189, 223, 179, 142, 202, 211, 221, 227, 180, 249, 197, 206, 212, 170, 148, 154, 302, 261, 199, 233, 286, 156, 180, 237, 180, 169, 161, 162, 171, 238, 195, 167, 197, 213, 168, 178, 172, 209, 200, 145, 144, 200, 109, 186, 218, 256, 149, 147, 175, 149, 175, 165, 187, 143, 198, 252, 154, 244, 230, 182, 210, 187, 174, 219, 156, 224, 276, 157, 256, 248, 240, 166, None, 226, 199] + [182, 155, 178, 151, 143, 181, 206, 214, 216, None, 148, 188, 200, 172, 168, 185, 149, 209, 185, 158, 148, 183, 279, 158, 204, 166, 226, 210, 181, 222, 292, None, 140, 208, 180, 155, 138, None, 268, 169, 180, 179, 150, 208, 208, 207, None, 230, 265, 263, 199, 133, 177, 194, 196, 184, 179, 245, 146, 157, 253, 221, 254, 199, None, 213, 261, 183, 256, None, 221, 166, 171, 146, 180, 174, 239, 223, 142, 154, 224, 188, 190, 163, 206, 159, 192, 212, 142, 160, 253, 236, 145, 164, 176, None, 318, 230, 145, 149]
# p2endgamescores =[137, 228, 216, 211, 160, 199, 157, 194, 226, 155, 175, 202, 163, 283, 214, 169, 196, 121, 237, 156, 165, 207, 136, 203, 151, 199, 178, 187, 176, 147, 177, 159, 279, 189, 189, 188, 167, 212, 216, 184, 229, 199, 214, 194, 173, 134, 127, 175, 174, 207, 186, 249, 169, 280, 134, 241, 210, 183, 199, 196, 159, 174, 196, 223, 224, 229, 195, 252, 207, 228, 167, 187, 275, 192, 157, 267, 157, 167, 184, 177, 179, 185, 146, 159, 216, 183, 193, 201, 137, 207, 208, 151, 240, 180, 144, 201, 312, None, 166, 168] + [178, 216, 169, 133, 229, 213, 231, 118, 202, None, 180, 179, 163, 189, 104, 227, 243, 167, 189, 223, 222, 206, 166, 202, 207, 257, 191, 148, 268, 170, 160, None, 242, 241, 235, 220, 247, None, 164, 288, 219, 145, 198, 151, 216, 223, None, 157, 202, 249, 171, 198, 168, 148, 192, 216, 194, 188, 222, 172, 209, 170, 201, 154, None, 180, 170, 162, 150, None, 184, 265, 185, 190, 204, 194, 172, 208, 272, 160, 183, 110, 196, 151, 174, 190, 180, 198, 193, 197, 143, 159, 225, 212, 165, None, 124, 231, 266, 194]
# times= [0.679628, 0.25508600000000015, 0.43363600000000013, 0.46399999999999997, 0.16895599999999966, 0.8028839999999997, 4.356975, 4.766655000000001, 13.009975999999998, 0.842490999999999, 0.28075099999999864, 0.9972959999999986, 48.782015, 0.5818909999999988, 6.880387999999996, 0.8260039999999975, 3.160426000000001, 0.48574299999999937, 0.8762100000000004, 0.8821650000000005, 0.4071739999999977, 1.005681999999993, 12.369773999999992, 0.17704700000000173, 1.269368, 0.37784999999999513, 1.5121890000000064, 0.5563420000000008, 0.47250099999999406, 0.23128400000000227, 26.600381999999996, 0.2803899999999828, 29.685314000000005, 0.7725399999999922, 6.294240000000002, 1.6343379999999854, 2.923070999999993, 1.9321170000000052, 2.941325000000006, 8.602185999999989, 0.6906869999999969, 0.7905009999999777, 1.866658000000001, 2.6730459999999994, 15.083501999999982, 14.42065199999999, 0.3328260000000114, 0.2328860000000077, 0.8476809999999944, 0.37371799999999666, 0.9207979999999907, 1.137835999999993, 2.7452109999999834, 6.731470999999999, 0.1816129999999987, 0.5684990000000028, 19.516876999999994, 0.6207129999999665, 0.2729100000000244, 3.6562700000000063, 0.8647639999999797, 2.9245769999999993, 3.6845069999999964, 0.446596999999997, 0.3615979999999581, 0.7543730000000437, 12.700273000000038, 0.9589679999999703, 12.965763999999979, 5.987181000000021, 5.982330000000047, 0.6078249999999912, 3.409419000000014, 2.7159429999999816, 0.5523069999999848, 1.0286279999999692, 4.084158000000002, 2.0033220000000256, 5.545056000000045, 0.4627570000000105, 0.5283400000000142, 6.723304999999982, 13.699906999999996, 0.4170040000000199, 1.166246000000001, 5.239834999999971, 11.313789999999983, 9.967671999999993, 6.971202000000005, 0.433037000000013, 5.9265589999999975, 29.824003000000005, 0.2711219999999912, 7.5386910000000285, 0.7378659999999968, 1.8787570000000073, 1.4262469999999894, 0.0019649999999842294, 8.404497000000049, 0.6995429999999487] + [0.8239759999999999, 2.8693069999999996, 0.6537499999999996, 2.7714349999999994, 8.983509, 0.8282300000000014, 0.47210600000000014, 0.38156700000000043, 1.2638299999999987, 0.0023620000000015295, 0.39808499999999825, 0.4976900000000022, 2.021621999999997, 0.5062250000000006, 1.0995509999999982, 44.594679000000006, 1.1106079999999992, 0.1901060000000001, 4.9927550000000025, 0.4491119999999995, 0.16447800000000257, 0.4472559999999959, 0.2007289999999955, 6.015974, 8.65227200000001, 2.798123000000004, 0.5117840000000058, 0.20239599999999314, 1.181874999999991, 6.446624, 0.2352120000000042, 0.0025700000000057344, 2.038622999999987, 8.662423999999987, 40.825494000000006, 3.801616999999993, 9.708668000000017, 0.003264999999998963, 1.3517200000000003, 0.3241820000000075, 4.665531999999985, 0.22598800000000097, 0.47939200000001847, 0.6513869999999997, 0.23165700000001266, 2.067575000000005, 0.002983000000000402, 1.1652159999999867, 0.2645569999999964, 1.0616219999999998, 1.220879999999994, 0.5819160000000068, 0.3263249999999971, 31.597102000000007, 1.9335740000000214, 0.31726299999999696, 9.984741999999983, 2.2907459999999844, 2.25433799999999, 0.3571900000000028, 0.2136199999999917, 4.687556999999998, 18.58814799999999, 1.8963210000000004, 0.01300999999995156, 2.2318690000000174, 0.21198600000002443, 1.8818820000000187, 1.4707750000000033, 0.002824000000032356, 0.660618999999997, 0.9758380000000102, 8.486921999999993, 0.17451399999998785, 1.6599539999999706, 2.7566459999999893, 0.20125600000000077, 0.2870110000000068, 0.9532130000000052, 0.268195999999989, 11.179752999999948, 0.2229910000000359, 3.120221000000015, 2.5662949999999682, 23.72242, 39.422394999999995, 0.36750200000000177, 0.8205089999999586, 4.176756000000012, 4.312453000000005, 0.8300380000000018, 0.7237190000000169, 2.1888329999999883, 0.21207099999998036, 15.47395499999999, 0.0029519999999934043, 8.639047000000005, 11.690905000000043, 30.171132999999998, 19.65263299999998]


print(statistics.median(nodes))

p1wins = 0
p2wins = 0
ties = 0
final_times = 0
ind = 0


for i in range(len(p1scores)):
    if p1endgamescores[i] is not None and p2endgamescores[i] is not None:
        ind = 1
        if p1scores[i] > p2scores[i]:
            p1wins += 1
        elif p1scores[i] < p2scores[i]:
            p2wins += 1
        else:
            ties += 1
        final_times += times[i]


average_time = statistics.median(times)
print(average_time)
print(p1wins)
print(p2wins)
print(ties)

        
        
"""""""""""""""""""""""""""NODE SORTING Evaluation"""""""""""""""""""""""""""""""""

# smoves = [499, 1628, 990, 2, 13, 133, 17, 1173, 60, 5, 1573, 333, 12296, 31539, 51, 35, 229, 61, 1644, 129, 52, 55, 2175, 41, 137, 73, 7, 12, 41, 993, 14, 23, 15, 670, 505, 7139, 23, 17, 492, 2, 7498, 17, 4, 859, 4331, 8, 3, 165, 3847]
# rmoves = [4, 866, 39, 487, 28, 838, 45, 5786, 14, 320, 869, 6, 16965, 322, 2250, 2670, 495, 87, 1490, 2, 473, 66, 35, 165, 25, 86, 19, 214, 87, 99, 361, 49, 17, 19382, 8629, 169, 754, 5, 734, 9616, 94, 2669, 309, 1071, 23607, 2763, 15]
# len_sorted_moves = [2918, 19, 20, 167, 223, 24, 66, 239, 901, 34, 5, 439, 14, 34, 667, 170, 79, 670, 308, 66, 62, 253, 343, 4, 79, 2440, 54, 2283, 92, 206, 433, 3, 43, 938, 54, 10, 197, 2857, 75, 13, 105, 44, 119, 19, 69, 4487, 1236, 25, 10, 2, 199, 981, 96, 12, 815, 53, 726, 61, 3301, 69, 1665, 1018, 2148, 120432, 49, 262, 27, 528, 75, 2508, 65, 43, 60, 109, 32, 1664, 287, 2759, 1428, 246, 128, 90, 898, 20, 371, 56, 13, 8, 1196, 6033, 129, 13, 1399]


# len_sorted_times = [28.457569, 0.33411299999999855, 0.3422729999999987, 1.7242899999999999, 1.9751229999999964, 0.34948599999999885, 0.8178440000000009, 2.284306000000001, 8.470917, 0.49538099999999474, 0.2556449999999941, 4.728405000000002, 0.34807999999999595, 0.023735999999999535, 0.12552500000000322, 0.483984999999997, 7.080171, 1.9386399999999995, 0.9097670000000022, 6.730280999999998, 2.9818090000000126, 0.7033690000000092, 0.72840699999999, 2.370058, 4.470228999999989, 0.2786049999999989, 0.8592219999999884, 20.823213999999993, 0.6710889999999949, 17.893317999999994, 0.8816439999999943, 1.7517409999999956, 3.9330260000000123, 0.20288299999999992, 0.6357670000000013, 7.664194999999992, 0.7840810000000147, 0.3089580000000183, 1.8615410000000168, 24.267165000000006, 0.848608999999982, 0.27916300000001115, 1.231020000000001, 0.5714190000000201, 0.04297500000001264, 1.3442199999999787, 0.3977549999999894, 0.8133920000000217, 36.14156200000002, 10.697302999999977, 28.457569, 0.33411299999999855, 0.3422729999999987, 1.7242899999999999, 1.9751229999999964, 0.34948599999999885, 0.8178440000000009, 2.284306000000001, 8.470917, 0.49538099999999474, 0.2556449999999941, 4.728405000000002, 0.34807999999999595, 0.023735999999999535, 0.12552500000000322, 0.483984999999997, 7.080171, 1.9386399999999995, 0.9097670000000022, 6.730280999999998, 2.9818090000000126, 0.7033690000000092, 0.72840699999999, 2.370058, 4.470228999999989, 0.2786049999999989, 0.8592219999999884, 20.823213999999993, 0.6710889999999949, 17.893317999999994, 0.8816439999999943, 1.7517409999999956, 3.9330260000000123, 0.20288299999999992, 0.6357670000000013, 7.664194999999992, 0.7840810000000147, 0.3089580000000183, 1.8615410000000168, 24.267165000000006, 0.848608999999982, 0.27916300000001115, 1.231020000000001, 0.5714190000000201, 0.04297500000001264, 1.3442199999999787, 0.3977549999999894, 0.8133920000000217, 36.14156200000002, 10.697302999999977, 0.5683670000000001, 0.26924099999999984, 0.17733, 2.02914, 8.453273, 0.8671249999999997, 0.2741490000000013, 8.405273000000001, 0.003088000000001756, 0.7832660000000011, 6.770484, 0.001788000000001233, 0.7870349999999995, 32.759317, 0.8810690000000037, 15.959770999999996, 9.082476, 23.287146000000007, 899.159576, 0.6069360000000188, 2.3854630000000725, 0.4569099999999935, 0.13301100000001043, 4.110847000000035, 0.874349000000052, 20.135546999999974, 0.7187790000000405, 0.4951149999999416, 0.675617000000102, 1.151836000000003, 0.4612999999999374, 13.440222000000176, 0.0017380000001594453, 2.7643929999999273, 22.713453000000072, 11.432960999999978, 2.4568299999998544, 1.368257000000085, 0.9997370000000956, 7.1294339999999465, 0.3538969999999608, 3.9655290000000605, 0.6393659999998818, 0.26812500000005457, 0.23288300000012896, 9.49111599999992, 55.96718499999997, 1.2186229999999796, 0.28220200000009754, 10.691080000000056]
# stimes = [5.462273, 13.628348999999998, 9.027452999999998, 0.20035800000000137, 0.350676, 1.452566000000001, 0.3063469999999988, 9.485683000000002, 0.7841099999999983, 0.20266799999999563, 17.036028, 3.118584999999996, 102.818501, 242.39040999999997, 0.5522730000000138, 0.5206039999999916, 2.3233899999999608, 0.7285350000000221, 14.949749999999995, 0.17539299999998548, 1.184301000000005, 0.577241000000015, 0.6432050000000231, 20.644333999999958, 0.5160099999999943, 1.323853999999983, 0.8926519999999982, 0.2601789999999937, 0.33121899999997595, 0.4770679999999743, 8.132326000000035, 0.31665900000001557, 0.39193999999997686, 0.3116600000000176, 6.508721000000037, 5.085165000000018, 55.82077000000004, 0.3661329999999907, 0.4054830000000038, 4.384490000000028, 0.16655200000002424, 70.42404600000009, 0.35272199999997156, 0.2003789999999981, 8.768035000000054, 35.55618399999992, 0.24955899999997655, 0.1856559999999945, 1.568037000000004, 29.536572999999976]
# rtimes = [0.338896, 8.730683, 0.4996229999999997, 4.7850139999999985, 0.37907200000000074, 7.887321, 0.5963290000000008, 47.081212, 0.29992200000000935, 2.7501099999999923, 7.412319000000011, 0.2718089999999904, 167.70644700000003, 3.072586000000001, 21.200469, 20.699720999999954, 4.870582000000013, 0.763321000000019, 13.35970900000001, 0.21506199999998898, 4.463702000000012, 0.7095789999999624, 0.539273000000037, 1.8219100000000026, 0.0038090000000465807, 0.36327099999999746, 1.0180799999999977, 0.3082679999999982, 2.0309389999999894, 0.7468609999999671, 0.9124090000000251, 2.8971290000000067, 0.584384, 0.28475700000001325, 163.11071900000002, 76.13204299999995, 2.0015580000000455, 6.309083999999984, 0.19872700000007626, 6.9720570000000635, 101.70751199999995, 0.002196000000026288, 0.14134500000000116, 0.9927730000000565, 26.648521000000073, 2.640781000000061, 11.751337000000035, 197.13955399999998, 26.002137999999945, 0.29406200000005356]

# lmoves = len_sorted_moves[:50]
# ltimes = len_sorted_times[:50]

# print(statistics.mean(smoves))
# print(statistics.median(rmoves))
# print(statistics.median(lmoves))
# print(statistics.median(stimes))
# print(statistics.median(rtimes))
# print(statistics.median(ltimes))








# print(len(baselinep1))
# print(len(baselinep2))

# p1wins = 0
# p2wins = 0

# for i in range(len(baselinep1)):
#     if baselinep1[i] >= baselinep2[i]:
#         p1wins += 1
#     else:
#         p2wins += 1


# print()
# sns.set_palette(["cadetblue", "gold", "tomato"])
# sns.barplot(x=["AB Pruning", "Baseline"], y = [p1wins, p2wins])
# plt.title("Wins after 200 Simulations")
# plt.xlabel("Agents")
# plt.ylabel("# of Wins")
# plt.show()
# print(p1wins)
# p1wins = 0
# p2wins = 0
# for i in range(len(randomp1)):
#     if randomp1[i] >= randomp2[i]:
#         p1wins += 1
#     else:
#         p2wins += 1


# print(p1wins)
# print(p1wins + p2wins)
# sns.set_palette(["cadetblue", "orangered"])
# sns.barplot(x=["AB Pruning", "Baseline"], y = [p1wins, p2wins])
# plt.title("Wins after 55 Simulations")
# plt.xlabel("Agents")
# plt.ylabel("# of Wins")


# plt.show()

import os #to run SLiM from python
import numpy as np #for vectors etc
import pyslim, tskit, msprime #to recapitate, mutate, and sample tree sequences, and compute statistics
import csv #to save statistics

# parameters for SLiM
K = 1e4 #carrying capacity
N0 = K #intial population size
d = 0.0 #decline rate of ancestral homozygote
s = 0.13 #selection coefficient
h = 0.5 #dominance coefficient
B = 2 #number of offspring per parent
L = 2e7 #number of sites on chromosome
L0 = round(L/2) #location of selected site (one of the center sites)
u = 1e-5 #mutation rate at selected site
m = 0 #migration rate
rbp = 2e-8 #per basepair recombination rate
k = 0 #initial number of beneficial alleles
datadir = "data/" #location to put output files
nreps = 100 #number of replicates (number of runs where allele fixes and population recovers)
maxt = 1000 #maximum number of generations (safestop that should never be reached)

#parameters for msprime
Ne = round(K * 4/7) #long-term effective population size
U = 6e-9 #per basepair mutation rate at neutral sites
nsamples = 100 #number of chromosomes to sample for stats
nwindows = 100 #number of windows across genome to compute stats for
R = 0.001 #recombination distance between neutral loci to calculate LD for

#genome window calculation
windows = np.linspace(0, L, nwindows+1) #delimit windows
midpoints = windows[:-1]+(windows[1:]-windows[:-1])/2 #midpoint of each window
distances = midpoints-L0 #distance of midpoint from selected site
recombination = [(1-(1-2*rbp)**abs(i))/2*np.sign(i) for i in distances] #recombination rate at midpoint (signed for plotting)

#empty vector for number of unique copies of beneficial allele remaining at fixation in each replicate
X = np.zeros(nreps)

#for each replicate
for i in range(nreps): 
	
	# define SLiM script ...
	script = """
	initialize() {
		
		defineConstant("K", %d); //carrying capacity (integer)
		defineConstant("N0", %d); //initial pop size (integer)
		defineConstant("d", %f); // wildtype decline rate [0,1]
		defineConstant("s", %f); //beneficial selection coefficient ([0,1]; s>d for rescue to be possible) 
		defineConstant("h", %f); //beneficial dominance [0,1]
		defineConstant("B", %d); //offspring per parent (positive integer; must be great than 1 for possible persistence) 
		defineConstant("L", %d - 1); //number of sites (positive integer)
		defineConstant("L0", %d - 1); //site number of beneficial locus (positive integer, L0<L)
		defineConstant("u", %.8f); //mutation rate at beneficial locus [0,1]
		defineConstant("m", %.8f); //migration rate [0,1]
		defineConstant("rbp", %.8f); //recombination rate per base pair [0,1]
		defineConstant("k", %d); //initial number of mutants
		defineConstant("outfile_dynamics", "%sdynamics_%d.txt"); //where to save dynamics
		defineConstant("outfile_tree", "%stree_%d.trees"); //where to save tree
		defineConstant("simID", getSeed()); //get the random seed to label temporary file

		initializeSLiMModelType("nonWF"); //non Wright Fisher model
		initializeTreeSeq(); //record the tree
		initializeMutationType("m1", h, "f", s); //beneficial mutation characteristics
		m1.mutationStackPolicy = "f"; //keep first mutation
		initializeMutationType("m2", 0.5, "f", 0.0); //neutral mutations (heritability has no affect)
		initializeGenomicElementType("g1", m1, 1.0); //define element g1 to have beneficial mutations
		initializeGenomicElementType("g2", m2, 1.0); //define element g2 to have neutral mutations
		initializeGenomicElement(g1, L0, L0); //element g1 is just one site
		initializeGenomicElement(g2, 0, L0 - 1); // element g2 is everything to the left...
		initializeGenomicElement(g2, L0 + 1, L); // ...and everything to the right of LO
		initializeMutationRate(c(0,u,0), c(L0-1, L0, L)); //mutation rate per site 
		initializeRecombinationRate(rbp); //recombination rate between sites
		writeFile(outfile_dynamics, "t n p"); //start writing to the dynamics file     
	}

	reproduction() { //occurs immediately before early events
		for (i in 1:B) //B matings per parent
			subpop.addCrossed(individual, subpop.sampleIndividuals(1)); //random mating, 1 offspring per pair
	}

	//discrete generations, hard carrying capacity, census and update fitness
	1:%d early() {

		//initialize population
		if (sim.generation == 1) {
			sim.addSubpop("p1", N0); //initialize population of wildtypes
			target = sample(p1.genomes, k); //choose k chromosomes without replacement...
			for (i in target)
				i.addNewDrawnMutation(m1, L0); //... and give beneficial mutation
			sim.outputFull("/tmp/slim_" + simID + ".txt"); //output this initial state to use for future runs if needed
		}

		//enforce discrete generations
		inds = sim.subpopulations.individuals; //get info on all individuals
		inds[inds.age > 0].fitnessScaling = 0.0; //parents all die at next instance of viability selection

		//hard carrying capacity by random culling
		off = inds[inds.age == 0]; //offspring
		N = length(off); //total number of offspring
		indices = which(inds.age == 0); //indices of offspring
		if (N > K) { //if more than K...
			inds[sample(indices, N-K)].fitnessScaling = 0.0; //...kill a random subset to reduce N to K
			off = inds[inds.fitnessScaling > 0]; //get surviving offspring
		}	

		// migration
		if (m>0 & N>0) { //if adapting from migration and some offspring made
			if (runif(1)<m) { //with probability m
				target = sample(off.genomes, 1); //choose a chromosome to add a migrant allele to
	 			target.addNewDrawnMutation(m1, L0); //add the migrant allele	
			}
		}
		
		// census offspring
		N = length(off); //population size
		freq = sum(asInteger(off.genomes.countOfMutationsOfType(m1)>0))/(2*N); //frequency of beneficial mutation
		if ((u==0 & m==0 & freq == 0) | (N==0)) { //if fail to adapt
	        writeFile("data/prescue.csv", "0", append=T); //record extinction
	        writeFile(outfile_dynamics, "t n p"); //erase and restart the output file
			catn("all hope was lost in generation " + sim.generation + " - RESTARTING"); //alert the user
			sim.readFromPopulationFile("/tmp/slim_" + simID + ".txt"); //reinitialize simulation
		}
		else {           
			catn(sim.generation + ": " + N + ", " + freq); //print generation and population size and frequency
			writeFile(outfile_dynamics, sim.generation + " " + N + " " + freq, append=T);
	        if (freq == 1.0 & N == K) { //if mutation fixed and population recovered
	        	writeFile("data/prescue.csv", "1", append=T); //record rescue
				catn("rescue complete in generation " + sim.generation);
	            sim.simulationFinished(); //end simulation
	            sim.treeSeqOutput(outfile_tree); //save tree sequence
	        }
		}

		//fitness scaling (viability selection occurs after early events)
		p1.fitnessScaling = (1.0 - d)/B; //survival probability V = X(1-d)/B, where X is the fitness effect of the selected site (X=1 for wildtype, X=1+s*h for heterozygotes, X=1+s for mutant homozygotes)
	}

	//backup: end simulation if runs too long and print warning to increase maxt
	%d late () {
		catn("times up, make maxt longer!");
		sim.simulationFinished();
	}
	""" %(K,N0,d,s,h,B,L,L0,u,m,rbp,k,datadir,i,datadir,i,maxt,maxt+1)

	# and run it
	os.system("echo '" + script + "' | slim") 

	# then load tree-sequences
	ts = pyslim.load("%stree_%d.trees" %(datadir, i)) 
	
	# calculate the number of unique copies of beneficial allele
	X[i] = len(np.unique([i.genotypes for i in ts.variants()][0])) 
	
	# recapitate the tree sequence and overlay neutral mutations
	ts = msprime.mutate(ts.recapitate(rbp, Ne=Ne).simplify(), rate=U, keep=True)
	
	# take a random sample
	offspring_nodes = (np.where(ts.tables.nodes.time==0.)[0]).astype(np.int32) #chromosomes in offspring (to exclude the parental generation from the samples)
	samples = np.random.choice(offspring_nodes, nsamples, replace=False) #random sample of chromosomes in offspring
	ts = ts.simplify(samples) #simplify to sample only
	ts.dump("%stree_%d_sample.trees" %(datadir, i)) #save sample tree (in case need to calculate any more statistics)
	# ts = tskit.load("%stree_%d_sample.trees" %(datadir,i)) #load dumped version if running new stats below

	# calculate pairwise diversity
	site_div = ts.diversity([ts.samples()], windows=windows) #compute average pairwise diversity in windows
	site_div = [i[0] for i in site_div] #flatten site_div list
	
	# calculate Tajima's D
	tajimasD = ts.Tajimas_D([ts.samples()], windows=windows) #compute average Tajima's D in windows
	tajimasD = [i[0] for i in tajimasD] #flatten tajimasD list
	
	# save pairwise diversity and Tajima's D to file, with associated recombination rates
	csvData = zip(recombination, site_div, tajimasD)
	with open('%sstats_%d.csv' %(datadir,i), 'w') as csvFile:
		writer = csv.writer(csvFile)
		writer.writerows(csvData)
	csvFile.close()

	# calculate site frequency spectrum
	sfs = ts.allele_frequency_spectrum([ts.samples()], windows=windows, polarised=True) #unfolded SFS, averaged within windows
	np.savetxt('%ssfs_%d.txt' %(datadir,i), sfs)

	# calculate linkage disequilibrium
	all_positions = [i.position for i in ts.mutations()] #positions of all mutations
	freqs = [sum(i.genotypes)/nsamples for i in ts.variants()] #frequency of derived alleles at these positions
	seg = [i for i,j in enumerate(freqs) if 0<j and j<1] #indices of segregating mutations
	positions = [all_positions[i] for i in seg] #positions of segregating mutations
	idxs = [np.argmin(np.abs(positions-i)) for i in midpoints] #find mutation indices nearest the window midpoints
	idx_positions = [positions[i] for i in idxs] #positions of mutations nearest midpoints
	distance = np.log(1-R)/np.log(1-2*rbp) #convert the specified recombination rate between mutations to number of sites
	other_positions = idx_positions + distance #this is where we want the other mutation
	other_idxs = [np.argmin(np.abs(positions-i)) for i in other_positions] #mutation indices nearest the desired poition
	lds = [tskit.LdCalculator(ts).r2(seg[idxs[i]], seg[other_idxs[i]]) for i in range(nwindows)] #linkage disequilibrium between the two mutations

	# save ld to file with associated recombination rates
	csvData = zip(recombination, lds)
	with open('%sld_%d.csv' %(datadir,i), 'w') as csvFile:
		writer = csv.writer(csvFile)
		writer.writerows(csvData)
	csvFile.close()

# save number of unique copies of beneficial allele remaining in each replicate
np.savetxt('%snalleles.txt' %datadir, X, fmt='%d')

'''
Models (mostly base classes) for the various kinds of renderer
types that Bokeh supports.

'''
#-----------------------------------------------------------------------------
# Boilerplate
#-----------------------------------------------------------------------------
import logging # isort:skip
log = logging.getLogger(__name__)

#-----------------------------------------------------------------------------
# Imports
#-----------------------------------------------------------------------------

# Standard library imports
from difflib import get_close_matches

# Bokeh imports
from ..core.enums import RenderLevel
from ..core.has_props import abstract
from ..core.properties import (
    Auto,
    Bool,
    Either,
    Enum,
    Float,
    Instance,
    Override,
    String,
)
from ..core.validation import error
from ..core.validation.errors import (
    BAD_COLUMN_NAME,
    CDSVIEW_FILTERS_WITH_CONNECTED,
    CDSVIEW_SOURCE_DOESNT_MATCH,
    MALFORMED_GRAPH_SOURCE,
    MISSING_GLYPH,
    NO_SOURCE_FOR_GLYPH,
)
from ..model import Model
from .glyphs import Circle, ConnectedXYGlyph, Glyph, MultiLine
from .graphs import GraphHitTestPolicy, LayoutProvider, NodesOnly
from .sources import CDSView, ColumnDataSource, DataSource, WebSource
from .tiles import TileSource, WMTSTileSource

#-----------------------------------------------------------------------------
# Globals and constants
#-----------------------------------------------------------------------------

__all__ = (
    'DataRenderer',
    'GlyphRenderer',
    'GraphRenderer',
    'GuideRenderer',
    'Renderer',
    'TileRenderer',
)

#-----------------------------------------------------------------------------
# General API
#-----------------------------------------------------------------------------

#-----------------------------------------------------------------------------
# Dev API
#-----------------------------------------------------------------------------

@abstract
class Renderer(Model):
    '''
    An abstract base class for renderer types.

    '''

    level = Enum(RenderLevel, help="""
    Specifies the level in which to paint this renderer.
    """)

    visible = Bool(default=True, help="""
    Is the renderer visible.
    """)

    x_range_name = String('default', help="""
    A particular (named) x-range to use for computing screen locations when
    rendering glyphs on the plot. If unset, use the default x-range.
    """)

    y_range_name = String('default', help="""
    A particular (named) y-range to use for computing screen locations when
    rendering glyphs on the plot. If unset, use the default y-range.
    """)


@abstract
class DataRenderer(Renderer):
    '''
    An abstract base class for data renderer types (e.g. ``GlyphRenderer``, ``TileRenderer``, ``GraphRenderer``).

    '''

    level = Override(default="glyph")

class TileRenderer(DataRenderer):
    '''

    '''

    tile_source = Instance(TileSource, default=lambda: WMTSTileSource(), help="""
    Local data source to use when rendering glyphs on the plot.
    """)

    alpha = Float(1.0, help="""
    tile opacity 0.0 - 1.0
    """)

    smoothing = Bool(default=True, help="""
    Enable image smoothing for the rendered tiles.
    """)

    render_parents = Bool(default=True, help="""
    Flag enable/disable drawing of parent tiles while waiting for new tiles to arrive. Default value is True.
    """)

class GlyphRenderer(DataRenderer):
    '''

    '''

    def __str__(self):
        return f"GlyphRenderer(id={self.id}, glyph={str(self.glyph)}, ...)"

    @error(CDSVIEW_FILTERS_WITH_CONNECTED)
    def _check_cdsview_filters_with_connected(self):
        if isinstance(self.glyph, ConnectedXYGlyph) and len(self.view.filters) > 0:
            return str(self)

    @error(MISSING_GLYPH)
    def _check_missing_glyph(self):
        if not self.glyph: return str(self)

    @error(NO_SOURCE_FOR_GLYPH)
    def _check_no_source_for_glyph(self):
        if not self.data_source: return str(self)

    @error(CDSVIEW_SOURCE_DOESNT_MATCH)
    def _check_cdsview_source(self):
        if self.data_source is not self.view.source: return str(self)

    @error(BAD_COLUMN_NAME)
    def _check_bad_column_name(self):
        if not self.glyph: return
        if not self.data_source: return
        if isinstance(self.data_source, WebSource): return
        missing_values = set()
        specs = self.glyph.dataspecs()
        for name, item in self.glyph.properties_with_values(include_defaults=False).items():
            if name not in specs: continue
            if not isinstance(item, dict): continue
            if not isinstance(self.data_source, ColumnDataSource): continue
            if 'field' in item and item['field'] not in self.data_source.column_names:
                missing_values.add((item['field'], name))
        if missing_values:
            suggestions = ['" (closest match: "%s")' % s[0] if s else '"' for s in [
                get_close_matches(term[0], self.data_source.column_names, n=1) for term in missing_values]]
            missing_values = [("".join([m[0], s]), m[1]) for m, s in zip(missing_values, suggestions)]
            missing = ['key "%s" value "%s' % (k, v) for v, k in missing_values]
            return "%s [renderer: %s]" % (", ".join(sorted(missing)), self)

    def __init__(self, **kw):
        super().__init__(**kw)
        if "view" not in kw:
            self.view = CDSView(source=self.data_source)

    data_source = Instance(DataSource, help="""
    Local data source to use when rendering glyphs on the plot.
    """)

    view = Instance(CDSView, help="""
    A view into the data source to use when rendering glyphs. A default view
    of the entire data source is created when a view is not passed in during
    initialization.

    .. note:
        Only the default (filterless) CDSView is compatible with glyphs that
        have connected topology, such as Line and Patch. Setting filters on
        views for these glyphs will result in a warning and undefined behavior.
    """)

    glyph = Instance(Glyph, help="""
    The glyph to render, in conjunction with the supplied data source
    and ranges.
    """)

    selection_glyph = Either(Auto, Instance(Glyph), default="auto", help="""
    An optional glyph used for selected points.

    If set to "auto" then the standard glyph will be used for selected
    points.
    """)

    nonselection_glyph = Either(Auto, Instance(Glyph), default="auto", help="""
    An optional glyph used for explicitly non-selected points
    (i.e., non-selected when there are other points that are selected,
    but not when no points at all are selected.)

    If set to "auto" then a glyph with a low alpha value (0.1) will
    be used for non-selected points.
    """)

    hover_glyph = Instance(Glyph, help="""
    An optional glyph used for inspected points, e.g., those that are
    being hovered over by a ``HoverTool``.
    """)

    muted_glyph = Instance(Glyph, help="""
    """)

    muted = Bool(False, help="""
    """)

_DEFAULT_NODE_RENDERER = lambda: GlyphRenderer(
    glyph=Circle(), data_source=ColumnDataSource(data=dict(index=[]))
)

_DEFAULT_EDGE_RENDERER = lambda: GlyphRenderer(
    glyph=MultiLine(), data_source=ColumnDataSource(data=dict(start=[], end=[]))
)

class GraphRenderer(DataRenderer):
    '''

    '''

    @error(MALFORMED_GRAPH_SOURCE)
    def _check_malformed_graph_source(self):
        missing = []
        if "index" not in self.node_renderer.data_source.column_names:
            missing.append("Column 'index' is missing in GraphSource.node_renderer.data_source")
        if "start" not in self.edge_renderer.data_source.column_names:
            missing.append("Column 'start' is missing in GraphSource.edge_renderer.data_source")
        if "end" not in self.edge_renderer.data_source.column_names:
            missing.append("Column 'end' is missing in GraphSource.edge_renderer.data_source")
        if missing:
            return " ,".join(missing) + " [%s]" % self

    layout_provider = Instance(LayoutProvider, help="""
    An instance of a ``LayoutProvider`` that supplies the layout of the network
    graph in cartesian space.
    """)

    node_renderer = Instance(GlyphRenderer, default=_DEFAULT_NODE_RENDERER, help="""
    Instance of a ``GlyphRenderer`` containing an ``XYGlyph`` that will be rendered
    as the graph nodes.
    """)

    edge_renderer = Instance(GlyphRenderer, default=_DEFAULT_EDGE_RENDERER, help="""
    Instance of a ``GlyphRenderer`` containing an ``MultiLine`` Glyph that will be
    rendered as the graph edges.
    """)

    selection_policy = Instance(GraphHitTestPolicy, default=lambda: NodesOnly(), help="""
    An instance of a ``GraphHitTestPolicy`` that provides the logic for selection
    of graph components.
    """)

    inspection_policy = Instance(GraphHitTestPolicy, default=lambda: NodesOnly(), help="""
    An instance of a ``GraphHitTestPolicy`` that provides the logic for inspection
    of graph components.
    """)

@abstract
class GuideRenderer(Renderer):
    ''' A base class for all guide renderer types. ``GuideRenderer`` is
    not generally useful to instantiate on its own.

    '''

    level = Override(default="guide")

#-----------------------------------------------------------------------------
# Private API
#-----------------------------------------------------------------------------

#-----------------------------------------------------------------------------
# Code
#-----------------------------------------------------------------------------

import sqlite3


def init():
    global database_file
    global db
    global cursor

    database_file = "db/database.db"
    db = sqlite3.connect(database_file)
    cursor = db.cursor()

    # Nomes das colunas do database
    # sql = "select * from database where 1=0;"
    # cursor.execute(sql)
    # p = [d[0] for d in cursor.description]
    # print(p)

    # def query(command, arguments=[]):
    #     _db = sqlite3.connect(database_file)
    #     _c = _db.cursor()
    #     _c.execute(command, arguments)
    #     results = _c.fetchall()
    #     return results

#!/usr/bin/env python
# encoding: utf-8
# Carlos Rafael Giani, 2007 (dv)
# Thomas Nagy, 2007-2008 (ita)

import os, sys, re, optparse
import ccroot # <- leave this
import TaskGen, Utils, Task, Configure, Logs, Build
from Logs import debug, error
from TaskGen import taskgen, feature, after, before, extension
from Configure import conftest

EXT_D = ['.d', '.di', '.D']
D_METHS = ['apply_core', 'apply_vnum', 'apply_objdeps'] # additional d methods

def filter_comments(filename):
	txt = Utils.readf(filename)
	buf = []

	i = 0
	max = len(txt)
	while i < max:
		c = txt[i]
		# skip a string
		if c == '"':
			i += 1
			c = ''
			while i < max:
				p = c
				c = txt[i]
				i += 1
				if i == max: return buf
				if c == '"':
					cnt = 0
					while i < cnt and i < max:
						#print "cntcnt = ", str(cnt), self.txt[self.i-2-cnt]
						if txt[i-2-cnt] == '\\': cnt+=1
						else: break
					#print "cnt is ", str(cnt)
					if (cnt%2)==0: break
			i += 1
		# skip a char
		elif c == "'":
			i += 1
			if i == max: return buf
			c = txt[i]
			if c == '\\':
				i += 1
				if i == max: return buf
				c = txt[i]
				if c == 'x':
					i += 2 # skip two chars
				elif c == 'u':
					i += 4 # skip unicode chars
			i += 1
			if i == max: return buf
			c = txt[i]
			if c != '\'': error("uh-oh, invalid character")

		# skip a comment
		elif c == '/':
			if i == max: break
			c = txt[i+1]
			# eat /+ +/ comments
			if c == '+':
				i += 1
				nesting = 1
				prev = 0
				while i < max:
					c = txt[i]
					if c == '+':
						prev = 1
					elif c == '/':
						if prev:
							nesting -= 1
							if nesting == 0: break
						else:
							if i < max:
								i += 1
								c = txt[i]
								if c == '+':
									nesting += 1
							else:
								return buf
					else:
						prev = 0
					i += 1
			# eat /* */ comments
			elif c == '*':
				i += 1
				while i < max:
					c = txt[i]
					if c == '*':
						prev = 1
					elif c == '/':
						if prev: break
					else:
						prev = 0
					i += 1
			# eat // comments
			elif c == '/':
				i += 1
				c = txt[i]
				while i < max and c != '\n':
					i += 1
					c = txt[i]
		# a valid char, add it to the buffer
		else:
			buf.append(c)
		i += 1
	return buf

class d_parser(object):
	def __init__(self, env, incpaths):
		#self.code = ''
		#self.module = ''
		#self.imports = []

		self.allnames = []

		self.re_module = re.compile("module\s+([^;]+)")
		self.re_import = re.compile("import\s+([^;]+)")
		self.re_import_bindings = re.compile("([^:]+):(.*)")
		self.re_import_alias = re.compile("[^=]+=(.+)")

		self.env = env

		self.nodes = []
		self.names = []

		self.incpaths = incpaths

	def tryfind(self, filename):
		found = 0
		for n in self.incpaths:
			found = n.find_resource(filename.replace('.', '/') + '.d')
			if found:
				self.nodes.append(found)
				self.waiting.append(found)
				break
		if not found:
			if not filename in self.names:
				self.names.append(filename)

	def get_strings(self, code):
		#self.imports = []
		self.module = ''
		lst = []

		# get the module name (if present)

		mod_name = self.re_module.search(code)
		if mod_name:
			self.module = re.sub('\s+', '', mod_name.group(1)) # strip all whitespaces

		# go through the code, have a look at all import occurrences

		# first, lets look at anything beginning with "import" and ending with ";"
		import_iterator = self.re_import.finditer(code)
		if import_iterator:
			for import_match in import_iterator:
				import_match_str = re.sub('\s+', '', import_match.group(1)) # strip all whitespaces

				# does this end with an import bindings declaration?
				# (import bindings always terminate the list of imports)
				bindings_match = self.re_import_bindings.match(import_match_str)
				if bindings_match:
					import_match_str = bindings_match.group(1)
					# if so, extract the part before the ":" (since the module declaration(s) is/are located there)

				# split the matching string into a bunch of strings, separated by a comma
				matches = import_match_str.split(',')

				for match in matches:
					alias_match = self.re_import_alias.match(match)
					if alias_match:
						# is this an alias declaration? (alias = module name) if so, extract the module name
						match = alias_match.group(1)

					lst.append(match)
		return lst

	def start(self, node):
		self.waiting = [node]
		# while the stack is not empty, add the dependencies
		while self.waiting:
			nd = self.waiting.pop(0)
			self.iter(nd)

	def iter(self, node):
		path = node.abspath(self.env) # obtain the absolute path
		code = "".join(filter_comments(path)) # read the file and filter the comments
		names = self.get_strings(code) # obtain the import strings
		for x in names:
			# optimization
			if x in self.allnames: continue
			self.allnames.append(x)

			# for each name, see if it is like a node or not
			self.tryfind(x)

def scan(self):
	"look for .d/.di the .d source need"
	env = self.env
	gruik = d_parser(env, env['INC_PATHS'])
	gruik.start(self.inputs[0])

	if Logs.verbose:
		debug('deps: nodes found for %s: %s %s' % (str(self.inputs[0]), str(gruik.nodes), str(gruik.names)))
		#debug("deps found for %s: %s" % (str(node), str(gruik.deps)), 'deps')
	return (gruik.nodes, gruik.names)

def get_target_name(self):
	"for d programs and libs"
	v = self.env
	tp = 'program'
	for x in self.features:
		if x in ['dshlib', 'dstaticlib']:
			tp = x.lstrip('d')
	return v['D_%s_PATTERN' % tp] % self.target

d_params = {
'dflags': '',
'importpaths':'',
'libs':'',
'libpaths':'',
'generate_headers':False,
}

@feature('d')
@before('apply_type_vars')
def init_d(self):
	for x in d_params:
		setattr(self, x, getattr(self, x, d_params[x]))

class d_taskgen(TaskGen.task_gen):
	def __init__(self, *k, **kw):
		TaskGen.task_gen.__init__(self, *k, **kw)

		# COMPAT
		if len(k) > 1:
			self.features.append('d' + k[1])

# okay, we borrow a few methods from ccroot
TaskGen.bind_feature('d', D_METHS)

@feature('d')
@before('apply_d_libs')
def init_d(self):
	Utils.def_attrs(self,
		dflags='',
		importpaths='',
		libs='',
		libpaths='',
		uselib='',
		uselib_local='',
		generate_headers=False, # set to true if you want .di files as well as .o
		compiled_tasks=[],
		add_objects=[],
		link_task=None)

@feature('d')
@after('apply_d_link', 'init_d')
@before('apply_vnum')
def apply_d_libs(self):
	"""after apply_link because of 'link_task'
	after default_cc because of the attribute 'uselib'"""
	env = self.env

	# 1. the case of the libs defined in the project (visit ancestors first)
	# the ancestors external libraries (uselib) will be prepended
	self.uselib = self.to_list(self.uselib)
	names = self.to_list(self.uselib_local)

	seen = set([])
	tmp = Utils.deque(names) # consume a copy of the list of names
	while tmp:
		lib_name = tmp.popleft()
		# visit dependencies only once
		if lib_name in seen:
			continue

		y = self.name_to_obj(lib_name)
		if not y:
			raise Utils.WafError('object %r was not found in uselib_local (required by %r)' % (lib_name, self.name))
		y.post()
		seen.add(lib_name)

		# object has ancestors to process (shared libraries): add them to the end of the list
		if getattr(y, 'uselib_local', None):
			lst = y.to_list(y.uselib_local)
			if 'dshlib' in y.features or 'cprogram' in y.features:
				lst = [x for x in lst if not 'cstaticlib' in self.name_to_obj(x).features]
			tmp.extend(lst)

		# link task and flags
		if getattr(y, 'link_task', None):

			link_name = y.target[y.target.rfind(os.sep) + 1:]
			if 'dstaticlib' in y.features or 'dshlib' in y.features:
				env.append_unique('DLINKFLAGS', env.DLIB_ST % link_name)
				env.append_unique('DLINKFLAGS', env.DLIBPATH_ST % y.link_task.outputs[0].parent.bldpath(env))

			# the order
			self.link_task.set_run_after(y.link_task)

			# for the recompilation
			dep_nodes = getattr(self.link_task, 'dep_nodes', [])
			self.link_task.dep_nodes = dep_nodes + y.link_task.outputs

		# add ancestors uselib too - but only propagate those that have no staticlib
		for v in self.to_list(y.uselib):
			if not v in self.uselib:
				self.uselib.insert(0, v)

		# if the library task generator provides 'export_incdirs', add to the include path
		# the export_incdirs must be a list of paths relative to the other library
		if getattr(y, 'export_incdirs', None):
			for x in self.to_list(y.export_incdirs):
				node = y.path.find_dir(x)
				if not node:
					raise Utils.WafError('object %r: invalid folder %r in export_incdirs' % (y.target, x))
				self.env.append_unique('INC_PATHS', node)

@feature('dprogram', 'dshlib', 'dstaticlib')
@after('apply_core')
def apply_d_link(self):
	link = getattr(self, 'link', None)
	if not link:
		if 'dstaticlib' in self.features: link = 'static_link'
		else: link = 'd_link'

	outputs = [t.outputs[0] for t in self.compiled_tasks]
	self.link_task = self.create_task(link, outputs, self.path.find_or_declare(get_target_name(self)))

@feature('d')
@after('apply_core')
def apply_d_vars(self):
	env = self.env
	dpath_st   = env['DPATH_ST']
	lib_st     = env['DLIB_ST']
	libpath_st = env['DLIBPATH_ST']

	importpaths = self.to_list(self.importpaths)
	libpaths = []
	libs = []
	uselib = self.to_list(self.uselib)

	for i in uselib:
		if env['DFLAGS_' + i]:
			env.append_unique('DFLAGS', env['DFLAGS_' + i])

	for x in self.features:
		if not x in ['dprogram', 'dstaticlib', 'dshlib']:
			continue
		x.lstrip('d')
		d_shlib_dflags = env['D_' + x + '_DFLAGS']
		if d_shlib_dflags:
			env.append_unique('DFLAGS', d_shlib_dflags)

	# add import paths
	for i in uselib:
		if env['DPATH_' + i]:
			for entry in self.to_list(env['DPATH_' + i]):
				if not entry in importpaths:
					importpaths.append(entry)

	# now process the import paths
	for path in importpaths:
		if os.path.isabs(path):
			env.append_unique('_DIMPORTFLAGS', dpath_st % path)
		else:
			node = self.path.find_dir(path)
			self.env.append_unique('INC_PATHS', node)
			env.append_unique('_DIMPORTFLAGS', dpath_st % node.srcpath(env))
			env.append_unique('_DIMPORTFLAGS', dpath_st % node.bldpath(env))

	# add library paths
	for i in uselib:
		if env['LIBPATH_' + i]:
			for entry in self.to_list(env['LIBPATH_' + i]):
				if not entry in libpaths:
					libpaths.append(entry)
	libpaths = self.to_list(self.libpaths) + libpaths

	# now process the library paths
	# apply same path manipulation as used with import paths
	for path in libpaths:
		env.append_unique('DLINKFLAGS', libpath_st % path)

	# add libraries
	for i in uselib:
		if env['LIB_' + i]:
			for entry in self.to_list(env['LIB_' + i]):
				if not entry in libs:
					libs.append(entry)
	libs.extend(self.to_list(self.libs))

	# process user flags
	for flag in self.to_list(self.dflags):
		env.append_unique('DFLAGS', flag)

	# now process the libraries
	for lib in libs:
		env.append_unique('DLINKFLAGS', lib_st % lib)

	# add linker flags
	for i in uselib:
		dlinkflags = env['DLINKFLAGS_' + i]
		if dlinkflags:
			for linkflag in dlinkflags:
				env.append_unique('DLINKFLAGS', linkflag)

@feature('dshlib')
@after('apply_d_vars')
def add_shlib_d_flags(self):
	for linkflag in self.env['D_shlib_LINKFLAGS']:
		self.env.append_unique('DLINKFLAGS', linkflag)

@extension(EXT_D)
def d_hook(self, node):
	# create the compilation task: cpp or cc
	task = self.create_task(self.generate_headers and 'd_with_header' or 'd')
	try: obj_ext = self.obj_ext
	except AttributeError: obj_ext = '_%d.o' % self.idx

	task.inputs = [node]
	task.outputs = [node.change_ext(obj_ext)]
	self.compiled_tasks.append(task)

	if self.generate_headers:
		header_node = node.change_ext(self.env['DHEADER_ext'])
		task.outputs += [header_node]

d_str = '${D_COMPILER} ${DFLAGS} ${_DIMPORTFLAGS} ${D_SRC_F}${SRC} ${D_TGT_F}${TGT}'
d_with_header_str = '${D_COMPILER} ${DFLAGS} ${_DIMPORTFLAGS} \
${D_HDR_F}${TGT[1].bldpath(env)} \
${D_SRC_F}${SRC} \
${D_TGT_F}${TGT[0].bldpath(env)}'
link_str = '${D_LINKER} ${DLNK_SRC_F}${SRC} ${DLNK_TGT_F}${TGT} ${DLINKFLAGS}'

def override_exec(cls):
	"""stupid dmd wants -of stuck to the file name"""
	old_exec = cls.exec_command
	def exec_command(self, *k, **kw):
		if isinstance(k[0], list):
			lst = k[0]
			for i in xrange(len(lst)):
				if lst[i] == '-of':
					del lst[i]
					lst[i] = '-of' + lst[i]
					break
		return old_exec(self, *k, **kw)
	cls.exec_command = exec_command

cls = Task.simple_task_type('d', d_str, 'GREEN', before='static_link d_link', shell=False)
cls.scan = scan
override_exec(cls)

cls = Task.simple_task_type('d_with_header', d_with_header_str, 'GREEN', before='static_link d_link', shell=False)
override_exec(cls)

cls = Task.simple_task_type('d_link', link_str, color='YELLOW', shell=False)
override_exec(cls)

# for feature request #104
@taskgen
def generate_header(self, filename, install_path):
	if not hasattr(self, 'header_lst'): self.header_lst = []
	self.meths.append('process_header')
	self.header_lst.append([filename, install_path])

@before('apply_core')
def process_header(self):
	env = self.env
	for i in getattr(self, 'header_lst', []):
		node = self.path.find_resource(i[0])

		if not node:
			raise Utils.WafError('file not found on d obj '+i[0])

		task = self.create_task('d_header')
		task.set_inputs(node)
		task.set_outputs(node.change_ext('.di'))

d_header_str = '${D_COMPILER} ${D_HEADER} ${SRC}'
Task.simple_task_type('d_header', d_header_str, color='BLUE', shell=False)

@conftest
def d_platform_flags(conf):
	v = conf.env
	binfmt = v.DEST_BINFMT or Utils.unversioned_sys_platform_to_binary_format(
		v.DEST_OS or Utils.unversioned_sys_platform())
	if binfmt == 'pe':
		v['D_program_PATTERN']   = '%s.exe'
		v['D_shlib_PATTERN']     = 'lib%s.dll'
		v['D_staticlib_PATTERN'] = 'lib%s.a'
	else:
		v['D_program_PATTERN']   = '%s'
		v['D_shlib_PATTERN']     = 'lib%s.so'
		v['D_staticlib_PATTERN'] = 'lib%s.a'

# quick test #
if __name__ == "__main__":
	#Logs.verbose = 2

	try: arg = sys.argv[1]
	except IndexError: arg = "file.d"

	print("".join(filter_comments(arg)))
	# TODO
	paths = ['.']

	#gruik = filter()
	#gruik.start(arg)

	#code = "".join(gruik.buf)

	#print "we have found the following code"
	#print code

	#print "now parsing"
	#print "-------------------------------------------"
	"""
	parser_ = d_parser()
	parser_.start(arg)

	print "module: %s" % parser_.module
	print "imports: ",
	for imp in parser_.imports:
		print imp + " ",
	print
"""


"""paintme URL Configuration

The `urlpatterns` list routes URLs to views. For more information please see:
    https://docs.djangoproject.com/en/3.1/topics/http/urls/
Examples:
Function views
    1. Add an import:  from my_app import views
    2. Add a URL to urlpatterns:  path('', views.home, name='home')
Class-based views
    1. Add an import:  from other_app.views import Home
    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')
Including another URLconf
    1. Import the include() function: from django.urls import include, path
    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))
"""
from django.contrib import admin
from django.urls import path
from backend import views

urlpatterns = [
    path('admin/', admin.site.urls),
    path('paintings/', views.random, name='random'),
    path('paintings/<str:artist>/<int:offset>', views.artist, name='artist'),
    path('paintme/', views.paintme, name='paintme')
]


v = '2.1'

try:
    import UEManifestReader
    import coloredlogs
    import aioconsole
    import webbrowser
    import subprocess
    import crayons
    import logging
    import asyncio
    import psutil
    import json
    import time
    import sys
    import os
except:
    print('It seems that some modules are missing. Run "INSTALL.bat" and try again.')
    input('Press ENTER to exit')

from os import kill
from modules import http

log = logging.getLogger('FortniteLauncher')

configuration = json.load(open('config.json', 'r', encoding = 'utf-8'))
auths = json.load(open('auths.json', 'r', encoding = 'utf-8'))

def get_colored_box(color, text):

    return f'{color("[")}{text}{color("]")}'

async def get_other_clients():

    log.debug('Looking for other running clients...')

    clients = []

    for p in psutil.process_iter(['name', 'pid']):
        if p.info['name'] == 'FortniteClient-Win64-Shipping.exe':
            clients.append(p.info['pid'])

    log.debug(f'Found {len(clients)} clients.')

    return clients

async def wait_for_game_spawn(process: psutil.Process, ignore: list):

    log.debug(f'Waiting for game to spawn...')

    while True:
        if process.is_running() == False:
            return False
        for p in psutil.process_iter(['name', 'pid']):
            if p.info['name'] == 'FortniteClient-Win64-Shipping.exe':
                if p.info['pid'] in ignore:
                    continue
                return True


async def add_account():

    log.debug('add_account flow started.')

    print()
    print(crayons.green('Add Account', bold=True))

    auth_type = configuration['auth_type']

    LAUNCHER_AUTHORIZATION_URL = 'https://www.epicgames.com/id/api/redirect?clientId=34a02cf8f4414e29b15921876da36f9a&responseType=code'
    LAUNCHER_AUTHORIZATION_URL_LOGIN = 'https://www.epicgames.com/id/logout?redirectUrl=https%3A%2F%2Fwww.epicgames.com%2Fid%2Flogin%3FredirectUrl%3Dhttps%253A%252F%252Fwww.epicgames.com%252Fid%252Fapi%252Fredirect%253FclientId%253D34a02cf8f4414e29b15921876da36f9a%2526responseType%253Dcode'

    IOS_AUTHORIZATION_URL = 'https://www.epicgames.com/id/api/redirect?clientId=3446cd72694c4a4485d81b77adbb2141&responseType=code'
    IOS_AUTHORIZATION_URL_LOGIN = 'https://www.epicgames.com/id/logout?redirectUrl=https%3A%2F%2Fwww.epicgames.com%2Fid%2Flogin%3FredirectUrl%3Dhttps%253A%252F%252Fwww.epicgames.com%252Fid%252Fapi%252Fredirect%253FclientId%253D3446cd72694c4a4485d81b77adbb2141%2526responseType%253Dcode'

    while True:
        user_selection = await aioconsole.ainput(f'Are you logged in to the required account in your web browser?\nType {crayons.white("1", bold=True)} if yes.\nType {crayons.white("2", bold=True)} if no.\n')

        user_logged = user_selection.strip(' ')

        if user_logged == '1':

            if auth_type == 'refresh':

                choosen_url = LAUNCHER_AUTHORIZATION_URL
            else:
                choosen_url = IOS_AUTHORIZATION_URL

        elif user_logged == '2':

            if auth_type == 'refresh':
                choosen_url = LAUNCHER_AUTHORIZATION_URL_LOGIN
            else:
                choosen_url = IOS_AUTHORIZATION_URL_LOGIN

        else:

            print('Select a valid option! Try again\n')

            continue
        break

    webbrowser.open_new_tab(choosen_url)

    print(choosen_url)
    if user_logged == '1':
        print('An epic games page should be opened in your web brower. Paste the authorizationCode here:')
    else:
        print('An epic games page should be opened in your web brower. Login on the required account and then paste the authorizationCode here:')
    
    user_code = await aioconsole.ainput('> ')

    code = user_code.strip(' ')

    if code in ['cancel', 'c']:
        log.debug('add_account flow stopped. User cancelled')
        print('Account add cancelled')
        return False

    if len(code) != 32:
        log.debug('add_account flow stopped. The code from the user was invalid.')
        print(f'Failed account add. The code\'s lenght is invalid. A valid authorization code is 32 characters long.')
        return False

    Auth = http.EpicAPI()

    if auth_type == 'refresh':

        auth_request = await Auth.authorization_code_auth(code)

        if 'errorCode' not in auth_request.text:

            oauth_json = auth_request.json()

            credentials = {}

            credentials['auth_type'] = 'refresh'
            credentials['refresh_token'] = str(oauth_json['refresh_token'])
            credentials['refresh_expires'] = int(time.time()) + oauth_json['refresh_expires']

            auths[oauth_json['displayName']] = credentials

            with open('auths.json', 'w', encoding='utf-8') as f:
                json.dump(auths, f, indent=4, ensure_ascii=False)

            log.debug('add_account flow completed without errors.')

            return f'Account "{oauth_json["displayName"]}" added successfully! (Note: this login will expire after 23 days of inactivity)'

        else:
            print(f'Authentication failed. {auth_request.json()["errorMessage"]}')
            log.debug('add_account flow stopped. The authentication failed.')
            return False
    
    elif auth_type == 'device':

        auth_request = await Auth.authorization_code_auth(code, client = http.Clients.fortniteIOSGameClient)

        if 'errorCode' not in auth_request.text:

            oauth_json = auth_request.json()

            device_create = await Auth.create_device_auth(oauth_json['access_token'], oauth_json['account_id'])

            if 'errorCode' not in device_create.text:

                device_json = device_create.json()

                credentials = {}
                
                credentials['auth_type'] = 'device'
                credentials['account_id'] = device_json['accountId']
                credentials['device_id'] = device_json['deviceId']
                credentials['secret'] = device_json['secret']


                auths[oauth_json['displayName']] = credentials

                with open('auths.json', 'w', encoding='utf-8') as f:
                    json.dump(auths, f, indent=4, ensure_ascii=False)

                await Auth.kill_oauth_session(oauth_json['access_token'])

                return f'Account "{oauth_json["displayName"]}" added successfully!'

            else:
                print(f'Device auth creation failed. {auth_request.json()["errorMessage"]}')
                log.debug('add_account flow stopped. The authentication failed.')
                return False

        else:
            print(f'Authentication failed. {auth_request.json()["errorMessage"]}')
            log.debug('add_account flow stopped. The authentication failed.')
            return False


async def remove_account():

    log.debug('remove_account flow started.')

    print()
    print(crayons.red('Remove Account', bold=True))

    while True:

        account_list = list(auths.keys())
        countlist = []
        count = 0

        for account in account_list:
            count += 1
            countlist.append(count)
            print(f'{get_colored_box(crayons.red, str(count))} {account}')

        print(f'{get_colored_box(crayons.green, "C")} Cancel\n')


        user_selection = await aioconsole.ainput(f'Select an account: ')

        try:
            user_selection.strip(' ')

            if user_selection.lower() in ['c', 'cancel']:
                print(crayons.red('Account remove cancelled.'))
                log.debug('remove_account flow cancelled by user.')
                return False

            if int(user_selection) not in countlist:
                print(crayons.red('Invalid selection\n'))
                continue

            else:
                break
        except:
            print(crayons.red('Select a valid option\n'))
            continue

    credentials = auths[account_list[int(user_selection) - 1]]

    if credentials['auth_type'] == 'refresh':

        if int(time.time()) > credentials['refresh_expires']:

            del auths[account_list[int(user_selection) - 1]]

            with open('auths.json', 'w', encoding='utf-8') as f:
                json.dump(auths, f, indent=4, ensure_ascii=False)

            log.debug('remove_account flow completed. The saved refresh wasn\'t valid and removed from auths.json file')
            print('Account removed successfully.')
            return True
        
        else:

            Auth = http.EpicAPI()
            auth_request = await Auth.refresh_token_auth(refresh_token = credentials['refresh_token'])

            if 'errorCode' not in auth_request.text:

                oauth_json = auth_request.json()
                
                kill_request = await Auth.kill_oauth_session(oauth_json['access_token'])

                if kill_request not in [401, 403]:

                    del auths[account_list[int(user_selection) - 1]]

                    with open('auths.json', 'w', encoding='utf-8') as f:
                        json.dump(auths, f, indent=4, ensure_ascii=False)

                    log.debug('remove_account flow completed without errors')
                    print('Account removed successfully.')
                    return True

            else:

                print(f'Authentication failed. {auth_request.json()["errorMessage"]}')
                print('Removing account from auths.json file anyway.')

                del auths[account_list[int(user_selection) - 1]]

                with open('auths.json', 'w', encoding='utf-8') as f:
                    json.dump(auths, f, indent=4, ensure_ascii=False)

                log.debug('remove_account flow failed successfully. Authentication failed but removed from auths.json anyways')

                print('Account removed.') # task failed successfully
                return True

    elif credentials['auth_type'] == 'device':

        Auth = http.EpicAPI()

        auth_request = await Auth.device_auths_auth(credentials)

        if 'errorCode' not in auth_request.text:

            oauth_json = auth_request.json()

            kill_device = await Auth.delete_device_auth(oauth_json['access_token'], account_id=credentials['account_id'], device_id=credentials['device_id'])

            if kill_device.status_code not in [401, 403]:

                del auths[account_list[int(user_selection) - 1]]

                with open('auths.json', 'w', encoding='utf-8') as f:
                    json.dump(auths, f, indent=4, ensure_ascii=False)

                await Auth.kill_oauth_session(oauth_json['access_token'])

                log.debug('remove_account flow completed without errors')
                print('Account removed successfully.')
                return True

            else:

                print(f'Device auth delete failed. {kill_device.json()["errorMessage"]}')
                print('Removing account from auths.json anyway. Change the account password to make sure you kill the device auth.')

                del auths[account_list[int(user_selection) - 1]]

                with open('auths.json', 'w', encoding='utf-8') as f:
                    json.dump(auths, f, indent=4, ensure_ascii=False)

                log.debug('remove_account flow failed successfully. Device delete failed but removed from auths.json anyways')

                await Auth.kill_oauth_session(oauth_json['access_token'])

                print('Account removed.') # task failed successfully
                return True

        else:

                print(f'Authentication failed. {auth_request.json()["errorMessage"]}')
                print('Removing account from auths.json anyway.')

                del auths[account_list[int(user_selection) - 1]]

                with open('auths.json', 'w', encoding='utf-8') as f:
                    json.dump(auths, f, indent=4, ensure_ascii=False)

                log.debug('remove_account flow failed successfully. Authentication failed but removed from auths.json anyways')

                print('Account removed.') # task failed successfully
                return True

    
async def launch_game(exchange_code: str, launch_command: str):

    log.debug('Launching game...')

    fortnite_path = configuration['fortnite_path']
    executable_args = launch_command
    additional_args = configuration["commandline_arguments"]

    log.debug('Preparing command line arguments.')

    args = [
        executable_args,
        '-AUTH_LOGIN=unused',
        f'-AUTH_PASSWORD={exchange_code}',
        '-AUTH_TYPE=exchangecode',
        '-epicapp=Fortnite',
        '-epicenv=Prod',
        '-EpicPortal',
    ]

    for i in additional_args:
        if i.startswith('-'):
            args.append(i)

    ignore_list = await get_other_clients()

    log.debug(f'Starting FortniteLauncher.exe with args {args}...')

    FortniteLauncher = subprocess.Popen([f'{fortnite_path}/FortniteGame/Binaries/Win64/FortniteLauncher.exe'] + args, cwd=f'{fortnite_path}/FortniteGame/Binaries/Win64/', stdout=subprocess.DEVNULL)
    process = psutil.Process(pid = FortniteLauncher.pid)

    wait_spawn = await wait_for_game_spawn(process, ignore_list)

    if wait_spawn == True:

        log.debug('Game launched correctly.')
        return True

    else:

        log.debug('Game did\'nt launch.')
        return False


async def start():

    if '--debug' in sys.argv:
        coloredlogs.install(
            level='DEBUG'
        )

    while True:

        print()

        print(f'\n{crayons.cyan("Fortnite Launcher", bold=True)} | {crayons.white(f"Beta v{v}", bold=True)}\n')

        try:
            configuration = json.load(open('config.json', 'r', encoding = 'utf-8'))

            if configuration['auth_type'] not in ['refresh', 'device']:
                print('Error, the choosen auth type in configuration file isn\'t valid. Auth type must be "refresh" or "device".')
                await aioconsole.ainput('Press ENTER to exit')
                exit()

        except Exception as e:
            print(f'An error ocurred loading config.json file. {e}')
            await aioconsole.ainput('Press ENTER to exit')
            exit()

        try:
            auths = json.load(open('auths.json', 'r', encoding = 'utf-8'))
        except Exception as e:
            print(f'An error ocurred loading auths.json file. {e}')
            await aioconsole.ainput('Press ENTER to exit')
            exit()

        account_list = list(auths.keys())
        countlist = []
        count = 0

        for account in account_list:
            count += 1
            countlist.append(count)
            print(f'{get_colored_box(crayons.green, str(count))} {account}')

        print(f'\n{get_colored_box(crayons.blue, "A")} Add an account')
        print(f'{get_colored_box(crayons.blue, "R")} Remove an account\n')
        print(f'{get_colored_box(crayons.red, "X")} Exit\n')

        user_selection = await aioconsole.ainput(f'Select an option: ')

        try:
            user_selection.strip(' ')

            if user_selection.lower() == 'x':
                exit()

            if user_selection.lower() == 'a':
                add = await add_account()
                if isinstance(add, str):
                    print(add)
                continue

            if user_selection.lower() == 'r':
                if len(account_list) == 0:
                    print('There is no accounts to remove!\n')
                    continue
                
                else:
                    await remove_account()
                    continue

            if int(user_selection) not in countlist:
                print(crayons.red('Invalid selection\n'))
                continue

        except:
            print(crayons.red('Select a valid option\n'))
            continue

        selected_account = int(user_selection) - 1

        game_folder = configuration['fortnite_path']

        if os.path.isdir(game_folder) == False:
            print('Seems like the fortnite path in configuration is not valid. Check it and try again')
            await aioconsole.ainput('Press ENTER to exit')

        else:

            credentials = auths[account_list[selected_account]]

            auth_type = credentials['auth_type']

            if auth_type == 'refresh':

                if int(time.time()) > credentials['refresh_expires']:
                    print('The credentials of this account have expired. Re-add the account and try again')

                Auth = http.EpicAPI()
                auth_request = await Auth.refresh_token_auth(refresh_token = credentials['refresh_token'])

                if 'errorCode' not in auth_request.text:

                    oauth_json = auth_request.json()

                    credentials['refresh_token'] = str(oauth_json['refresh_token'])
                    credentials['refresh_expires'] = int(time.time()) + oauth_json['refresh_expires']

                    auths[account_list[selected_account]] = credentials

                    with open('auths.json', 'w', encoding='utf-8') as f:
                        json.dump(auths, f, indent=4, ensure_ascii=False)

                    exchange_request = await Auth.get_exchange_code(oauth_json['access_token'])

                    if 'errorCode' not in exchange_request.text:

                        exchange_json = exchange_request.json()
                        launch_command = ''

                        launch_info = await Auth.get_launch_info()
                        if launch_info.status_code == 200:
                            log.debug('Using baydev api launch args.')
                            launch_command = launch_info.json()['data']['launch_args']
                            log.debug(f'Launch args for build {launch_info.json()["data"]["build"]}')

                        else:
                            log.debug('Using epicgames manifest launch args.')
                            Reader = UEManifestReader.UEManifestReader()
                            manifest = await Reader.download_manifest()
                            launch_command = manifest['LaunchCommand']

                        print('Launching...')

                        launch_try = await launch_game(exchange_json['code'], launch_command)

                        if launch_try == False:
                            print('Failed game launch.')
                            await asyncio.sleep(2)
                            continue

                        else:

                            print('Launched.')
                            await asyncio.sleep(3)
                            exit()

                    else:
                        print(f'Exchange code request failed. {exchange_request.json()["errorMessage"]}')
                        continue

                else:
                    print(f'Authentication failed. {auth_request.json()["errorMessage"]}')
                    continue

            else:

                Auth = http.EpicAPI()
                auth_request = await Auth.device_auths_auth(credentials)

                if 'errorCode' not in auth_request.text:

                    oauth_json = auth_request.json()

                    exchange_request = await Auth.get_exchange_code(oauth_json['access_token'])

                    if 'errorCode' not in exchange_request.text:

                        exchange_auth = exchange_request.json()

                        launcher_auth_request = await Auth.exchange_code_auth(exchange_auth['code'])

                        if 'errorCode' not in launcher_auth_request.text:

                            launcher_auth = launcher_auth_request.json()

                            launcher_exchange_request = await Auth.get_exchange_code(launcher_auth['access_token'])

                            if 'errorCode' not in launcher_exchange_request.text:

                                final_exchange_json = launcher_exchange_request.json()

                                launch_command = ''

                                launch_info = await Auth.get_launch_info()
                                if launch_info.status_code == 200:
                                    log.debug('Using baydev api launch args.')
                                    launch_command = launch_info.json()['data']['launch_args']

                                else:
                                    print(f'Baydev api returned status {launch_info.status_code}. Downloading and parsing manifest manually. (This may take a while)')
                                    log.debug('Using epicgames manifest launch args. (This may take a while)')
                                    Reader = UEManifestReader.UEManifestReader()
                                    manifest = await Reader.download_manifest()
                                    launch_command = manifest['LaunchCommand']

                                print('Launching...')

                                launch_try = await launch_game(final_exchange_json['code'], launch_command)

                                if launch_try == False:
                                    print('Failed game launch.')
                                    await Auth.kill_oauth_session(oauth_json['access_token'])
                                    await Auth.kill_oauth_session(launcher_auth['access_token'])
                                    await asyncio.sleep(2)
                                    continue

                                else:

                                    print('Launched.')
                                    await asyncio.sleep(3)
                                    await Auth.kill_oauth_session(oauth_json['access_token'])
                                    await Auth.kill_oauth_session(launcher_auth['access_token'])
                                    exit()
                            
                            else:
                                print(f'Launcher exchange code generate failed. {launcher_exchange_request.json()["errorMessage"]}')
                                await Auth.kill_oauth_session(oauth_json['access_token'])
                                await Auth.kill_oauth_session(launcher_auth['access_token'])
                                continue

                        else:
                            print(f'Launcher exchange auth failed. {launcher_auth_request.json()["errorMessage"]}')
                            await Auth.kill_oauth_session(oauth_json['access_token'])
                            continue

                    else:
                        print(f'Exchange code request failed. {exchange_request.json()["errorMessage"]}')
                        await Auth.kill_oauth_session(oauth_json['access_token'])
                        continue
                
                else:
                    print(f'Authentication failed. {auth_request.json()["errorMessage"]}')
                    continue



if __name__ == '__main__':

    loop = asyncio.get_event_loop()
    loop.run_until_complete(start())
import pymysql as pymysql
a = None
user_data = {}
def mysql_db(name,age):
	conn = pymysql.connect(host='127.0.0.1', user='root', password='mysql', database='zhangqiang_db', charset='utf8')
	cursor = conn.cursor()
	sql = '''
	insert into USER1(name, age) value(%s, %s);
	'''
	# name = name
	# age = age
	cursor.execute(sql, [name, age])
	conn.commit()
	cursor.close()
	conn.close()


def serach_db(name):
	conn = pymysql.connect(host='127.0.0.1', user='root', password='mysql', database='zhangqiang_db', charset='utf8')
	cursor = conn.cursor()
	sql = '''
	select name from USER1 where name=%s;
	'''
	# name = name
	cursor.execute(sql, [name])
	a = cursor.fetchone()
	cursor.close()
	conn.close()
	print(a)
	return a

def serach_db_passwd(name):
	conn = pymysql.connect(host='127.0.0.1', user='root', password='mysql', database='zhangqiang_db', charset='utf8')
	cursor = conn.cursor()
	sql = '''
	select age from USER1 where age=%s;
	'''
	# name = name
	cursor.execute(sql, [name])
	passwd = cursor.fetchone()
	cursor.close()
	conn.close()
	print(passwd)
	return passwd

def new_user():
	name = input('new_name\t\n')
	while 1:

		if name in user_data:
			name = input('The user name already exists')
			continue
		else:
			break
	passwd = int(input('password\n'))
	mysql_db(name, passwd)
	# user_data[name] = passwd
	print('successful')


def old_user():

	name = input('name')
	while 1:
		username = serach_db(name)
		# print(serach_db(name))
		username = username[0]
		# print(a)

		# if name not in user_data:
		if username == name:
			print('name ok')
			password = input('password\n	')

			passwd = serach_db_passwd(password)
			passwd = str(passwd[0])
			if passwd == password:

				print('')


			name = input('name is error,please enter again')
			continue
		else:
			break
	pwd= user_data.get(name)
	if passwd == pwd:
		print('successful')
		
	else:
		print('password is error')
def showmenu():
	name = ''
	print('---new_user:N/n--')
	print('---login:E/e------')
	print('---quit:Q/q------')
	print('---input code----')
	choose=input('input code\n')	
	while 1 :
		if choose not in 'NEQneq':
			choose = input('code is error,try again')
		elif choose == 'q' or choose == 'Q':
			break
		elif  choose == 'N' or choose =='n':
			new_user()
			showmenu()
		elif  choose =='e' or choose == 'E':
			old_user()
showmenu()
# if serach_db('laowang') == ('laowang',):
# 	print('1')
# else:
# 	print('2')
#


		

# Auto generated configuration file
# using: 
# Revision: 1.19 
# Source: /local/reps/CMSSW/CMSSW/Configuration/Applications/python/ConfigBuilder.py,v 
# with command line options: l1Ntuple -s RAW2DIGI --python_filename=mc_L1TReEmulMCFromRAW_L1NtupleEMU.py -n 2 --no_output --era=Run3 --mc --conditions=112X_mcRun3_2021_realistic_v13 --customise=L1Trigger/Configuration/customiseReEmul.L1TReEmulMCFromRAW --customise=L1Trigger/L1TNtuples/customiseL1Ntuple.L1NtupleEMU --customise=L1Trigger/Configuration/customiseSettings.L1TSettingsToCaloParams_2018_v1_4 --filein=/store/mc/Run3Winter20DRPremixMiniAOD/TT_TuneCP5_14TeV-powheg-pythia8/GEN-SIM-RAW/110X_mcRun3_2021_realistic_v6-v2/20000/CFCAE998-5A17-FB48-A36F-A31EA28D2A72.root
import FWCore.ParameterSet.Config as cms

from Configuration.Eras.Era_Run3_cff import Run3

process = cms.Process('RAW2DIGI',Run3)

# import of standard configurations
process.load('Configuration.StandardSequences.Services_cff')
process.load('SimGeneral.HepPDTESSource.pythiapdt_cfi')
process.load('FWCore.MessageService.MessageLogger_cfi')
process.load('Configuration.EventContent.EventContent_cff')
process.load('SimGeneral.MixingModule.mixNoPU_cfi')
process.load('Configuration.StandardSequences.GeometryRecoDB_cff')
process.load('Configuration.StandardSequences.MagneticField_cff')
process.load('Configuration.StandardSequences.RawToDigi_cff')
process.load('Configuration.StandardSequences.EndOfProcess_cff')
process.load('Configuration.StandardSequences.FrontierConditions_GlobalTag_cff')

process.maxEvents = cms.untracked.PSet(
    input = cms.untracked.int32(10),
    output = cms.optional.untracked.allowed(cms.int32,cms.PSet)
)

# Input source
process.source = cms.Source("PoolSource",
    fileNames = cms.untracked.vstring('/store/mc/Run3Winter20DRPremixMiniAOD/TT_TuneCP5_14TeV-powheg-pythia8/GEN-SIM-RAW/110X_mcRun3_2021_realistic_v6-v2/20000/CFCAE998-5A17-FB48-A36F-A31EA28D2A72.root'),
    secondaryFileNames = cms.untracked.vstring()
)

process.options = cms.untracked.PSet(
    FailPath = cms.untracked.vstring(),
    IgnoreCompletely = cms.untracked.vstring(),
    Rethrow = cms.untracked.vstring(),
    SkipEvent = cms.untracked.vstring(),
    allowUnscheduled = cms.obsolete.untracked.bool,
    canDeleteEarly = cms.untracked.vstring(),
    emptyRunLumiMode = cms.obsolete.untracked.string,
    eventSetup = cms.untracked.PSet(
        forceNumberOfConcurrentIOVs = cms.untracked.PSet(
            allowAnyLabel_=cms.required.untracked.uint32
        ),
        numberOfConcurrentIOVs = cms.untracked.uint32(1)
    ),
    fileMode = cms.untracked.string('FULLMERGE'),
    forceEventSetupCacheClearOnNewRun = cms.untracked.bool(False),
    makeTriggerResults = cms.obsolete.untracked.bool,
    numberOfConcurrentLuminosityBlocks = cms.untracked.uint32(1),
    numberOfConcurrentRuns = cms.untracked.uint32(1),
    numberOfStreams = cms.untracked.uint32(0),
    numberOfThreads = cms.untracked.uint32(1),
    printDependencies = cms.untracked.bool(False),
    sizeOfStackForThreadsInKB = cms.optional.untracked.uint32,
    throwIfIllegalParameter = cms.untracked.bool(True),
    wantSummary = cms.untracked.bool(False)
)

# Production Info
process.configurationMetadata = cms.untracked.PSet(
    annotation = cms.untracked.string('l1Ntuple nevts:2'),
    name = cms.untracked.string('Applications'),
    version = cms.untracked.string('$Revision: 1.19 $')
)

# Output definition

# Additional output definition

# Other statements
from Configuration.AlCa.GlobalTag import GlobalTag
process.GlobalTag = GlobalTag(process.GlobalTag, '112X_mcRun3_2021_realistic_v13', '')

process.GlobalTag.toGet = cms.VPSet(
        cms.PSet(record = cms.string("GEMeMapRcd"),
                       tag = cms.string("GEMeMapDummy"),
                       connect = cms.string("sqlite_file:L1Trigger/Configuration/test/GEMeMapDummy.db")
                )
)
process.muonGEMDigis.useDBEMap = True

# Path and EndPath definitions
process.raw2digi_step = cms.Path(process.RawToDigi)
process.endjob_step = cms.EndPath(process.endOfProcess)

# Schedule definition
process.schedule = cms.Schedule(process.raw2digi_step,process.endjob_step)
from PhysicsTools.PatAlgos.tools.helpers import associatePatAlgosToolsTask
associatePatAlgosToolsTask(process)

# customisation of the process.

# Automatic addition of the customisation function from L1Trigger.Configuration.customiseReEmul
from L1Trigger.Configuration.customiseReEmul import L1TReEmulMCFromRAW 

#call to customisation function L1TReEmulMCFromRAW imported from L1Trigger.Configuration.customiseReEmul
process = L1TReEmulMCFromRAW(process)

# Automatic addition of the customisation function from L1Trigger.L1TNtuples.customiseL1Ntuple
from L1Trigger.L1TNtuples.customiseL1Ntuple import L1NtupleEMU 

#call to customisation function L1NtupleEMU imported from L1Trigger.L1TNtuples.customiseL1Ntuple
process = L1NtupleEMU(process)

# Automatic addition of the customisation function from L1Trigger.Configuration.customiseSettings
from L1Trigger.Configuration.customiseSettings import L1TSettingsToCaloParams_2018_v1_4 

#call to customisation function L1TSettingsToCaloParams_2018_v1_4 imported from L1Trigger.Configuration.customiseSettings
process = L1TSettingsToCaloParams_2018_v1_4(process)

# End of customisation functions


# Customisation from command line

# Add early deletion of temporary data products to reduce peak memory need
from Configuration.StandardSequences.earlyDeleteSettings_cff import customiseEarlyDelete
process = customiseEarlyDelete(process)
# End adding early deletion

i = 0  # This is an example of the while loop
while i < 5:
    print(i)
    i += 1
i = None  # 0
            # 1
            # 2
            # 3
            # 4



for i in range(5):  # This example uses the iterable object 'range'
    print(i)  # 0
                # 1
                # 2
                # 3
                # 4



for i in [1, 2, 3, 4]:
    print(i)  # 1
                # 2
                # 3
                # 4



for c in 'hello':
    print(c)  # h
                # e
                # l
                # l
                # o



for x in ('a',"b", 'c', 4):
    print(x)  # a
                # b
                # c
                # 4

for x in [(1, 2), (3, 4), (5, 6)]:
    print(x)  # (1, 2)
            # (3, 4)
            # (5, 6)



for i in range(5):
    if i==3:
        continue
    print(i)  # 0
                # 1
                # 2
                # 4







for i in range(5):
    if i==3:
        break
    print(i)  # 0
                # 1
                # 2



for i in range(1, 5):
    print(i)
    if i % 7 == 0:
        print('multiple of 7 found')
        break
else:
    print('no multiple of 7 found')  # 1
                                    # 2
                                    # 3
                                    # 4
                                    # no multiple of 7 found



for i in range(1, 8):
    print(i)
    if i % 7 == 0:
        print('multiple of 7 found')
        break
else:
    print('no multiple of 7 found')  # 1
                                    # 2
                                    # 3
                                    # 4
                                    # 5
                                    # 6
                                    # 7
                                    # multiple of 7 found



for i in range(6):
    print('------------------')
    

from nose.plugins.attrib import attr
from test.integration.base import DBTIntegrationTest


class TestMacros(DBTIntegrationTest):

    def setUp(self):
        DBTIntegrationTest.setUp(self)
        self.run_sql_file("test/integration/016_macro_tests/seed.sql")

    @property
    def schema(self):
        return "test_macros_016"

    @property
    def models(self):
        return "test/integration/016_macro_tests/models"

    @property
    def project_config(self):
        return {
            "models": {
                "vars": {
                    "test": "DUMMY"
                }
            },
            "macro-paths": ["test/integration/016_macro_tests/macros"],
            "repositories": [
                'https://github.com/fishtown-analytics/dbt-integration-project'
            ]
        }

    @attr(type='postgres')
    def test_working_macros(self):
        self.run_dbt(["deps"])
        self.run_dbt(["run"])

        self.assertTablesEqual("expected_dep_macro", "dep_macro")
        self.assertTablesEqual("expected_local_macro", "local_macro")


class TestInvalidMacros(DBTIntegrationTest):

    def setUp(self):
        DBTIntegrationTest.setUp(self)

    @property
    def schema(self):
        return "test_macros_016"

    @property
    def models(self):
        return "test/integration/016_macro_tests/models"

    @property
    def project_config(self):
        return {
            "macro-paths": ["test/integration/016_macro_tests/bad-macros"]
        }

    @attr(type='postgres')
    def test_invalid_macro(self):

        try:
            self.run_dbt(["run"], expect_pass=False)
            self.assertTrue(False,
                            'compiling bad macro should raise a runtime error')

        except RuntimeError:
            pass


class TestMisusedMacros(DBTIntegrationTest):

    def setUp(self):
        DBTIntegrationTest.setUp(self)

    @property
    def schema(self):
        return "test_macros_016"

    @property
    def models(self):
        return "test/integration/016_macro_tests/bad-models"

    @property
    def project_config(self):
        return {
            "macro-paths": ["test/integration/016_macro_tests/macros"],
            "repositories": [
                'https://github.com/fishtown-analytics/dbt-integration-project'
            ]
        }

    # TODO: compilation no longer exists, so while the model calling this macro
    # fails, it does not raise a runtime exception. change this test to verify
    # that the model finished with ERROR state.
    #
    # @attr(type='postgres')
    # def test_working_macros(self):
    #     self.run_dbt(["deps"])

    #     try:
    #         self.run_dbt(["run"])
    #         self.assertTrue(False, 'invoked a package macro from global scope')
    #     except RuntimeError:
    #         pass

# coding: utf-8

"""
    Buy Marketing API

    The Marketing API retrieves eBay products based on a metric, such as Best Selling, as well as products that were also bought and also viewed.  # noqa: E501

    OpenAPI spec version: v1_beta.1.0
    
    Generated by: https://github.com/swagger-api/swagger-codegen.git
"""

import pprint
import re  # noqa: F401

import six

class Image(object):
    """NOTE: This class is auto generated by the swagger code generator program.

    Do not edit the class manually.
    """
    """
    Attributes:
      swagger_types (dict): The key is attribute name
                            and the value is attribute type.
      attribute_map (dict): The key is attribute name
                            and the value is json key in definition.
    """
    swagger_types = {
        'height': 'int',
        'image_url': 'str',
        'width': 'int'
    }

    attribute_map = {
        'height': 'height',
        'image_url': 'imageUrl',
        'width': 'width'
    }

    def __init__(self, height=None, image_url=None, width=None):  # noqa: E501
        """Image - a model defined in Swagger"""  # noqa: E501
        self._height = None
        self._image_url = None
        self._width = None
        self.discriminator = None
        if height is not None:
            self.height = height
        if image_url is not None:
            self.image_url = image_url
        if width is not None:
            self.width = width

    @property
    def height(self):
        """Gets the height of this Image.  # noqa: E501

        Reserved for future use.  # noqa: E501

        :return: The height of this Image.  # noqa: E501
        :rtype: int
        """
        return self._height

    @height.setter
    def height(self, height):
        """Sets the height of this Image.

        Reserved for future use.  # noqa: E501

        :param height: The height of this Image.  # noqa: E501
        :type: int
        """

        self._height = height

    @property
    def image_url(self):
        """Gets the image_url of this Image.  # noqa: E501

        The URL of the image.  # noqa: E501

        :return: The image_url of this Image.  # noqa: E501
        :rtype: str
        """
        return self._image_url

    @image_url.setter
    def image_url(self, image_url):
        """Sets the image_url of this Image.

        The URL of the image.  # noqa: E501

        :param image_url: The image_url of this Image.  # noqa: E501
        :type: str
        """

        self._image_url = image_url

    @property
    def width(self):
        """Gets the width of this Image.  # noqa: E501

        Reserved for future use.  # noqa: E501

        :return: The width of this Image.  # noqa: E501
        :rtype: int
        """
        return self._width

    @width.setter
    def width(self, width):
        """Sets the width of this Image.

        Reserved for future use.  # noqa: E501

        :param width: The width of this Image.  # noqa: E501
        :type: int
        """

        self._width = width

    def to_dict(self):
        """Returns the model properties as a dict"""
        result = {}

        for attr, _ in six.iteritems(self.swagger_types):
            value = getattr(self, attr)
            if isinstance(value, list):
                result[attr] = list(map(
                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
                    value
                ))
            elif hasattr(value, "to_dict"):
                result[attr] = value.to_dict()
            elif isinstance(value, dict):
                result[attr] = dict(map(
                    lambda item: (item[0], item[1].to_dict())
                    if hasattr(item[1], "to_dict") else item,
                    value.items()
                ))
            else:
                result[attr] = value
        if issubclass(Image, dict):
            for key, value in self.items():
                result[key] = value

        return result

    def to_str(self):
        """Returns the string representation of the model"""
        return pprint.pformat(self.to_dict())

    def __repr__(self):
        """For `print` and `pprint`"""
        return self.to_str()

    def __eq__(self, other):
        """Returns true if both objects are equal"""
        if not isinstance(other, Image):
            return False

        return self.__dict__ == other.__dict__

    def __ne__(self, other):
        """Returns true if both objects are not equal"""
        return not self == other

# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import csv
import io
import urllib.request

output_columns = [
    'Date', 'GeoId', 'CumulativeCount_MedicalTest_ConditionCOVID_19',
    'CumulativeCount_MedicalTest_ConditionCOVID_19_Positive',
    'CumulativeCount_MedicalTest_ConditionCOVID_19_Negative',
    'Count_MedicalTest_ConditionCOVID_19_Pending',
    'CumulativeCount_MedicalConditionIncident_COVID_19_PatientRecovered',
    'CumulativeCount_MedicalConditionIncident_COVID_19_PatientDeceased',
    'Count_MedicalConditionIncident_COVID_19_PatientHospitalized',
    'CumulativeCount_MedicalConditionIncident_COVID_19_PatientHospitalized',
    'Count_MedicalConditionIncident_COVID_19_PatientInICU',
    'CumulativeCount_MedicalConditionIncident_COVID_19_PatientInICU',
    'Count_MedicalConditionIncident_COVID_19_PatientOnVentilator',
    'CumulativeCount_MedicalConditionIncident_COVID_19_PatientOnVentilator'
]
with open('COVIDTracking_States.csv', 'w', newline='') as f_out:
    writer = csv.DictWriter(f_out,
                            fieldnames=output_columns,
                            lineterminator='\n')
    with urllib.request.urlopen(
            'https://covidtracking.com/api/v1/states/daily.csv') as response:
        reader = csv.DictReader(io.TextIOWrapper(response))

        writer.writeheader()
        for row_dict in reader:
            processed_dict = {
                'Date':
                    '%s-%s-%s' % (row_dict['date'][:4], row_dict['date'][4:6],
                                  row_dict['date'][6:]),
                'GeoId':
                    'dcid:geoId/%s' % row_dict['fips'],
                'CumulativeCount_MedicalTest_ConditionCOVID_19':
                    row_dict['totalTestResults'],
                'CumulativeCount_MedicalTest_ConditionCOVID_19_Positive':
                    row_dict['positive'],
                'CumulativeCount_MedicalTest_ConditionCOVID_19_Negative':
                    row_dict['negative'],
                'Count_MedicalTest_ConditionCOVID_19_Pending':
                    row_dict['pending'],
                ('CumulativeCount_MedicalConditionIncident'
                 '_COVID_19_PatientRecovered'):
                    row_dict['recovered'],
                ('CumulativeCount_MedicalConditionIncident'
                 '_COVID_19_PatientDeceased'):
                    row_dict['death'],
                'Count_MedicalConditionIncident_COVID_19_PatientHospitalized':
                    row_dict['hospitalizedCurrently'],
                ('CumulativeCount_MedicalConditionIncident'
                 '_COVID_19_PatientHospitalized'):
                    row_dict['hospitalizedCumulative'],
                'Count_MedicalConditionIncident_COVID_19_PatientInICU':
                    row_dict['inIcuCurrently'],
                ('CumulativeCount_MedicalConditionIncident'
                 '_COVID_19_PatientInICU'):
                    row_dict['inIcuCumulative'],
                'Count_MedicalConditionIncident_COVID_19_PatientOnVentilator':
                    row_dict['onVentilatorCurrently'],
                ('CumulativeCount_MedicalConditionIncident'
                 '_COVID_19_PatientOnVentilator'):
                    row_dict['onVentilatorCumulative'],
            }

            writer.writerow(processed_dict)

# Automate Template MCF generation since there are many Statitical Variables.
TEMPLATE_MCF_TEMPLATE = """
Node: E:COVIDTracking_States->E{index}
typeOf: dcs:StatVarObservation
variableMeasured: dcs:{stat_var}
measurementMethod: dcs:CovidTrackingProject
observationAbout: C:COVIDTracking_States->GeoId
observationDate: C:COVIDTracking_States->Date
value: C:COVIDTracking_States->{stat_var}
"""

stat_vars = output_columns[2:]
with open('COVIDTracking_States.tmcf', 'w', newline='') as f_out:
    for i in range(len(stat_vars)):
        f_out.write(
            TEMPLATE_MCF_TEMPLATE.format_map({
                'index': i,
                'stat_var': output_columns[2:][i]
            }))

##########################################################################
#
#  Copyright (c) 2011-2012, Image Engine Design Inc. All rights reserved.
#  Copyright (c) 2011-2012, John Haddon. All rights reserved.
#
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions are
#  met:
#
#      * Redistributions of source code must retain the above
#        copyright notice, this list of conditions and the following
#        disclaimer.
#
#      * Redistributions in binary form must reproduce the above
#        copyright notice, this list of conditions and the following
#        disclaimer in the documentation and/or other materials provided with
#        the distribution.
#
#      * Neither the name of John Haddon nor the names of
#        any other contributors to this software may be used to endorse or
#        promote products derived from this software without specific prior
#        written permission.
#
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
#
##########################################################################

from __future__ import with_statement

import IECore

import GafferUI
import GafferCortexUI

## Supported parameter userData entries :
#
# ["UI"]["collapsible"]
# ["UI"]["collapsed"]
#
# Supported child userData entries :
#
# ["UI"]["visible"]
class CompoundParameterValueWidget( GafferCortexUI.ParameterValueWidget ) :

	## If collapsible is not None then it overrides any ["UI]["collapsible"] userData the parameter might have.
	def __init__( self, parameterHandler, collapsible=None, _plugValueWidgetClass=None, **kw ) :

		if collapsible is None :
			collapsible = True
			with IECore.IgnoredExceptions( KeyError ) :
				collapsible = parameterHandler.parameter().userData()["UI"]["collapsible"].value

		collapsed = None
		if collapsible :
			collapsed = True
			with IECore.IgnoredExceptions( KeyError ) :
				collapsed = parameterHandler.parameter().userData()["UI"]["collapsed"].value

		if _plugValueWidgetClass is None :
			_plugValueWidgetClass = _PlugValueWidget

		GafferCortexUI.ParameterValueWidget.__init__(
			self,
			_plugValueWidgetClass( parameterHandler, collapsed ),
			parameterHandler,
			**kw
		)

# CompoundParameterValueWidget is simply a lightweight wrapper around this CompoundPlugValueWidget
# derived class. This allows us to take advantage of all the code in CompoundPlugValueWidget that
# deals with dynamically adding and removing children etc.
class _PlugValueWidget( GafferUI.CompoundPlugValueWidget ) :

	def __init__( self, parameterHandler, collapsed ) :

		GafferUI.CompoundPlugValueWidget.__init__( self, parameterHandler.plug(), collapsed )

		self.__parameterHandler = parameterHandler

	def _childPlugs( self ) :

		plug = self.getPlug()
		orderedChildren = []
		for childName in self.__parameterHandler.parameter().keys() :
			if childName in plug :
				orderedChildren.append( plug[childName] )

		return orderedChildren

	def _childPlugWidget( self, childPlug ) :

		childParameter = self.__parameterHandler.parameter()[childPlug.getName()]

		with IECore.IgnoredExceptions( KeyError ) :
			if not childParameter.userData()["UI"]["visible"].value :
				return None

		childParameterHandler = self.__parameterHandler.childParameterHandler( childParameter )
		valueWidget = GafferCortexUI.ParameterValueWidget.create( childParameterHandler )
		if not valueWidget :
			return None

		if isinstance( valueWidget, CompoundParameterValueWidget ) :
			return valueWidget

		return GafferUI.PlugWidget( valueWidget )

	def _parameter( self ) :

		return self.__parameterHandler.parameter()

	def _parameterHandler( self ) :

		return self.__parameterHandler

	def _parameterLabelText( self, parameterHandler ) :

 		return IECore.CamelCase.toSpaced( parameterHandler.plug().getName() )

	def _parameterToolTip( self, parameterHandler ) :

		plug = parameterHandler.plug()

		result = "<h3>" + plug.relativeName( plug.node() ) + "</h3>"
		if parameterHandler.parameter().description :
			result += "\n\n" + parameterHandler.parameter().description

		return result

# install implementation class as a protected member, so it can be used by
# derived classes.
CompoundParameterValueWidget._PlugValueWidget = _PlugValueWidget

GafferCortexUI.ParameterValueWidget.registerType( IECore.CompoundParameter, CompoundParameterValueWidget )

"""Generic SSH class providing method to connect switches, routers and other devices using SSHv2.

cNetSSH uses paramiko, see API docs http://docs.paramiko.org/en/latest/ 
"""

__author__    = "dumplab"
__copyright__ = "2015 dumplab"
__license__   = "MIT"
__version__   = "0.5"
__status__    = "Developement"

import paramiko,re,sys,time

class cNetSSH(object):
	"""SSH connection object"""
	
        def __init__(self):
		"""Set default attribute values only
		
		No arguments
		"""
		self.__host      = ""
		self.__user      = ""
		self.__pass      = ""
		self.__conn      = None # paramiko connection
		self.__shell     = None # when using a channel
		self.__connected = False
		self.__input     = ""
		self.__output    = ""
		self.__timeout   = 1.0  # right now we use a timeout of 2 seconds to connect
		self.__outEnd    = ""   # will be considered as the prompt and end of an output see recv is discovered during the login
		self.__debug     = False

	def connect(self,hostName,userName="",userPass="",newShell=True,paging=False):
		"""connect a device using provided credentials and start an interactive shell
		
		Keyword arguments:
		hostName = hostname (default "")
		userName = username to authenticate (default "")
		userPass = password to use for authenticating and for unlocking a private key (default "")
		newShell = shall we start a shell, opens a new Channel (default True)
		paging   = enable or disable paging/more (Default False)
		returns the configuration as a string
		"""
		self.__host = hostName
		self.__user = userName
		self.__pass = userPass

		try:
			self.__conn = paramiko.SSHClient()
			# add untrusted hosts
			self.__conn.set_missing_host_key_policy(paramiko.AutoAddPolicy())
			# connect to host
			self.__conn.connect(self.__host,username=self.__user,password=self.__pass,look_for_keys=False,timeout=1.0)
			# set connected flag
			self.__connected = True
			# debug
			if self.__debug:
				print("cNetSSH::connect - connected to device " + self.__outEnd)
			# Start an interactive shell session on the SSH server. A new Channel is opened and connected to a pseudo-terminal using the requested terminal type and size.
			if newShell==True:
				self.__shell = self.__conn.invoke_shell()
				time.sleep(0.3)
				# Save the initial router prompt
				self.__output = self.__shell.recv(32000)
				self.__output = self.__output.splitlines(True)
				self.__outEnd = self.__output[len(self.__output)-1]
				if self.__debug:
					print("cNetSSH::connect - I'll consider this string as the prompt in further requests: " + self.__outEnd)
				if paging==False:
					self.__disablePaging()
		except paramiko.AuthenticationException:
			print("Authentication failed when connecting to " + self.__host)
			sys.exit(1)
		except:
			print("Could not connect to host " + self.__host)


	def enable(self,password=""):
		"""enable - enter enable mode. please use this method as it stores the new prompt to spped up futher command processing
		
		Keyword arguments:
		password = enable password (default "")
		"""
		if self.__connected:
			self.__input = "enable"
			numBytes     = self.__shell.send(self.__input + "\n" + password + "\n")
			if numBytes > 0:
				time.sleep(0.3)
				# Save the router prompt
				self.__output = self.__shell.recv(32000)
				self.__output = self.__output.splitlines(True)
				self.__outEnd = self.__output[len(self.__output)-1]
				if self.__debug:
					print("cNetSSH::enable - change expected prompt to " + self.__outEnd)
				return True
		else:
			print("Not connected to " + self.__host + ". Connect first.")

	def send(self,command):
		if self.__connected:
			self.__input = command
			numBytes     = self.__shell.send(command + "\n")
			if numBytes > 0:
				self.__output = ""
				myTempBuffer  = ""
				max_try       = 500
				x             = 0
				sTime         = time.time()
				bailedOut     = False
				while x < max_try:
					if self.__shell.recv_ready():
						if self.__debug:
							print("cNetSSH::send - recv_ready() on cycle=" + str(x))
						while True:
							# note recv returns if there is > 0 < 1024
							myTempBuffer = self.__shell.recv(1024)
							self.__output += myTempBuffer
							#print("cNetSSH: recv() returned ... len=" + str(len(myTempBuffer)))
							if len(myTempBuffer)==0 or self.__shell.recv_ready()==False:
								break
					else:
						time.sleep(0.00005)
					x += 1
					# bail out if we've found the prompt again
					if re.search(self.__outEnd,self.__output):
						bailedOut = True
						break
				if self.__debug:
					eTime = time.time()-sTime
					print("cNetSSH::send - received " + str(len(self.__output)) + " bytes in " + str(x) + " cycles and " + str(eTime) + "s. BailedOut: " + str(bailedOut))
				return self.__sanitizeOutput(self.__output)
		else:
			print("Not connected to " + self.__host + ". Connect first.")

	def disconnect(self):
		"""disconnect
		
		returns the configuration as a string
		"""
		self.__conn = None

	def __sanitizeOutput(self,output):
		"""sanitizeOutput - remove the sent command from output and the prompt
		
		returns the configuration as a string
		"""
		tempOut = output.splitlines(True)
		newOut  = ""
		for line in tempOut:
			if not re.search("^" + self.__outEnd,line) and not re.search(self.__input,line):
				newOut += line
		return newOut

	def __disablePaging(self):
		if self.__connected:
			self.__shell.send("terminal length 0\n")
			time.sleep(0.25)
			if self.__shell.recv_ready():
				self.__output = self.__shell.recv(200)
		else:
			print("Not connected to " + self.__host + ". Connect first.")

	def configure(self):
		"""enter configuration mode using configure terminal
		
		returns True on success, False on problems
		"""
		if self.__connected:
			self.__shell.send("\nconfigure terminal\n")
			time.sleep(0.25)
			if self.__shell.recv_ready():
				self.__output = self.__shell.recv(200)
				# we expect (config)#
				if re.search("config\)#",self.__output):
					return True
				return False
		else:
			print("Not connected to " + self.__host + ". Connect first.")
			return False

	def noconfigure(self):
		"""leave configuration mode using configure terminal
		
		returns True on success, False on problems
		"""
		if self.__connected:
			self.__shell.send("\nend\n")
			time.sleep(0.25)
			if self.__shell.recv_ready():
				self.__output = self.__shell.recv(200)
				# we expect (config)#
				if re.search("#",self.__output):
					return True
				return False
		else:
			print("Not connected to " + self.__host + ". Connect first.")
			return False

if __name__ == '__main__':
	print("This class should only be imported and not run directly!")

from django.apps import AppConfig


class GenericConfig(AppConfig):
    default_auto_field = 'django.db.models.BigAutoField'
    name = 'generic'

    def ready(self) -> None:
        from generic import signals
        return super().ready()

# Generated by Django 3.2.2 on 2021-05-18 15:01

from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        ('usd_rest_api', '0006_alter_lesson_group'),
    ]

    operations = [
        migrations.AddField(
            model_name='event',
            name='account',
            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, to='usd_rest_api.account'),
        ),
    ]


from pl_bolts.transforms.self_supervised import RandomTranslateWithReflect, Patchify

try:
    from torchvision import transforms
except ImportError:
    warn('You want to use `torchvision` which is not installed yet,'  # pragma: no-cover
                      ' install it with `pip install torchvision`.')


class CPCTrainTransformsCIFAR10:

    def __init__(self, patch_size=8, overlap=4):
        """
        Transforms used for CPC:

        Args:

            patch_size: size of patches when cutting up the image into overlapping patches
            overlap: how much to overlap patches

        Transforms::

            random_flip
            img_jitter
            col_jitter
            rnd_gray
            transforms.ToTensor()
            normalize
            Patchify(patch_size=patch_size, overlap_size=patch_size // 2)

        Example::

            # in a regular dataset
            CIFAR10(..., transforms=CPCTrainTransformsCIFAR10())

            # in a DataModule
            module = CIFAR10DataModule(PATH)
            train_loader = module.train_dataloader(batch_size=32, transforms=CPCTrainTransformsCIFAR10())

        """
        self.patch_size = patch_size
        self.overlap = overlap
        self.flip_lr = transforms.RandomHorizontalFlip(p=0.5)

        normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],
                                         std=[x / 255.0 for x in [63.0, 62.1, 66.7]])
        col_jitter = transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.2)], p=0.8)
        img_jitter = transforms.RandomApply([RandomTranslateWithReflect(4)], p=0.8)
        rnd_gray = transforms.RandomGrayscale(p=0.25)

        self.transforms = transforms.Compose([
            img_jitter,
            col_jitter,
            rnd_gray,
            transforms.ToTensor(),
            normalize,
            Patchify(patch_size=patch_size, overlap_size=overlap),
        ])

    def __call__(self, inp):
        inp = self.flip_lr(inp)
        out1 = self.transforms(inp)
        return out1


class CPCEvalTransformsCIFAR10:

    def __init__(self, patch_size=8, overlap=4):
        """
        Transforms used for CPC:

        Args:

            patch_size: size of patches when cutting up the image into overlapping patches
            overlap: how much to overlap patches

        Transforms::

            random_flip
            transforms.ToTensor()
            normalize
            Patchify(patch_size=patch_size, overlap_size=overlap)

        Example::

            # in a regular dataset
            CIFAR10(..., transforms=CPCEvalTransformsCIFAR10())

            # in a DataModule
            module = CIFAR10DataModule(PATH)
            train_loader = module.train_dataloader(batch_size=32, transforms=CPCEvalTransformsCIFAR10())

        """

        # flipping image along vertical axis
        self.patch_size = patch_size
        self.overlap = overlap
        self.flip_lr = transforms.RandomHorizontalFlip(p=0.5)

        normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],
                                         std=[x / 255.0 for x in [63.0, 62.1, 66.7]])

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            normalize,
            Patchify(patch_size=patch_size, overlap_size=overlap),
        ])

    def __call__(self, inp):
        out1 = self.transforms(inp)
        return out1


class CPCTrainTransformsSTL10:

    def __init__(self, patch_size=16, overlap=8):
        """
        Transforms used for CPC:

        Args:

            patch_size: size of patches when cutting up the image into overlapping patches
            overlap: how much to overlap patches

        Transforms::

            random_flip
            img_jitter
            col_jitter
            rnd_gray
            transforms.ToTensor()
            normalize
            Patchify(patch_size=patch_size, overlap_size=patch_size // 2)

        Example::

            # in a regular dataset
            STL10(..., transforms=CPCTrainTransformsSTL10())

            # in a DataModule
            module = STL10DataModule(PATH)
            train_loader = module.train_dataloader(batch_size=32, transforms=CPCTrainTransformsSTL10())


        """
        # flipping image along vertical axis
        self.patch_size = patch_size
        self.overlap = overlap
        self.flip_lr = transforms.RandomHorizontalFlip(p=0.5)
        normalize = transforms.Normalize(mean=(0.43, 0.42, 0.39), std=(0.27, 0.26, 0.27))

        # image augmentation functions
        col_jitter = transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.2)], p=0.8)
        rnd_gray = transforms.RandomGrayscale(p=0.25)
        rand_crop = transforms.RandomResizedCrop(64, scale=(0.3, 1.0), ratio=(0.7, 1.4), interpolation=3)

        self.transforms = transforms.Compose([
            rand_crop,
            col_jitter,
            rnd_gray,
            transforms.ToTensor(),
            normalize,
            Patchify(patch_size=patch_size, overlap_size=overlap)
        ])

    def __call__(self, inp):
        inp = self.flip_lr(inp)
        out1 = self.transforms(inp)
        return out1


class CPCEvalTransformsSTL10:

    def __init__(self, patch_size=16, overlap=8):
        """
        Transforms used for CPC:

        Args:

            patch_size: size of patches when cutting up the image into overlapping patches
            overlap: how much to overlap patches

        Transforms::

            random_flip
            transforms.ToTensor()
            normalize
            Patchify(patch_size=patch_size, overlap_size=patch_size // 2)

        Example::

            # in a regular dataset
            STL10(..., transforms=CPCEvalTransformsSTL10())

            # in a DataModule
            module = STL10DataModule(PATH)
            train_loader = module.train_dataloader(batch_size=32, transforms=CPCEvalTransformsSTL10())

        """
        # flipping image along vertical axis
        self.patch_size = patch_size
        self.overlap = overlap
        self.flip_lr = transforms.RandomHorizontalFlip(p=0.5)
        normalize = transforms.Normalize(mean=(0.43, 0.42, 0.39), std=(0.27, 0.26, 0.27))

        self.transforms = transforms.Compose([
            transforms.Resize(70, interpolation=3),
            transforms.CenterCrop(64),
            transforms.ToTensor(),
            normalize,
            Patchify(patch_size=patch_size, overlap_size=overlap)
        ])

    def __call__(self, inp):
        out1 = self.transforms(inp)
        return out1


class CPCTrainTransformsImageNet128:
    def __init__(self, patch_size=32, overlap=16):
        """
        Transforms used for CPC:

        Args:

            patch_size: size of patches when cutting up the image into overlapping patches
            overlap: how much to overlap patches

        Transforms::

            random_flip
            transforms.ToTensor()
            normalize
            Patchify(patch_size=patch_size, overlap_size=patch_size // 2)

        Example::

            # in a regular dataset
            Imagenet(..., transforms=CPCTrainTransformsImageNet128())

            # in a DataModule
            module = ImagenetDataModule(PATH)
            train_loader = module.train_dataloader(batch_size=32, transforms=CPCTrainTransformsImageNet128())
        """
        # image augmentation functions
        self.patch_size = patch_size
        self.overlap = overlap
        self.flip_lr = transforms.RandomHorizontalFlip(p=0.5)
        rand_crop = transforms.RandomResizedCrop(128, scale=(0.3, 1.0), ratio=(0.7, 1.4), interpolation=3)
        col_jitter = transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8)
        rnd_gray = transforms.RandomGrayscale(p=0.25)

        post_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225]),
            Patchify(patch_size=patch_size, overlap_size=overlap),
        ])

        self.transforms = transforms.Compose([
            rand_crop,
            col_jitter,
            rnd_gray,
            post_transform
        ])

    def __call__(self, inp):
        inp = self.flip_lr(inp)
        out1 = self.transforms(inp)
        return out1


class CPCEvalTransformsImageNet128:
    def __init__(self, patch_size=32, overlap=16):
        """
        Transforms used for CPC:

        Args:

            patch_size: size of patches when cutting up the image into overlapping patches
            overlap: how much to overlap patches

        Transforms::

            random_flip
            transforms.ToTensor()
            normalize
            Patchify(patch_size=patch_size, overlap_size=patch_size // 2)

        Example::

            # in a regular dataset
            Imagenet(..., transforms=CPCEvalTransformsImageNet128())

            # in a DataModule
            module = ImagenetDataModule(PATH)
            train_loader = module.train_dataloader(batch_size=32, transforms=CPCEvalTransformsImageNet128())
        """
        # image augmentation functions
        self.patch_size = patch_size
        self.overlap = overlap
        self.flip_lr = transforms.RandomHorizontalFlip(p=0.5)
        post_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225]),
            Patchify(patch_size=patch_size, overlap_size=overlap),
        ])
        self.transforms = transforms.Compose([
            transforms.Resize(146, interpolation=3),
            transforms.CenterCrop(128),
            post_transform
        ])

    def __call__(self, inp):
        inp = self.flip_lr(inp)
        out1 = self.transforms(inp)
        return out1

# model settings
temperature = 0.2
with_norm = True
query_dim = 128
model = dict(
    type='SimSiamBaseTracker',
    backbone=dict(
        type='ResNet',
        pretrained=None,
        depth=18,
        out_indices=(3, ),
        # strides=(1, 2, 1, 1),
        norm_cfg=dict(type='SyncBN', requires_grad=True),
        norm_eval=False,
        zero_init_residual=True),
    # cls_head=None,
    # patch_head=None,
    img_head=dict(
        type='SimSiamHead',
        in_channels=512,
        norm_cfg=dict(type='SyncBN'),
        num_projection_fcs=3,
        projection_mid_channels=512,
        projection_out_channels=512,
        num_predictor_fcs=2,
        predictor_mid_channels=128,
        predictor_out_channels=512,
        with_norm=True,
        loss_feat=dict(type='CosineSimLoss', negative=False),
        spatial_type='avg'))
# model training and testing settings
train_cfg = dict(intra_video=False, transpose_temporal=True)
test_cfg = dict(
    precede_frames=20,
    topk=10,
    temperature=0.2,
    strides=(1, 2, 1, 1),
    out_indices=(2, 3),
    neighbor_range=24,
    with_first=True,
    with_first_neighbor=True,
    output_dir='eval_results')
# dataset settings
dataset_type = 'VideoDataset'
dataset_type_val = 'DavisDataset'
data_prefix = 'data/kinetics400/videos_train'
ann_file_train = 'data/kinetics400/kinetics400_train_list_videos.txt'
data_prefix_val = 'data/davis/DAVIS/JPEGImages/480p'
anno_prefix_val = 'data/davis/DAVIS/Annotations/480p'
data_root_val = 'data/davis/DAVIS'
ann_file_val = 'data/davis/DAVIS/ImageSets/davis2017_val_list_rawframes.txt'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)
train_pipeline = [
    dict(type='DecordInit'),
    dict(type='SampleFrames', clip_len=2, frame_interval=8, num_clips=1),
    # dict(type='DuplicateFrames', times=2),
    dict(type='DecordDecode'),
    dict(
        type='RandomResizedCrop',
        area_range=(0.2, 1.),
        same_across_clip=False,
        same_on_clip=False),
    dict(type='Resize', scale=(224, 224), keep_ratio=False),
    dict(
        type='Flip',
        flip_ratio=0.5,
        same_across_clip=False,
        same_on_clip=False),
    # dict(
    #     type='ColorJitter',
    #     brightness=0.4,
    #     contrast=0.4,
    #     saturation=0.4,
    #     hue=0.1,
    #     p=0.8,
    #     same_across_clip=False,
    #     same_on_clip=False),
    # dict(
    #     type='RandomGrayScale',
    #     p=0.2,
    #     same_across_clip=False,
    #     same_on_clip=False),
    # dict(
    #     type='RandomGaussianBlur',
    #     p=0.5,
    #     same_across_clip=False,
    #     same_on_clip=False),
    dict(type='Normalize', **img_norm_cfg),
    dict(type='FormatShape', input_format='NCTHW'),
    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),
    dict(type='ToTensor', keys=['imgs', 'label'])
]
val_pipeline = [
    dict(type='SequentialSampleFrames', frame_interval=1),
    dict(type='RawFrameDecode'),
    dict(type='Resize', scale=(-1, 480), keep_ratio=True),
    dict(type='Flip', flip_ratio=0),
    dict(type='Normalize', **img_norm_cfg),
    dict(type='FormatShape', input_format='NCTHW'),
    dict(
        type='Collect',
        keys=['imgs', 'ref_seg_map'],
        meta_keys=('frame_dir', 'frame_inds', 'original_shape', 'seg_map')),
    dict(type='ToTensor', keys=['imgs', 'ref_seg_map'])
]
data = dict(
    videos_per_gpu=128,
    workers_per_gpu=16,
    val_workers_per_gpu=1,
    train=dict(
        type='RepeatDataset',
        times=5,
        dataset=dict(
            type=dataset_type,
            ann_file=ann_file_train,
            data_prefix=data_prefix,
            pipeline=train_pipeline)),
    val=dict(
        type=dataset_type_val,
        ann_file=ann_file_val,
        data_prefix=data_prefix_val,
        data_root=data_root_val,
        anno_prefix=anno_prefix_val,
        pipeline=val_pipeline,
        test_mode=True),
    test=dict(
        type=dataset_type_val,
        ann_file=ann_file_val,
        data_prefix=data_prefix_val,
        data_root=data_root_val,
        anno_prefix=anno_prefix_val,
        pipeline=val_pipeline,
        test_mode=True))
# optimizer
# optimizer = dict(type='Adam', lr=1e-4)
optimizer = dict(type='SGD', lr=0.05, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=None)
# learning policy
lr_config = dict(policy='CosineAnnealing', min_lr=0, by_epoch=False)
# lr_config = dict(policy='Fixed')
# lr_config = dict(
#     policy='step',
#     warmup='linear',
#     warmup_iters=100,
#     warmup_ratio=0.001,
#     step=[1, 2])
total_epochs = 100
checkpoint_config = dict(interval=1)
evaluation = dict(
    interval=1,
    metrics='davis',
    key_indicator='feat_1.J&F-Mean',
    rule='greater')
log_config = dict(
    interval=50,
    hooks=[
        dict(type='TextLoggerHook'),
        # dict(type='TensorboardLoggerHook'),
        dict(
            type='WandbLoggerHook',
            init_kwargs=dict(
                project='mmaction2',
                name='{{fileBasenameNoExtension}}',
                resume=True,
                tags=['ssb'],
                dir='wandb/{{fileBasenameNoExtension}}',
                config=dict(
                    model=model,
                    train_cfg=train_cfg,
                    test_cfg=test_cfg,
                    data=data))),
    ])
# runtime settings
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
find_unused_parameters = False

"""
This module contains the class for detecting
the presence of keywords in an audio stream
"""
import logging
import os

import numpy as np  # type: ignore

from spokestack.context import SpeechContext
from spokestack.models.tensorflow import TFLiteModel
from spokestack.ring_buffer import RingBuffer

from pydub import AudioSegment
from pydub.playback import play

_LOG = logging.getLogger(__name__)


class WakewordTrigger:
    """Detects the presence of a wakeword in the audio input

    Args:
            pre_emphasis (float): The value of the pre-emmphasis filter
            sample_rate (int): The number of audio samples per second of audio (kHz)
            fft_window_type (str): The type of fft window. (only support for hann)
            fft_hop_length (int): Audio sliding window for STFT calculation (ms)
            model_dir (str): Path to the directory containing .tflite models
            posterior_threshold (float): Probability threshold for if a wakeword
                                         was detected
    """

    def __init__(
        self,
        pre_emphasis: float = 0.0,
        sample_rate: int = 16000,
        fft_window_type: str = "hann",
        fft_hop_length: int = 10,
        model_dir: str = "",
        model_type: str = "",
        posterior_threshold: float = 0.5,
        **kwargs,
    ) -> None:

        self.pre_emphasis: float = pre_emphasis
        self.hop_length: int = int(fft_hop_length * sample_rate / 1000)

        if fft_window_type != "hann":
            raise ValueError("Invalid fft_window_type")

        self.filter_model: TFLiteModel = TFLiteModel(
            model_path=os.path.join(model_dir, "filter.tflite")
        )
        self.encode_model: TFLiteModel = TFLiteModel(
            model_path=os.path.join(model_dir, "encode.tflite")
        )
        self.detect_model: TFLiteModel = TFLiteModel(
            model_path=os.path.join(model_dir, "detect.tflite")
        )

        # Model architecture 
        self.model_type = model_type.upper()

        # window size calculated based on fft
        # the filter inputs are (fft_size - 1) / 2
        # which makes the window size (post_fft_size - 1) * 2
        self._window_size = (self.filter_model.input_details[0]["shape"][-1] - 1) * 2
        self._fft_window = np.hanning(self._window_size)

        # retrieve the mel_length and mel_width based on the encoder model metadata
        # these allocate the buffer to the correct size
        if self.model_type == 'CRNN':
            self.mel_length: int = self.encode_model.input_details[0]["shape"][2]
            self.mel_width: int = self.encode_model.input_details[0]["shape"][1]
        elif self.model_type == 'WAVENET':
            self.mel_length: int = self.encode_model.input_details[0]["shape"][1]
            self.mel_width: int = self.encode_model.input_details[0]["shape"][2]

        # initialize the first state input for autoregressive encoder model
        # retrieve the encode_lenth and encode_width from the model detect_model
        # metadata. We get the dimensions from the detect_model inputs because the
        # encode_model runs autoregressively and outputs a single encoded sample.
        # the detect_model input is a collection of these samples.

        if self.model_type == 'CRNN':
            self.encode_length: int = self.detect_model.input_details[0]["shape"][0]
            self.encode_width: int = self.detect_model.input_details[0]["shape"][-1]
        elif self.model_type == 'WAVENET':
            self.encode_length: int = self.detect_model.input_details[0]["shape"][1]
            self.encode_width: int = self.detect_model.input_details[0]["shape"][-1]

        self.sample_window: RingBuffer = RingBuffer(shape=[self._window_size])
        self.frame_window: RingBuffer = RingBuffer(
            shape=[self.mel_length, self.mel_width]
        )
        self.encode_window: RingBuffer = RingBuffer(
            shape=[1, self.encode_length, self.encode_width]
        )

        # initialize the frame and encode windows with zeros
        # this minimizes the delay caused by filling the buffer
        self.frame_window.fill(0.0)
        self.encode_window.fill(-1.0)

        self._posterior_threshold: float = posterior_threshold
        self._posterior_max: float = 0.0
        self._prev_sample: float = 0.0
        self._is_speech: bool = False

        # audio segments for reponse on wake
        self.load_awake_responses('audio_responses')

    def load_awake_responses(self, audio_path):
        # load all mp3's from audio_path for wake responses
        segs = []
        for f in os.listdir(audio_path):
            f_path = os.path.join(audio_path, f)
            if os.path.isfile(f_path) and '.mp3' in f_path:
                segs.append(AudioSegment.from_mp3(f_path))

        self.audio_responses = np.array(segs, dtype=object)

    def __call__(self, context: SpeechContext, frame) -> None:
        """Entry point of the trigger

        Args:
            context (SpeechContext): current state of the speech pipeline
            frame (np.ndarray): a single frame of an audio signal

        Returns: None

        """

        # detect vad edges for wakeword deactivation
        vad_fall = self._is_speech and not context.is_speech
        self._is_speech = context.is_speech

        # sample frame to detect the presence of wakeword
        if not context.is_active:
            self._sample(context, frame)

        # reset on vad fall deactivation
        if vad_fall:
            if not context.is_active:
                _LOG.info(f"wake: {self._posterior_max}")
            self.reset()

    def _sample(self, context: SpeechContext, frame) -> None:
        # convert the PCM-16 audio to float32 in (-1.0, 1.0)
        frame = frame.astype(np.float32) / (2 ** 15 - 1)
        frame = np.clip(frame, -1.0, 1.0)

        # pull out a single value from the frame and apply pre-emphasis
        # with the previous sample then cache the previous sample
        # to be use in the next iteration
        prev_sample = frame[-1]
        frame -= self.pre_emphasis * np.append(self._prev_sample, frame[:-1])
        self._prev_sample = prev_sample

        # fill the sample window to analyze speech containing samples
        # after each window fill the buffer advances by the hop length
        # to produce an overlapping window
        for sample in frame:
            self.sample_window.write(sample)
            if self.sample_window.is_full:
                if context.is_speech:
                    self._analyze(context)
                self.sample_window.rewind().seek(self.hop_length)

    def _analyze(self, context: SpeechContext) -> None:
        # read the full contents of the sample window to calculate a single frame
        # of the STFT by applying the DFT to a real-valued input and
        # taking the magnitude of the complex DFT
        frame = self.sample_window.read_all()
        frame = np.fft.rfft(frame * self._fft_window, n=self._window_size)
        frame = np.abs(frame).astype(np.float32)

        # compute mel spectrogram
        self._filter(context, frame)

    def _filter(self, context: SpeechContext, frame) -> None:
        # add the batch dimension and compute the mel spectrogram with filter model
        frame = np.expand_dims(frame, 0)
        frame = self.filter_model(frame)[0]

        # advance the window by 1 and write mel frame to the frame buffer
        self.frame_window.rewind().seek(1)
        self.frame_window.write(frame)

        # encode the mel spectrogram
        self._encode(context)

    def _encode(self, context: SpeechContext) -> None:
        # read the full contents of the frame window and add the batch dimension
        # run the encoder and save the output state for autoregression
        frame = self.frame_window.read_all()

        # different architectures have different input requirements 
        if self.model_type == 'CRNN':
            # swap timesteps and features
            frame = np.expand_dims(frame.T, 0)
            # add channel dimension
            frame = np.expand_dims(frame, -1)

        elif self.model_type == 'WAVENET':
            frame = np.expand_dims(frame, 0)

        # inference for encoder
        frame = self.encode_model(frame)

        # accumulate encoded samples until size of detection window
        self.encode_window.rewind().seek(1)
        self.encode_window.write(np.squeeze(frame))
        #self.encode_window.write(frame)
        self._detect(context)

    def _detect(self, context: SpeechContext) -> None:
        # read the full contents of the encode window and add the batch dimension
        # calculate a scalar probability of if the frame contains the wakeword
        # with the detect model
        frame = self.encode_window.read_all()

        # frame is (batch,timesteps,features), same as Wavenet
        # CRNN detect input is (batch,features)
        if self.model_type == 'CRNN':
            frame = frame.squeeze(0)

        if self.model_type == "CRNN":
            posterior = self.detect_model(frame)[0][0][0]
        else:
            posterior = self.detect_model(frame)[0][0][1]

        if posterior > self._posterior_max:
            self._posterior_max = posterior
        if posterior > self._posterior_threshold:
            if not context.is_active:
                _LOG.info(f"AWAKE!: {self._posterior_max}")
                play(np.random.choice(self.audio_responses))
                context.is_active = True

    def reset(self) -> None:
        """ Resets the currect WakewordDetector state """
        self.sample_window.reset()
        self.frame_window.reset().fill(0.0)
        self.encode_window.reset().fill(-1.0)
        self._posterior_max = 0.0

    def close(self) -> None:
        """ Close interface for use in the pipeline """
        self.reset()

"Rules for running Rollup under Bazel"

load("@build_bazel_rules_nodejs//:providers.bzl", "JSEcmaScriptModuleInfo", "NodeContextInfo", "NpmPackageInfo", "node_modules_aspect", "run_node")
load("@build_bazel_rules_nodejs//internal/linker:link_node_modules.bzl", "module_mappings_aspect")

_DOC = """Runs the Rollup.js CLI under Bazel.

See https://rollupjs.org/guide/en/#command-line-reference

Typical example:
```python
load("@npm_bazel_rollup//:index.bzl", "rollup_bundle")

rollup_bundle(
    name = "bundle",
    srcs = ["dependency.js"],
    entry_point = "input.js",
    config_file = "rollup.config.js",
)
```

Note that the command-line options set by Bazel override what appears in the rollup config file.
This means that typically a single `rollup.config.js` can contain settings for your whole repo,
and multiple `rollup_bundle` rules can share the configuration.

Thus, setting options that Bazel controls will have no effect, e.g.

```javascript
module.exports = {
    output: { file: 'this_is_ignored.js' },
}
```

You must determine ahead of time whether Rollup needs to produce a directory output.
This is the case if you have dynamic imports which cause code-splitting, or if you
provide multiple entry points. Use the `output_dir` attribute to specify that you want a
directory output.
Rollup's CLI has the same behavior, forcing you to pick `--output.file` or `--output.dir`.

To get multiple output formats, wrap the rule with a macro or list comprehension, e.g.

```python
[
    rollup_bundle(
        name = "bundle.%s" % format,
        entry_point = "foo.js",
        format = format,
    )
    for format in [
        "cjs",
        "umd",
    ]
]
```

This will produce one output per requested format.
"""

_ROLLUP_ATTRS = {
    "srcs": attr.label_list(
        doc = """Non-entry point JavaScript source files from the workspace.

You must not repeat file(s) passed to entry_point/entry_points.
""",
        # Don't try to constrain the filenames, could be json, svg, whatever
        allow_files = True,
    ),
    "args": attr.string_list(
        doc = """Command line arguments to pass to rollup. Can be used to override config file settings.

These argument passed on the command line before all arguments that are always added by the
rule such as `--output.dir` or `--output.file`, `--format`, `--config` and `--preserveSymlinks` and
also those that are optionally added by the rule such as `--sourcemap`.

See rollup CLI docs https://rollupjs.org/guide/en/#command-line-flags for complete list of supported arguments.""",
        default = [],
    ),
    "config_file": attr.label(
        doc = """A rollup.config.js file

Passed to the --config 
See https://rollupjs.org/guide/en/#configuration-files

If not set, a default basic Rollup config is used.
""",
        allow_single_file = True,
        default = "@npm_bazel_rollup//:rollup.config.js",
    ),
    "entry_point": attr.label(
        doc = """The bundle's entry point (e.g. your main.js or app.js or index.js).

This is just a shortcut for the `entry_points` attribute with a single output chunk named the same as the rule.

For example, these are equivalent:

```python
rollup_bundle(
    name = "bundle",
    entry_point = "index.js",
)
```

```python
rollup_bundle(
    name = "bundle",
    entry_points = {
        "index.js": "bundle"
    }
)
```

If `rollup_bundle` is used on a `ts_library`, the `rollup_bundle` rule handles selecting the correct outputs from `ts_library`.
In this case, `entry_point` can be specified as the `.ts` file and `rollup_bundle` will handle the mapping to the `.mjs` output file.

For example:

```python
ts_library(
    name = "foo",
    srcs = [
        "foo.ts",
        "index.ts",
    ],
)

rollup_bundle(
    name = "bundle",
    deps = [ "foo" ],
    entry_point = "index.ts",
)
```
""",
        allow_single_file = True,
    ),
    "entry_points": attr.label_keyed_string_dict(
        doc = """The bundle's entry points (e.g. your main.js or app.js or index.js).

Passed to the [`--input` option](https://github.com/rollup/rollup/blob/master/docs/999-big-list-of-options.md#input) in Rollup.

Keys in this dictionary are labels pointing to .js entry point files.
Values are the name to be given to the corresponding output chunk.

Either this attribute or `entry_point` must be specified, but not both.
""",
        allow_files = True,
    ),
    "format": attr.string(
        doc = """"Specifies the format of the generated bundle. One of the following:

- `amd`: Asynchronous Module Definition, used with module loaders like RequireJS
- `cjs`: CommonJS, suitable for Node and other bundlers
- `esm`: Keep the bundle as an ES module file, suitable for other bundlers and inclusion as a `<script type=module>` tag in modern browsers
- `iife`: A self-executing function, suitable for inclusion as a `<script>` tag. (If you want to create a bundle for your application, you probably want to use this.)
- `umd`: Universal Module Definition, works as amd, cjs and iife all in one
- `system`: Native format of the SystemJS loader
""",
        values = ["amd", "cjs", "esm", "iife", "umd", "system"],
        default = "esm",
    ),
    "node_context_data": attr.label(
        default = "@build_bazel_rules_nodejs//internal:node_context_data",
        providers = [NodeContextInfo],
        doc = "Internal use only",
    ),
    "output_dir": attr.bool(
        doc = """Whether to produce a directory output.

We will use the [`--output.dir` option](https://github.com/rollup/rollup/blob/master/docs/999-big-list-of-options.md#outputdir) in rollup
rather than `--output.file`.

If the program produces multiple chunks, you must specify this attribute.
Otherwise, the outputs are assumed to be a single file.
""",
    ),
    "rollup_bin": attr.label(
        doc = "Target that executes the rollup binary",
        executable = True,
        cfg = "host",
        default = "@npm//rollup/bin:rollup",
    ),
    "rollup_worker_bin": attr.label(
        doc = "Internal use only",
        executable = True,
        cfg = "host",
        # NB: will be substituted with "@npm//@bazel/rollup/bin:rollup-worker" when the pkg_npm target is built
        default = "@npm//@bazel/rollup/bin:rollup-worker",
    ),
    "silent": attr.bool(
        doc = """Whether to execute the rollup binary with the --silent flag, defaults to False.

Using --silent can cause rollup to [ignore errors/warnings](https://github.com/rollup/rollup/blob/master/docs/999-big-list-of-options.md#onwarn) 
which are only surfaced via logging.  Since bazel expects printing nothing on success, setting silent to True
is a more Bazel-idiomatic experience, however could cause rollup to drop important warnings.
""",
    ),
    "sourcemap": attr.string(
        doc = """Whether to produce sourcemaps.

Passed to the [`--sourcemap` option](https://github.com/rollup/rollup/blob/master/docs/999-big-list-of-options.md#outputsourcemap") in Rollup
""",
        default = "inline",
        values = ["inline", "hidden", "true", "false"],
    ),
    "supports_workers": attr.bool(
        doc = """Experimental! Use only with caution.

Allows you to enable the Bazel Worker strategy for this library.
When enabled, this rule invokes the "rollup_worker_bin"
worker aware binary rather than "rollup_bin".""",
        default = False,
    ),
    "deps": attr.label_list(
        aspects = [module_mappings_aspect, node_modules_aspect],
        doc = """Other libraries that are required by the code, or by the rollup.config.js""",
    ),
}

def _desugar_entry_point_names(name, entry_point, entry_points):
    """Users can specify entry_point (sugar) or entry_points (long form).

    This function allows our code to treat it like they always used the long form.

    It also performs validation:
    - exactly one of these attributes should be specified
    """
    if entry_point and entry_points:
        fail("Cannot specify both entry_point and entry_points")
    if not entry_point and not entry_points:
        fail("One of entry_point or entry_points must be specified")
    if entry_point:
        return [name]
    return entry_points.values()

def _desugar_entry_points(name, entry_point, entry_points, inputs):
    """Like above, but used by the implementation function, where the types differ.

    It also performs validation:
    - attr.label_keyed_string_dict doesn't accept allow_single_file
      so we have to do validation now to be sure each key is a label resulting in one file

    It converts from dict[target: string] to dict[file: string]
    """
    names = _desugar_entry_point_names(name, entry_point.label if entry_point else None, entry_points)

    if entry_point:
        return {_resolve_js_input(entry_point.files.to_list()[0], inputs): names[0]}

    result = {}
    for ep in entry_points.items():
        entry_point = ep[0]
        name = ep[1]
        f = entry_point.files.to_list()
        if len(f) != 1:
            fail("keys in rollup_bundle#entry_points must provide one file, but %s has %s" % (entry_point.label, len(f)))
        result[_resolve_js_input(f[0], inputs)] = name
    return result

def _resolve_js_input(f, inputs):
    if f.extension == "js" or f.extension == "mjs":
        return f

    # look for corresponding js file in inputs
    no_ext = _no_ext(f)
    for i in inputs:
        if i.extension == "js" or i.extension == "mjs":
            if _no_ext(i) == no_ext:
                return i
    fail("Could not find corresponding javascript entry point for %s. Add the %s.js to your deps." % (f.path, no_ext))

def _rollup_outs(sourcemap, name, entry_point, entry_points, output_dir):
    """Supply some labelled outputs in the common case of a single entry point"""
    result = {}
    entry_point_outs = _desugar_entry_point_names(name, entry_point, entry_points)
    if output_dir:
        # We can't declare a directory output here, because RBE will be confused, like
        # com.google.devtools.build.lib.remote.ExecutionStatusException:
        # INTERNAL: failed to upload outputs: failed to construct CAS files:
        # failed to calculate file hash:
        # read /b/f/w/bazel-out/k8-fastbuild/bin/packages/rollup/test/multiple_entry_points/chunks: is a directory
        #result["chunks"] = output_dir
        return {}
    else:
        if len(entry_point_outs) > 1:
            fail("Multiple entry points require that output_dir be set")
        out = entry_point_outs[0]
        result[out] = out + ".js"
        if sourcemap == "true":
            result[out + "_map"] = "%s.map" % result[out]
    return result

def _no_ext(f):
    return f.short_path[:-len(f.extension) - 1]

def _filter_js(files):
    return [f for f in files if f.extension == "js" or f.extension == "mjs"]

def _rollup_bundle(ctx):
    "Generate a rollup config file and run rollup"

    # rollup_bundle supports deps with JS providers. For each dep,
    # JSEcmaScriptModuleInfo is used if found, then JSModuleInfo and finally
    # the DefaultInfo files are used if the former providers are not found.
    deps_depsets = []
    for dep in ctx.attr.deps:
        if JSEcmaScriptModuleInfo in dep:
            deps_depsets.append(dep[JSEcmaScriptModuleInfo].sources)
        elif hasattr(dep, "files"):
            deps_depsets.append(dep.files)

        # Also include files from npm deps as inputs.
        # These deps are identified by the NpmPackageInfo provider.
        if NpmPackageInfo in dep:
            deps_depsets.append(dep[NpmPackageInfo].sources)
    deps_inputs = depset(transitive = deps_depsets).to_list()

    inputs = _filter_js(ctx.files.entry_point) + _filter_js(ctx.files.entry_points) + ctx.files.srcs + deps_inputs
    outputs = [getattr(ctx.outputs, o) for o in dir(ctx.outputs)]

    # See CLI documentation at https://rollupjs.org/guide/en/#command-line-reference
    args = ctx.actions.args()

    if ctx.attr.supports_workers:
        # Set to use a multiline param-file for worker mode
        args.use_param_file("@%s", use_always = True)
        args.set_param_file_format("multiline")

    # Add user specified arguments *before* rule supplied arguments
    args.add_all(ctx.attr.args)

    # List entry point argument first to save some argv space
    # Rollup doc says
    # When provided as the first options, it is equivalent to not prefix them with --input
    entry_points = _desugar_entry_points(ctx.label.name, ctx.attr.entry_point, ctx.attr.entry_points, inputs).items()

    # If user requests an output_dir, then use output.dir rather than output.file
    if ctx.attr.output_dir:
        outputs.append(ctx.actions.declare_directory(ctx.label.name))
        for entry_point in entry_points:
            args.add_joined([entry_point[1], entry_point[0]], join_with = "=")
        args.add_all(["--output.dir", outputs[0].path])
    else:
        args.add(entry_points[0][0])
        args.add_all(["--output.file", outputs[0].path])

    args.add_all(["--format", ctx.attr.format])

    if ctx.attr.silent:
        # Run the rollup binary with the --silent flag
        args.add("--silent")

    stamp = ctx.attr.node_context_data[NodeContextInfo].stamp

    config = ctx.actions.declare_file("_%s.rollup_config.js" % ctx.label.name)
    ctx.actions.expand_template(
        template = ctx.file.config_file,
        output = config,
        substitutions = {
            "bazel_stamp_file": "\"%s\"" % ctx.version_file.path if stamp else "undefined",
        },
    )

    args.add_all(["--config", config.path])
    inputs.append(config)

    if stamp:
        inputs.append(ctx.version_file)

    # Prevent rollup's module resolver from hopping outside Bazel's sandbox
    # When set to false, symbolic links are followed when resolving a file.
    # When set to true, instead of being followed, symbolic links are treated as if the file is
    # where the link is.
    args.add("--preserveSymlinks")

    if (ctx.attr.sourcemap and ctx.attr.sourcemap != "false"):
        args.add_all(["--sourcemap", ctx.attr.sourcemap])

    executable = "rollup_bin"
    execution_requirements = {}

    if ctx.attr.supports_workers:
        executable = "rollup_worker_bin"
        execution_requirements["supports-workers"] = str(int(ctx.attr.supports_workers))

    run_node(
        ctx,
        progress_message = "Bundling JavaScript %s [rollup]" % outputs[0].short_path,
        executable = executable,
        inputs = inputs,
        outputs = outputs,
        arguments = [args],
        mnemonic = "Rollup",
        execution_requirements = execution_requirements,
        env = {"COMPILATION_MODE": ctx.var["COMPILATION_MODE"]},
    )

    return [
        DefaultInfo(files = depset(outputs)),
    ]

rollup_bundle = rule(
    doc = _DOC,
    implementation = _rollup_bundle,
    attrs = _ROLLUP_ATTRS,
    outputs = _rollup_outs,
)

# -*- coding: utf-8 -*-
"""
Tencent is pleased to support the open source community by making PaaS (BlueKing PaaS Community
Edition) available.
Copyright (C) 2017-2021 THL A29 Limited, a Tencent company. All rights reserved.
Licensed under the MIT License (the "License"); you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://opensource.org/licenses/MIT

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
"""
import os
import uuid
from unittest import mock

import pytest
from kubernetes import client

from backend.tests.conftest import TESTING_API_SERVER_URL
from backend.tests.testing_utils.mocks.collection import StubComponentCollection


class FakeBcsKubeConfigurationService:
    """Fake configuration service which return local apiserver as config"""

    def __init__(self, *args, **kwargs):
        pass

    def make_configuration(self):
        configuration = client.Configuration()
        configuration.api_key = {"authorization": f'Bearer {os.environ.get("TESTING_SERVER_API_KEY")}'}
        configuration.verify_ssl = False
        configuration.host = TESTING_API_SERVER_URL
        return configuration


@pytest.fixture(autouse=True)
def setup_fake_cluster_dependencies():
    #  Comp  Stub  API Server
    with mock.patch(
        'backend.container_service.core.ctx_models.ComponentCollection', new=StubComponentCollection
    ), mock.patch(
        'backend.resources.utils.kube_client.BcsKubeConfigurationService', new=FakeBcsKubeConfigurationService
    ):
        yield

import os
import sys
import pprint
from math import sin, cos, sqrt, atan2, radians
from deprecated import deprecated


# classe di supporto per controllare la quantit
# di output stampato a video
class Verbosity:
    def __init__(self, quiet, verbose, more_verbose):
        self.quiet = quiet
        self.verbose = verbose or more_verbose  # se ho output more_verbose voglio che si stampi anche il verbose
        self.more_verbose = more_verbose


# Inutilizzato.
# Creato in origine per calcolare la distanza in 2D, ora si usa
# un metodo pi preciso che tiene conto della curvatura terrestre.
@deprecated(reason="Utilizzare il metodo distance(), che tiene conto della curvatura terrestre")
def distance_in_2d(sens_one, sens_two):
    x_0 = sens_one.longitudine
    y_0 = sens_one.latitudine
    x_1 = sens_two.longitudine
    y_1 = sens_two.latitudine
    return sqrt((y_0 - y_1) ** 2 + (x_0 - x_1) ** 2)


def find_sensor_by_id(sensor):
    for sen in get_global_sensors():
        if sen.id == sensor:
            return sen
    return None


# Prende in input due tuple di coordinate e restituisce la loro distanza sulla superficie terrestre
def distance_by_coord(node_one, node_two):
    # Approssimazione del raggio della Terra in Km
    raggio_terra = 6373.0

    lat1 = radians(node_one[0])
    lon1 = radians(node_one[1])
    lat2 = radians(node_two[0])
    lon2 = radians(node_two[1])

    diff_lon = lon2 - lon1
    diff_lat = lat2 - lat1

    a = sin(diff_lat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(diff_lon / 2) ** 2
    c = 2 * atan2(sqrt(a), sqrt(1 - a))

    distanza = raggio_terra * c * 1000
    return distanza


# Prende in input due sensori e restituisce
# la loro distanza sulla superficie terrestre
def distance(sens_one, sens_two):
    return distance_by_coord((sens_one.latitudine, sens_one.longitudine),
                             (sens_two.latitudine, sens_two.longitudine))


def print_scenario(a_dict, order_by):
    print("\n\n\n\n\n---------------------------------------------------\n\n\n\n\n")
    print("SCENARIO - ORDINATO PER: " + order_by)
    for temp_sens in a_dict.keys():
        print("\nSensore " + str(temp_sens.id) + ":")
        temp_val = a_dict[temp_sens]
        temp_sens_list = temp_val["senders"]
        temp_tot_cap = temp_val["tot_capacita"]
        temp_rapp_cap_costo = temp_val["rapp_cap_costo"]
        temp_rapp_numsensori_costo = temp_val["rapp_numsensori_costo"]
        print("Senders: ", end='')
        for temp_sender in temp_sens_list:
            print(str(temp_sender.id) + " ", end='')
        print("\nTot_capacit: " + str(temp_tot_cap))
        print("Rapporto capacit/costo: " + str(temp_rapp_cap_costo))
        print("Rapporto numsensori/costo: " + str(temp_rapp_numsensori_costo))
    print("\n\n")


def print_greedy_result(result):
    if get_verbosity().verbose:
        print("\n\n\n")
        print("Dispositivi installati dalla greedy:\n")
        pp = pprint.PrettyPrinter(indent=3)
        pp.pprint(result)
    elif not get_verbosity().quiet:  # Se ho verbosity "normale" stampo solo i primi 3
        print("\n\n\n")
        print("Dispositivi installati dalla greedy (parziale):\n")
        pp = pprint.PrettyPrinter(indent=3)
        pp.pprint(dict(list(result.items())[:3]))
        print("\t\t.\n\t\t.\n\t\t.\n")


def print_mst_result(mst):
    if get_verbosity().verbose:
        print("\n\n\nArchi selezionati per il MST:\n")
        for edge in mst:
            print(f"{edge['node_one']} - {edge['node_two']} - Costo {edge['costo']}")
    elif not get_verbosity().quiet:  # Se ho verbosity "normale" stampo solo i primi 3
        print("\n\n\nArchi selezionati per il MST (parziale):\n")
        for edge in mst[:3]:
            print(f"{edge['node_one']} - {edge['node_two']} - Costo {edge['costo']}")
        print("\t.\n\t.\n\t.\n")


def prepara_cartelle_e_file(num_sensori, order_by, pack_by, num_iter, no_display):
    if not os.path.isdir("./solutions"):
        os.mkdir("./solutions")

    intestazione_csv = "seed,numsensori,order_by,pack_by,num_iter_ls," + \
                       "greedy_cost,mst_cost,first_tot,first_ls_tot,second_ls_tot," + \
                       "num_gw_class_1,fattore_riduzione"

    # Se viene passata l'opzione --no-display si aggiunge solamente il risultato
    # dell'esecuzione al file .csv (per analisi e creazione di grafici)
    if no_display:
        text_output_path_grafici = f"./solutions/graph_data.csv"

        if not os.path.isfile(text_output_path_grafici):
            with open(text_output_path_grafici, 'w') as f:
                original_stdout = sys.stdout
                sys.stdout = f
                print(intestazione_csv)
                sys.stdout = original_stdout

        return None, None, None, text_output_path_grafici

    saving_path = f"./solutions/{num_sensori}/{get_seed()}/{order_by}+{pack_by}+{num_iter}/"
    saving_path_ls = saving_path + "localsearch/"
    text_output_path = saving_path + "output.txt"
    text_output_path_grafici = f"./solutions/graph_data.csv"

    if not os.path.isdir(f"./solutions/{num_sensori}"):
        os.mkdir(f"./solutions/{num_sensori}")

    if not os.path.isdir(f"./solutions/{num_sensori}/{get_seed()}"):
        os.mkdir(f"./solutions/{num_sensori}/{get_seed()}")

    if not os.path.isdir(saving_path):
        os.mkdir(saving_path)

    if not os.path.isdir(saving_path_ls):
        os.mkdir(saving_path_ls)

    if os.path.isfile(text_output_path):
        os.remove(text_output_path)

    if not os.path.isfile(text_output_path_grafici):
        with open(text_output_path_grafici, 'w') as f:
            original_stdout = sys.stdout
            sys.stdout = f
            print(intestazione_csv)
            sys.stdout = original_stdout

    return saving_path, saving_path_ls, text_output_path, text_output_path_grafici


verbosity = Verbosity(False, False, False)


def get_verbosity():
    return verbosity


def set_verbosity(quiet=False, verbose=False, more_verbose=False):
    global verbosity
    verbosity = Verbosity(quiet, verbose, more_verbose)


random_seed = 12345  # Per la riproducibilit degli esempi
# Il seed originale  1625


def get_seed():
    return random_seed


def set_seed(new_seed):
    global random_seed
    random_seed = new_seed


gateway_classes = []


def get_gateways_classes():
    return gateway_classes


def set_gateways_classes(new_gateway_classes):
    global gateway_classes
    gateway_classes = new_gateway_classes


sensors = []


def get_global_sensors():
    return sensors


def set_global_sensors(new_sensors):
    global sensors
    sensors = new_sensors

from collections import namedtuple

from prettyparse import create_parser

from precise.network_runner import Listener
from precise.params import inject_params
from precise.train_data import TrainData
from select import select
from sys import stdin
import pygame
import time
import os
import shutil
from collections import Counter

usage = '''
    Retag false negatives as wakeword or not wakeword
    
    :model str
        Either Keras (.net) or TensorFlow (.pb) model to test
    
    :-t --use-train
        Evaluate training data instead of test data
    
    :-nf --no-filenames
        Don't print out the names of files that failed
    
    :-ap --append
        Append new tags file to old one
    ...
'''

Stats = namedtuple('Stats', 'false_pos false_neg true_pos true_neg')

def calc_stats(filenames, targets, predictions) -> Stats:
    stats = Stats([], [], [], [])
    for name, target, prediction in zip(filenames, targets, predictions):
        {
            (True, False): stats.false_pos,
            (True, True): stats.true_pos,
            (False, True): stats.false_neg,
            (False, False): stats.true_neg
        }[prediction[0] > 0.5, target[0] > 0.5].append(name)
    return stats

def main():
    args = TrainData.parse_args(create_parser(usage))

    inject_params(args.model)

    data = TrainData.from_both(args.tags_file, args.tags_folder, args.folder)
    train, test = data.load(args.use_train, not args.use_train)
    inputs, targets = train if args.use_train else test

    filenames = sum(data.train_files if args.use_train else data.test_files, [])
    predictions = Listener.find_runner(args.model)(args.model).predict(inputs)
    stats = calc_stats(filenames, targets, predictions)
    false_negatives_array = stats.false_neg
    new_tags = open('tags.txt', 'w')
    

    changed_tags_array = []
    for i in range(0, len(false_negatives_array)):
        pygame.mixer.init(frequency=8000, size=-16, channels=2, buffer=4096)
        pygame.mixer.music.load(false_negatives_array[i])
        pygame.mixer.music.play()
        print('False negative ', (i + 1), ' of ', (len(false_negatives_array)) + 1)
        user_input = input('Enter y if wakeword, enter n for not wakeword \n')
        time.sleep(5)
        false_negatives_array[i] = false_negatives_array[i].lstrip('/Users/madmitrienko/wakewords/files/')
        false_negatives_array[i] = false_negatives_array[i].rstrip('.wav')
        if(user_input == 'y'):
            write_to_tags = '\n' + false_negatives_array[i] + '	wake-word'
            new_tags.write(write_to_tags)

        elif(user_input == 'n'):
            write_to_tags = '\n' + false_negatives_array[i] + '	not-wake-word'          
            new_tags.write(write_to_tags)
    new_tags.close()
    tags.close()
    
if __name__ == '__main__':
    main()

import os
import signal
import subprocess
import json
import time
from datetime import datetime
import threading
import logging

import django

os.environ.setdefault("DJANGO_SETTINGS_MODULE", "gpu_tasker.settings")
django.setup()

from base.utils import get_admin_config
from task.models import GPUTask
from task.utils import run_task
from gpu_info.models import GPUServer
from gpu_info.utils import GPUInfoUpdater

task_logger = logging.getLogger('django.task')


if __name__ == '__main__':
    server_username, server_private_key_path = get_admin_config()

    gpu_updater = GPUInfoUpdater(server_username, server_private_key_path)
    while True:
        task_logger.info('Running processes: {:d}'.format(
            threading.active_count() - 1
        ))
        start_time = time.time()
        gpu_updater.update_gpu_info()
        for task in GPUTask.objects.filter(status=0):
            available_server = task.find_available_server()
            if available_server is not None:
                t = threading.Thread(target=run_task, args=(task, available_server))
                t.start()
                time.sleep(5)
        end_time = time.time()

        # 
        duration = end_time - start_time
        if duration < 10:
            time.sleep(10 - duration)

# Copyright 2021 UW-IT, University of Washington
# SPDX-License-Identifier: Apache-2.0

from uw_r25.models import Event, BindingReservation
from uw_r25 import nsmap, get_resource
from uw_r25.reservations import reservations_from_xml
from urllib.parse import urlencode, quote


def get_event_by_id(event_id):
    url = "event.xml?event_id={}".format(event_id)
    return events_from_xml(get_resource(url))[0]


def get_event_by_alien_id(alien_id):
    url = "event.xml?alien_uid={}".format(quote(alien_id))
    event = events_from_xml(get_resource(url))
    return event[0] if event else None


def get_events(**kwargs):
    """
    Return a list of events matching the passed filter.
    Supported kwargs are listed at
    http://knowledge25.collegenet.com/display/WSW/events.xml
    """
    url = "events.xml"
    if len(kwargs):
        url += "?{}".format(urlencode(kwargs))

    return events_from_xml(get_resource(url))


def events_from_xml(tree):
    events = []
    for node in tree.xpath("r25:event", namespaces=nsmap):
        event = Event()
        event.event_id = node.xpath("r25:event_id", namespaces=nsmap)[0].text
        event.alien_uid = node.xpath("r25:alien_uid", namespaces=nsmap)[0].text
        event.name = node.xpath("r25:event_name", namespaces=nsmap)[0].text
        event.title = node.xpath("r25:event_title", namespaces=nsmap)[0].text
        event.start_date = node.xpath("r25:start_date",
                                      namespaces=nsmap)[0].text
        event.end_date = node.xpath("r25:end_date", namespaces=nsmap)[0].text
        event.state = node.xpath("r25:state", namespaces=nsmap)[0].text
        event.parent_id = node.xpath("r25:parent_id", namespaces=nsmap)[0].text
        event.cabinet_id = node.xpath("r25:cabinet_id",
                                      namespaces=nsmap)[0].text
        event.cabinet_name = node.xpath("r25:cabinet_name",
                                        namespaces=nsmap)[0].text

        event.binding_reservations = []
        event.reservations = []
        for pnode in node.xpath("r25:profile", namespaces=nsmap):
            event.binding_reservations += binding_reservations_from_xml(pnode)
            event.reservations += reservations_from_xml(pnode)
        events.append(event)

    return events


def binding_reservations_from_xml(tree):
    binding_reservations = []
    for node in tree.xpath("//r25:binding_reservation", namespaces=nsmap):
        bind_res = BindingReservation()
        bind_res.bound_reservation_id = node.xpath("r25:bound_reservation_id",
                                                   namespaces=nsmap)[0].text
        bind_res.primary_reservation = node.xpath("r25:primary_reservation",
                                                  namespaces=nsmap)[0].text
        bind_res.name = node.xpath("r25:bound_name", namespaces=nsmap)[0].text
        bind_res.bound_event_id = node.xpath("r25:bound_event_id",
                                             namespaces=nsmap)[0].text
        binding_reservations.append(bind_res)

    return binding_reservations

# -*- coding: utf-8 -*-

# This code is part of Qiskit.
#
# (C) Copyright IBM 2017, 2019.
#
# This code is licensed under the Apache License, Version 2.0. You may
# obtain a copy of this license in the LICENSE.txt file in the root directory
# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
#
# Any modifications or derivative works of this code must retain this
# copyright notice, and modified files need to carry a notice indicating
# that they have been altered from the originals.

"""Assemble function for converting a list of circuits into a qobj"""
import uuid
import copy

from qiskit.circuit import QuantumCircuit
from qiskit.exceptions import QiskitError
from qiskit.pulse import ScheduleComponent, LoConfig
from qiskit.assembler.run_config import RunConfig
from qiskit.assembler import assemble_circuits, assemble_schedules
from qiskit.qobj import QobjHeader
from qiskit.validation.exceptions import ModelValidationError
from qiskit.qobj.utils import MeasLevel, MeasReturnType


# TODO: parallelize over the experiments (serialize each separately, then add global header/config)
def assemble(experiments,
             backend=None,
             qobj_id=None, qobj_header=None,
             shots=1024, memory=False, max_credits=None, seed_simulator=None,
             qubit_lo_freq=None, meas_lo_freq=None,
             qubit_lo_range=None, meas_lo_range=None,
             schedule_los=None, meas_level=MeasLevel.CLASSIFIED,
             meas_return=MeasReturnType.AVERAGE, meas_map=None,
             memory_slot_size=100, rep_time=None, parameter_binds=None,
             **run_config):
    """Assemble a list of circuits or pulse schedules into a Qobj.

    This function serializes the payloads, which could be either circuits or schedules,
    to create Qobj "experiments". It further annotates the experiment payload with
    header and configurations.

    Args:
        experiments (QuantumCircuit or list[QuantumCircuit] or Schedule or list[Schedule]):
            Circuit(s) or pulse schedule(s) to execute

        backend (BaseBackend):
            If set, some runtime options are automatically grabbed from
            backend.configuration() and backend.defaults().
            If any other option is explicitly set (e.g., rep_rate), it
            will override the backend's.
            If any other options is set in the run_config, it will
            also override the backend's.

        qobj_id (str):
            String identifier to annotate the Qobj

        qobj_header (QobjHeader or dict):
            User input that will be inserted in Qobj header, and will also be
            copied to the corresponding Result header. Headers do not affect the run.

        shots (int):
            Number of repetitions of each circuit, for sampling. Default: 1024

        memory (bool):
            If True, per-shot measurement bitstrings are returned as well
            (provided the backend supports it). For OpenPulse jobs, only
            measurement level 2 supports this option. Default: False

        max_credits (int):
            Maximum credits to spend on job. Default: 10

        seed_simulator (int):
            Random seed to control sampling, for when backend is a simulator

        qubit_lo_freq (list):
            List of default qubit LO frequencies in Hz. Will be overridden by
            `schedule_los` if set.

        meas_lo_freq (list):
            List of default measurement LO frequencies in Hz. Will be overridden
            by `schedule_los` if set.

        qubit_lo_range (list):
            List of drive LO ranges each of form `[range_min, range_max]` in Hz.
            Used to validate the supplied qubit frequencies.

        meas_lo_range (list):
            List of measurement LO ranges each of form `[range_min, range_max]` in Hz.
            Used to validate the supplied qubit frequencies.

        schedule_los (None or list[Union[Dict[PulseChannel, float], LoConfig]] or \
                      Union[Dict[PulseChannel, float], LoConfig]):
            Experiment LO configurations, frequencies are given in Hz.

        meas_level (int or MeasLevel):
            Set the appropriate level of the measurement output for pulse experiments.

        meas_return (str or MeasReturn):
            Level of measurement data for the backend to return.

            For `meas_level` 0 and 1:
                * "single" returns information from every shot.
                * "avg" returns average measurement output (averaged over number of shots).

        meas_map (list):
            List of lists, containing qubits that must be measured together.

        memory_slot_size (int):
            Size of each memory slot if the output is Level 0.

        rep_time (int): repetition time of the experiment in s.
            The delay between experiments will be rep_time.
            Must be from the list provided by the device.

        parameter_binds (list[dict{Parameter: Value}]):
            List of Parameter bindings over which the set of experiments will be
            executed. Each list element (bind) should be of the form
            {Parameter1: value1, Parameter2: value2, ...}. All binds will be
            executed across all experiments; e.g., if parameter_binds is a
            length-n list, and there are m experiments, a total of m x n
            experiments will be run (one for each experiment/bind pair).

        **run_config (dict):
            extra arguments used to configure the run (e.g., for Aer configurable
            backends). Refer to the backend documentation for details on these
            arguments.

    Returns:
            Qobj: a qobj that can be run on a backend. Depending on the type of input,
            this will be either a QasmQobj or a PulseQobj.

    Raises:
        QiskitError: if the input cannot be interpreted as either circuits or schedules
    """
    experiments = experiments if isinstance(experiments, list) else [experiments]
    qobj_id, qobj_header, run_config_common_dict = _parse_common_args(backend, qobj_id, qobj_header,
                                                                      shots, memory, max_credits,
                                                                      seed_simulator, **run_config)

    # assemble either circuits or schedules
    if all(isinstance(exp, QuantumCircuit) for exp in experiments):
        run_config = _parse_circuit_args(parameter_binds, **run_config_common_dict)

        # If circuits are parameterized, bind parameters and remove from run_config
        bound_experiments, run_config = _expand_parameters(circuits=experiments,
                                                           run_config=run_config)
        return assemble_circuits(circuits=bound_experiments, qobj_id=qobj_id,
                                 qobj_header=qobj_header, run_config=run_config)

    elif all(isinstance(exp, ScheduleComponent) for exp in experiments):
        run_config = _parse_pulse_args(backend, qubit_lo_freq, meas_lo_freq,
                                       qubit_lo_range, meas_lo_range,
                                       schedule_los, meas_level, meas_return,
                                       meas_map, memory_slot_size, rep_time,
                                       **run_config_common_dict)

        return assemble_schedules(schedules=experiments, qobj_id=qobj_id,
                                  qobj_header=qobj_header, run_config=run_config)

    else:
        raise QiskitError("bad input to assemble() function; "
                          "must be either circuits or schedules")


# TODO: rework to return a list of RunConfigs (one for each experiments), and a global one
def _parse_common_args(backend, qobj_id, qobj_header, shots,
                       memory, max_credits, seed_simulator,
                       **run_config):
    """Resolve the various types of args allowed to the assemble() function through
    duck typing, overriding args, etc. Refer to the assemble() docstring for details on
    what types of inputs are allowed.

    Here the args are resolved by converting them to standard instances, and prioritizing
    them in case a run option is passed through multiple args (explicitly setting an arg
    has more priority than the arg set by backend)

    Returns:
        RunConfig: a run config, which is a standardized object that configures the qobj
            and determines the runtime environment.

    Raises:
        QiskitError: if the memory arg is True and the backend does not support
        memory.
    """
    # grab relevant info from backend if it exists
    backend_config = None
    if backend:
        backend_config = backend.configuration()
        # check for memory flag applied to backend that does not support memory
        if memory and not backend_config.memory:
            raise QiskitError("memory not supported by backend {}"
                              .format(backend_config.backend_name))

    # an identifier for the Qobj
    qobj_id = qobj_id or str(uuid.uuid4())

    # The header that goes at the top of the Qobj (and later Result)
    # we process it as dict, then write entries that are not None to a QobjHeader object
    qobj_header = qobj_header or {}
    if isinstance(qobj_header, QobjHeader):
        qobj_header = qobj_header.to_dict()
    backend_name = getattr(backend_config, 'backend_name', None)
    backend_version = getattr(backend_config, 'backend_version', None)
    qobj_header = {**dict(backend_name=backend_name, backend_version=backend_version),
                   **qobj_header}
    qobj_header = QobjHeader(**{k: v for k, v in qobj_header.items() if v is not None})

    # create run configuration and populate
    run_config_dict = dict(shots=shots,
                           memory=memory,
                           max_credits=max_credits,
                           seed_simulator=seed_simulator,
                           **run_config)

    return qobj_id, qobj_header, run_config_dict


def _parse_pulse_args(backend, qubit_lo_freq, meas_lo_freq, qubit_lo_range,
                      meas_lo_range, schedule_los, meas_level,
                      meas_return, meas_map,
                      memory_slot_size, rep_time,
                      **run_config):
    """Build a pulse RunConfig replacing unset arguments with defaults derived from the `backend`.
    See `assemble` for more information on the required arguments.

    Returns:
        RunConfig: a run config, which is a standardized object that configures the qobj
            and determines the runtime environment.
    """
    # grab relevant info from backend if it exists
    backend_config = None
    backend_default = None
    if backend:
        backend_config = backend.configuration()
        # TODO : Remove usage of config.defaults when backend.defaults() is updated.
        try:
            backend_default = backend.defaults()
        except (ModelValidationError, AttributeError):
            from collections import namedtuple
            backend_config_defaults = getattr(backend_config, 'defaults', {})
            BackendDefault = namedtuple('BackendDefault', ('qubit_freq_est', 'meas_freq_est'))
            backend_default = BackendDefault(
                qubit_freq_est=backend_config_defaults.get('qubit_freq_est'),
                meas_freq_est=backend_config_defaults.get('meas_freq_est')
            )

    meas_map = meas_map or getattr(backend_config, 'meas_map', None)

    schedule_los = schedule_los or []
    if isinstance(schedule_los, (LoConfig, dict)):
        schedule_los = [schedule_los]

    # Convert to LoConfig if LO configuration supplied as dictionary
    schedule_los = [lo_config if isinstance(lo_config, LoConfig) else LoConfig(lo_config)
                    for lo_config in schedule_los]

    if not qubit_lo_freq and hasattr(backend_default, 'qubit_freq_est'):
        qubit_lo_freq = backend_default.qubit_freq_est
    if not meas_lo_freq and hasattr(backend_default, 'meas_freq_est'):
        meas_lo_freq = backend_default.meas_freq_est

    qubit_lo_range = qubit_lo_range or getattr(backend_config, 'qubit_lo_range', None)
    meas_lo_range = meas_lo_range or getattr(backend_config, 'meas_lo_range', None)

    rep_time = rep_time or getattr(backend_config, 'rep_times', None)
    if isinstance(rep_time, list):
        rep_time = rep_time[0]

    # create run configuration and populate
    run_config_dict = dict(qubit_lo_freq=qubit_lo_freq,
                           meas_lo_freq=meas_lo_freq,
                           qubit_lo_range=qubit_lo_range,
                           meas_lo_range=meas_lo_range,
                           schedule_los=schedule_los,
                           meas_level=meas_level,
                           meas_return=meas_return,
                           meas_map=meas_map,
                           memory_slot_size=memory_slot_size,
                           rep_time=rep_time,
                           **run_config)
    run_config = RunConfig(**{k: v for k, v in run_config_dict.items() if v is not None})

    return run_config


def _parse_circuit_args(parameter_binds, **run_config):
    """Build a circuit RunConfig replacing unset arguments with defaults derived from the `backend`.
    See `assemble` for more information on the required arguments.

    Returns:
        RunConfig: a run config, which is a standardized object that configures the qobj
            and determines the runtime environment.
    """
    parameter_binds = parameter_binds or []

    # create run configuration and populate
    run_config_dict = dict(parameter_binds=parameter_binds, **run_config)
    run_config = RunConfig(**{k: v for k, v in run_config_dict.items() if v is not None})

    return run_config


def _expand_parameters(circuits, run_config):
    """Verifies that there is a single common set of parameters shared between
    all circuits and all parameter binds in the run_config. Returns an expanded
    list of circuits (if parameterized) with all parameters bound, and a copy of
    the run_config with parameter_binds cleared.

    If neither the circuits nor the run_config specify parameters, the two are
    returned unmodified.

    Raises:
        QiskitError: if run_config parameters are not compatible with circuit parameters

    Returns:
        Tuple(List[QuantumCircuit], RunConfig):
          - List of input circuits expanded and with parameters bound
          - RunConfig with parameter_binds removed
    """

    parameter_binds = run_config.parameter_binds
    if parameter_binds or \
       any(circuit.parameters for circuit in circuits):

        all_bind_parameters = [bind.keys()
                               for bind in parameter_binds]
        all_circuit_parameters = [circuit.parameters for circuit in circuits]

        # Collect set of all unique parameters across all circuits and binds
        unique_parameters = {param
                             for param_list in all_bind_parameters + all_circuit_parameters
                             for param in param_list}

        # Check that all parameters are common to all circuits and binds
        if not all_bind_parameters \
           or not all_circuit_parameters \
           or any(unique_parameters != bind_params for bind_params in all_bind_parameters) \
           or any(unique_parameters != parameters for parameters in all_circuit_parameters):
            raise QiskitError(
                ('Mismatch between run_config.parameter_binds and all circuit parameters. ' +
                 'Parameter binds: {} ' +
                 'Circuit parameters: {}').format(all_bind_parameters, all_circuit_parameters))

        circuits = [circuit.bind_parameters(binds)
                    for circuit in circuits
                    for binds in parameter_binds]

        # All parameters have been expanded and bound, so remove from run_config
        run_config = copy.deepcopy(run_config)
        run_config.parameter_binds = []

    return circuits, run_config

"""
Tests for DragonflyBackend class.
"""

from tuun.backend import DragonflyBackend


def test_initialize():
    """Test initialize DragonflyBackend."""
    domain_config = {'name': 'real', 'min_max': [[-5, 5]]}
    opt_config = {'name': 'real'}
    dragonfly_config = {'acq_str': 'ucb-ei', 'n_init_rs': 0}
    db = DragonflyBackend(domain_config, opt_config, dragonfly_config)
    assert getattr(db, 'domain_config', None)

def test_suggest_to_minimize():
    """Test DragonflyBackend suggest_to_minimize on a dataset."""
    domain_config = {'name': 'real', 'min_max': [[0.0, 2.0]]}
    opt_config = {'name': 'real'}
    dragonfly_config = {'acq_str': 'ucb-ei', 'n_init_rs': 0}
    db = DragonflyBackend(domain_config, opt_config, dragonfly_config)

    data = {
        'x': [[0.5], [1.0], [1.5]],
        'y': [6.0, 1.0, 4.0],
    }

    suggestion = db.suggest_to_minimize(data)
    assert 0.75 < suggestion[0] < 1.25

"""Proto related build rules for fhir.
"""

load("@rules_proto//proto:defs.bzl", "proto_library")
load("@rules_cc//cc:defs.bzl", "cc_proto_library")
load("@io_bazel_rules_go//proto:def.bzl", "go_proto_library")
load("@com_google_protobuf//:protobuf.bzl", "py_proto_library")

WELL_KNOWN_PROTOS = ["descriptor_proto", "any_proto"]
GO_WELL_KNOWN_PROTOS = {
    "descriptor_proto": "@org_golang_google_protobuf//types/descriptorpb:go_default_library",
    "any_proto": "@org_golang_google_protobuf//types/known/anypb:go_default_library",
}

def fhir_proto_library(proto_library_prefix, srcs = [], proto_deps = [], **kwargs):
    """Generates proto_library target, as well as {py,cc,java,go}_proto_library targets.

    Args:
      proto_library_prefix: Name prefix to be added to various proto libraries.
      srcs: Srcs for the proto library.
      proto_deps: Deps by the proto_library.
      **kwargs: varargs. Passed through to proto rules.
    """
    py_deps = []
    cc_deps = []
    go_deps = []
    has_well_known_dep = False
    for x in proto_deps:
        tokens = x.split(":")
        if len(tokens) == 2 and tokens[1] in WELL_KNOWN_PROTOS:
            go_deps.append(GO_WELL_KNOWN_PROTOS[tokens[1]])
            if not has_well_known_dep:
                py_deps.append(tokens[0] + ":protobuf_python")
                cc_deps.append(tokens[0] + ":cc_wkt_protos")
                has_well_known_dep = True
        elif x.endswith("_proto"):
            py_deps.append(x[:-6] + "_py_pb2")
            cc_deps.append(x[:-6] + "_cc_proto")
            go_deps.append(x[:-6] + "_go_proto")

    proto_library(
        name = proto_library_prefix + "_proto",
        srcs = srcs,
        deps = proto_deps,
        **kwargs
    )

    py_proto_library(
        name = proto_library_prefix + "_py_pb2",
        srcs = srcs,
        deps = py_deps,
        default_runtime = "@com_google_protobuf//:protobuf_python",
        protoc = "@com_google_protobuf//:protoc",
        **kwargs
    )

    cc_proto_library(
        name = proto_library_prefix + "_cc_proto",
        deps = [proto_library_prefix + "_proto"],
    )

    native.java_proto_library(
        name = proto_library_prefix + "_java_proto",
        deps = [
            ":" + proto_library_prefix + "_proto",
        ],
        **kwargs
    )

    importpath_prefix = "github.com/google/fhir/go/"
    if native.package_name().startswith("go/"):
        importpath_prefix = "github.com/google/fhir/"

    go_proto_library(
        name = proto_library_prefix + "_go_proto",
        deps = go_deps,
        proto = ":" + proto_library_prefix + "_proto",
        importpath = importpath_prefix + native.package_name() + "/" + proto_library_prefix + "_go_proto",
        **kwargs
    )

def _fhir_individual_resource_rules(resource_files, deps):
    for resource_file in resource_files:
        fhir_proto_library(
            srcs = [resource_file],
            proto_deps = deps,
            proto_library_prefix = resource_file[:-6],
        )

def fhir_resource_rules(resource_files, deps):
    resource_files.remove("bundle_and_contained_resource.proto")

    _fhir_individual_resource_rules(resource_files, deps)

    resources_as_dep = [":" + file[:-6] + "_proto" for file in resource_files]

    fhir_proto_library(
        srcs = ["bundle_and_contained_resource.proto"],
        proto_deps = deps + resources_as_dep,
        proto_library_prefix = "bundle_and_contained_resource",
    )

def fhir_profile_rules(resource_files, deps):
    _fhir_individual_resource_rules(resource_files, deps)

# Author    : Andrzej Wojciechowski (AAWO)
# Copyright : Andrzej Wojciechowski (AAWO)
# --------------------------------------------
from sys import argv, stdout
from random import randrange

if len(argv) == 3:
   stdout.write(str(randrange(int(argv[1]), int(argv[2])+1)))
elif len(argv) == 4:
   stdout.write(str(randrange(int(argv[1]), int(argv[2])+1, int(argv[3]))))
else:
   argv_num = (len(argv)-1)
   raise TypeError("Wrong number of arguments. Expected 2 or 3 - received %d" % argv_num)

# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
import datetime
import json
import random

import pandas as pd
from sqlalchemy import Date, Float, String

from superset import db
from superset.models.dashboard import Dashboard
from superset.models.slice import Slice
from superset.utils import core as utils

from .helpers import (
    config,
    get_example_data,
    get_slice_json,
    merge_slice,
    TBL,
    update_slice_ids,
)


def load_unicode_test_data(only_metadata: bool = False, force: bool = False) -> None:
    """Loading unicode test dataset from a csv file in the repo"""
    tbl_name = "unicode_test"
    database = utils.get_example_database()
    table_exists = database.has_table_by_name(tbl_name)

    if not only_metadata and (not table_exists or force):
        data = get_example_data(
            "unicode_utf8_unixnl_test.csv", make_bytes=False
        )
        df = pd.read_csv(data, encoding="utf-8")
        # generate date/numeric data
        df["dttm"] = datetime.datetime.now().date()
        df["value"] = [random.randint(1, 100) for _ in range(len(df))]
        df.to_sql(  # pylint: disable=no-member
            tbl_name,
            database.get_sqla_engine(),
            if_exists="replace",
            chunksize=500,
            dtype={
                "phrase": String(500),
                "short_phrase": String(10),
                "with_missing": String(100),
                "dttm": Date(),
                "value": Float(),
            },
            index=False,
        )
        print("Done loading table!")
        print("-" * 80)

    print("Creating table [unicode_test] reference")
    obj = db.session.query(TBL).filter_by(table_name=tbl_name).first()
    if not obj:
        obj = TBL(table_name=tbl_name)
    obj.main_dttm_col = "dttm"
    obj.database = database
    db.session.merge(obj)
    db.session.commit()
    obj.fetch_metadata()
    tbl = obj

    slice_data = {
        "granularity_sqla": "dttm",
        "groupby": [],
        "metric": {
            "aggregate": "SUM",
            "column": {"column_name": "value"},
            "expressionType": "SIMPLE",
            "label": "Value",
        },
        "row_limit": config["ROW_LIMIT"],
        "since": "100 years ago",
        "until": "now",
        "viz_type": "word_cloud",
        "size_from": "10",
        "series": "short_phrase",
        "size_to": "70",
        "rotation": "square",
        "limit": "100",
    }

    print("Creating a slice")
    slc = Slice(
        slice_name="Unicode Cloud",
        viz_type="word_cloud",
        datasource_type="table",
        datasource_id=tbl.id,
        params=get_slice_json(slice_data),
    )
    merge_slice(slc)

    print("Creating a dashboard")
    dash = db.session.query(Dashboard).filter_by(slug="unicode-test").first()

    if not dash:
        dash = Dashboard()
    js = """\
{
    "CHART-Hkx6154FEm": {
        "children": [],
        "id": "CHART-Hkx6154FEm",
        "meta": {
            "chartId": 2225,
            "height": 30,
            "sliceName": "slice 1",
            "width": 4
        },
        "type": "CHART"
    },
    "GRID_ID": {
        "children": [
            "ROW-SyT19EFEQ"
        ],
        "id": "GRID_ID",
        "type": "GRID"
    },
    "ROOT_ID": {
        "children": [
            "GRID_ID"
        ],
        "id": "ROOT_ID",
        "type": "ROOT"
    },
    "ROW-SyT19EFEQ": {
        "children": [
            "CHART-Hkx6154FEm"
        ],
        "id": "ROW-SyT19EFEQ",
        "meta": {
            "background": "BACKGROUND_TRANSPARENT"
        },
        "type": "ROW"
    },
    "DASHBOARD_VERSION_KEY": "v2"
}
    """
    dash.dashboard_title = "Unicode Test"
    pos = json.loads(js)
    update_slice_ids(pos, [slc])
    dash.position_json = json.dumps(pos, indent=4)
    dash.slug = "unicode-test"
    dash.slices = [slc]
    db.session.merge(dash)
    db.session.commit()

class Solution:
    def minSwaps(self, nums: List[int]) -> int:
        ones, N = sum(nums), len(nums)
        min_swap = s = ones - sum(nums[:ones])
        for i in range(N):
            s += nums[i] - nums[(i + ones) % N]
            min_swap = min(s, min_swap)
        return min_swap
import radiate
import numpy as np
import os

# path to the sequence
root_path = 'data/radiate/'
sequence_name = 'tiny_foggy'

# time (s) to retrieve next frame
dt = 0.25

# load sequence
seq = radiate.Sequence(os.path.join(root_path, sequence_name))

# play sequence
for t in np.arange(seq.init_timestamp, seq.end_timestamp, dt):
    output = seq.get_from_timestamp(t)
    seq.vis_all(output, 0)
"""Creates a brownian tree"""
import random
import os
import numpy as np
from PIL import Image

random.seed()

class MyTuple():
    """Custom tuple with operator overloads"""
    def __init__(self, x, y):
        self.x_val = x
        self.y_val = y

    def __add__(self, rhs):
        return MyTuple(self.x_val + rhs.x_val, self.y_val + rhs.y_val)

    def __sub__(self, rhs):
        return MyTuple(self.x_val - rhs.x_val, self.y_val - rhs.y_val)

    def __lt__(self, rhs):
        return self.x_val < rhs.x_val and self.y_val < rhs.y_val

    def __gt__(self, rhs):
        return self.x_val > rhs.x_val and self.y_val > rhs.y_val

    def set_vals(self, x, y):
        """sets both values"""
        self.x_val = x
        self.y_val = y

    def get_vals(self):
        """casts to a normal tuple"""
        return (self.x_val, self.y_val)


class Grid():
    """Stores the data about the grid and tree"""
    def __init__(self, size):
        """grid constructor"""
        self.size = size
        self.data = np.zeros(self.size.get_vals(), dtype=np.uint8)
        self.max_steps = 0

    def get_val(self, location):
        """returns the cell value at that location"""
        return self.data[location.x_val, location.y_val]

    def is_occupied(self, location):
        """returns True if the location is occupied"""
        return self.data[location.x_val, location.y_val]

    def set_val(self, location, steps):
        """sets the value at that location"""
        if self.max_steps < steps:
            self.max_steps = steps
        self.data[location.x_val, location.y_val] = 255

    def is_outside_bounds(self, location):
        """Checks if the location supplied is inside the grid"""
        lower = location > MyTuple(-1, -1)
        upper = location < self.size
        return upper and lower

    def normalise(self):
        max = self.max_steps
        def normalise(data):
            return 255 - (data / (self.max_steps / 255))
        np.vectorize(normalise)(self.data)

BOUNDS = MyTuple(100, 75)
# BOUNDS = MyTuple(160, 120)

LEFT = MyTuple(-1, 0)
UP = MyTuple(0, 1)
RIGHT = MyTuple(1, 0)
DOWN = MyTuple(0, -1)
NONE = MyTuple(0, 0)
DIRECTIONS = [UP, DOWN, LEFT, RIGHT]


class Particle():
    """Stores data about individual particles"""
    def __init__(self, direction, location):
        """particle constructor"""
        self.direction = direction
        self.location = location
        self.is_free = True

    def move(self, new_direction):
        """moves particle in a direction"""
        self.direction = new_direction
        self.location += new_direction

    def fix_location(self):
        """sets the location of said particle"""
        self.direction = NONE
        self.is_free = False


def random_xy_loc(bounds):
    """Creates a random xy tuple within bounds"""
    return MyTuple(random.randint(0, bounds.x_val - 1), random.randint(0, bounds.y_val - 1))


def get_next_direction():
    return random.choice(DIRECTIONS)


def main():
    """main init"""
    data_grid = Grid(BOUNDS)

    max_particles = int(BOUNDS.x_val * BOUNDS.y_val / 3)

    for y in range(BOUNDS.y_val):
        for x in range(BOUNDS.x_val):
            if x == 0 or x == (BOUNDS.x_val - 1) or y == 0 or y == (BOUNDS.y_val - 1):
                data_grid.set_val(MyTuple(x, y), 1)

    data_grid.set_val(MyTuple(int(BOUNDS.x_val / 2), int(BOUNDS.y_val / 2)), 1)

    is_occupied = data_grid.is_occupied
    is_outside_bounds = data_grid.is_outside_bounds

    for i in range(max_particles):
        count = -1
        location = random_xy_loc(BOUNDS)
        while is_occupied(location):
            location = random_xy_loc(BOUNDS)
        new_particle = Particle(direction=NONE, location=location)
        while new_particle.is_free:
            count += 1
            direction = get_next_direction()
            new_location = new_particle.location + direction
            if not is_outside_bounds(new_location):
                continue
            if not is_occupied(new_location):
                new_particle.move(direction)
            else:
                new_particle.fix_location()
        data_grid.set_val(new_particle.location, count)
        print(f"Finished {i + 1}, loops: {count}")
    output_name = f"time_data_{max_particles}_{BOUNDS.get_vals()}.bmp"
    full_path = os.path.join(os.path.dirname(__file__), output_name)
    Image.fromarray(data_grid.data).save(full_path)


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
# Copyright (c) 2014-2017 The Bitcoin Core developers
# Distributed under the MIT software license, see the accompanying
# file COPYING or http://www.opensource.org/licenses/mit-license.php.
"""Test running bitcoind with -reindex and -reindex-chainstate options.

- Start a single node and generate 3 blocks.
- Stop the node and restart it with -reindex. Verify that the node has reindexed up to block 3.
- Stop the node and restart it with -reindex-chainstate. Verify that the node has reindexed up to block 3.
"""

from test_framework.test_framework import UnoTestFramework
from test_framework.util import assert_equal
import time

class ReindexTest(UnoTestFramework):

    def set_test_params(self):
        self.setup_clean_chain = True
        self.num_nodes = 1

    def reindex(self):
        self.nodes[0].generate(3)
        blockcount = self.nodes[0].getblockcount()
        self.stop_nodes()
        extra_args = [["-reindex", "-checkblockindex=1"]]
        self.start_nodes(extra_args)
        assert_equal(self.nodes[0].getblockcount(), blockcount)  # start_node is blocking on reindex
        self.log.info("Success")

    def run_test(self):
        self.reindex()

if __name__ == '__main__':
    ReindexTest().main()

from .entry_storage import EntryOperations
from .functions import batch_list

import datetime
import logging
import multiprocessing
import os
import shutil
from mimetypes import guess_type
from typing import Any, Dict, Iterable, List, Optional, Tuple

import orjson
from bs4 import BeautifulSoup
from django.conf import settings
from django.core.cache import cache
from django.db import connection
from django.utils.timezone import now as timezone_now
from psycopg2.extras import execute_values
from psycopg2.sql import SQL, Identifier

from analytics.models import RealmCount, StreamCount, UserCount
from zerver.lib.actions import (
    UserMessageLite,
    bulk_insert_ums,
    do_change_avatar_fields,
    do_change_plan_type,
)
from zerver.lib.avatar_hash import user_avatar_path_from_ids
from zerver.lib.bulk_create import bulk_create_users, bulk_set_users_or_streams_recipient_fields
from zerver.lib.export import DATE_FIELDS, Field, Path, Record, TableData, TableName
from zerver.lib.markdown import markdown_convert
from zerver.lib.markdown import version as markdown_version
from zerver.lib.message import get_last_message_id
from zerver.lib.server_initialization import create_internal_realm, server_initialized
from zerver.lib.streams import render_stream_description
from zerver.lib.timestamp import datetime_to_timestamp
from zerver.lib.upload import BadImageError, get_bucket, sanitize_name, upload_backend
from zerver.lib.utils import generate_api_key, process_list_in_batches
from zerver.models import (
    AlertWord,
    Attachment,
    BotConfigData,
    BotStorageData,
    Client,
    CustomProfileField,
    CustomProfileFieldValue,
    DefaultStream,
    Huddle,
    Message,
    MutedUser,
    Reaction,
    Realm,
    RealmAuditLog,
    RealmDomain,
    RealmEmoji,
    RealmFilter,
    RealmPlayground,
    RealmUserDefault,
    Recipient,
    Service,
    Stream,
    Subscription,
    UserActivity,
    UserActivityInterval,
    UserGroup,
    UserGroupMembership,
    UserHotspot,
    UserMessage,
    UserPresence,
    UserProfile,
    UserTopic,
    get_huddle_hash,
    get_realm,
    get_system_bot,
    get_user_profile_by_id,
)

realm_tables = [
    ("zerver_defaultstream", DefaultStream, "defaultstream"),
    ("zerver_realmemoji", RealmEmoji, "realmemoji"),
    ("zerver_realmdomain", RealmDomain, "realmdomain"),
    ("zerver_realmfilter", RealmFilter, "realmfilter"),
    ("zerver_realmplayground", RealmPlayground, "realmplayground"),
]  # List[Tuple[TableName, Any, str]]


# ID_MAP is a dictionary that maps table names to dictionaries
# that map old ids to new ids.  We use this in
# re_map_foreign_keys and other places.
#
# We explicitly initialize ID_MAP with the tables that support
# id re-mapping.
#
# Code reviewers: give these tables extra scrutiny, as we need to
# make sure to reload related tables AFTER we re-map the ids.
ID_MAP: Dict[str, Dict[int, int]] = {
    "alertword": {},
    "client": {},
    "user_profile": {},
    "huddle": {},
    "realm": {},
    "stream": {},
    "recipient": {},
    "subscription": {},
    "defaultstream": {},
    "reaction": {},
    "realmemoji": {},
    "realmdomain": {},
    "realmfilter": {},
    "realmplayground": {},
    "message": {},
    "user_presence": {},
    "useractivity": {},
    "useractivityinterval": {},
    "usermessage": {},
    "customprofilefield": {},
    "customprofilefieldvalue": {},
    "attachment": {},
    "realmauditlog": {},
    "recipient_to_huddle_map": {},
    "userhotspot": {},
    "mutedtopic": {},
    "muteduser": {},
    "service": {},
    "usergroup": {},
    "usergroupmembership": {},
    "botstoragedata": {},
    "botconfigdata": {},
    "analytics_realmcount": {},
    "analytics_streamcount": {},
    "analytics_usercount": {},
    "realmuserdefault": {},
}

id_map_to_list: Dict[str, Dict[int, List[int]]] = {
    "huddle_to_user_list": {},
}

path_maps: Dict[str, Dict[str, str]] = {
    "attachment_path": {},
}


def update_id_map(table: TableName, old_id: int, new_id: int) -> None:
    if table not in ID_MAP:
        raise Exception(
            f"""
            Table {table} is not initialized in ID_MAP, which could
            mean that we have not thought through circular
            dependencies.
            """
        )
    ID_MAP[table][old_id] = new_id


def fix_datetime_fields(data: TableData, table: TableName) -> None:
    for item in data[table]:
        for field_name in DATE_FIELDS[table]:
            if item[field_name] is not None:
                item[field_name] = datetime.datetime.fromtimestamp(
                    item[field_name], tz=datetime.timezone.utc
                )


def fix_upload_links(data: TableData, message_table: TableName) -> None:
    """
    Because the URLs for uploaded files encode the realm ID of the
    organization being imported (which is only determined at import
    time), we need to rewrite the URLs of links to uploaded files
    during the import process.
    """
    for message in data[message_table]:
        if message["has_attachment"] is True:
            for key, value in path_maps["attachment_path"].items():
                if key in message["content"]:
                    message["content"] = message["content"].replace(key, value)
                    if message["rendered_content"]:
                        message["rendered_content"] = message["rendered_content"].replace(
                            key, value
                        )


def create_subscription_events(data: TableData, realm_id: int) -> None:
    """
    When the export data doesn't contain the table `zerver_realmauditlog`,
    this function creates RealmAuditLog objects for `subscription_created`
    type event for all the existing Stream subscriptions.

    This is needed for all the export tools which do not include the
    table `zerver_realmauditlog` (Slack, Gitter, etc.) because the appropriate
    data about when a user was subscribed is not exported by the third-party
    service.
    """
    all_subscription_logs = []

    event_last_message_id = get_last_message_id()
    event_time = timezone_now()

    recipient_id_to_stream_id = {
        d["id"]: d["type_id"] for d in data["zerver_recipient"] if d["type"] == Recipient.STREAM
    }

    for sub in data["zerver_subscription"]:
        recipient_id = sub["recipient_id"]
        stream_id = recipient_id_to_stream_id.get(recipient_id)

        if stream_id is None:
            continue

        user_id = sub["user_profile_id"]

        all_subscription_logs.append(
            RealmAuditLog(
                realm_id=realm_id,
                acting_user_id=user_id,
                modified_user_id=user_id,
                modified_stream_id=stream_id,
                event_last_message_id=event_last_message_id,
                event_time=event_time,
                event_type=RealmAuditLog.SUBSCRIPTION_CREATED,
            )
        )
    RealmAuditLog.objects.bulk_create(all_subscription_logs)


def fix_service_tokens(data: TableData, table: TableName) -> None:
    """
    The tokens in the services are created by 'generate_api_key'.
    As the tokens are unique, they should be re-created for the imports.
    """
    for item in data[table]:
        item["token"] = generate_api_key()


def process_huddle_hash(data: TableData, table: TableName) -> None:
    """
    Build new huddle hashes with the updated ids of the users
    """
    for huddle in data[table]:
        user_id_list = id_map_to_list["huddle_to_user_list"][huddle["id"]]
        huddle["huddle_hash"] = get_huddle_hash(user_id_list)


def get_huddles_from_subscription(data: TableData, table: TableName) -> None:
    """
    Extract the IDs of the user_profiles involved in a huddle from the subscription object
    This helps to generate a unique huddle hash from the updated user_profile ids
    """
    id_map_to_list["huddle_to_user_list"] = {
        value: [] for value in ID_MAP["recipient_to_huddle_map"].values()
    }

    for subscription in data[table]:
        if subscription["recipient"] in ID_MAP["recipient_to_huddle_map"]:
            huddle_id = ID_MAP["recipient_to_huddle_map"][subscription["recipient"]]
            id_map_to_list["huddle_to_user_list"][huddle_id].append(subscription["user_profile_id"])


def fix_customprofilefield(data: TableData) -> None:
    """
    In CustomProfileField with 'field_type' like 'USER', the IDs need to be
    re-mapped.
    """
    field_type_USER_id_list = []
    for item in data["zerver_customprofilefield"]:
        if item["field_type"] == CustomProfileField.USER:
            field_type_USER_id_list.append(item["id"])

    for item in data["zerver_customprofilefieldvalue"]:
        if item["field_id"] in field_type_USER_id_list:
            old_user_id_list = orjson.loads(item["value"])

            new_id_list = re_map_foreign_keys_many_to_many_internal(
                table="zerver_customprofilefieldvalue",
                field_name="value",
                related_table="user_profile",
                old_id_list=old_user_id_list,
            )
            item["value"] = orjson.dumps(new_id_list).decode()


def fix_message_rendered_content(
    realm: Realm, sender_map: Dict[int, Record], messages: List[Record]
) -> None:
    """
    This function sets the rendered_content of all the messages
    after the messages have been imported from a non-Zulip platform.
    """
    for message in messages:
        if message["rendered_content"] is not None:
            # For Zulip->Zulip imports, we use the original rendered
            # Markdown; this avoids issues where e.g. a mention can no
            # longer render properly because a user has changed their
            # name.
            #
            # However, we still need to update the data-user-id and
            # similar values stored on mentions, stream mentions, and
            # similar syntax in the rendered HTML.
            soup = BeautifulSoup(message["rendered_content"], "html.parser")

            user_mentions = soup.findAll("span", {"class": "user-mention"})
            if len(user_mentions) != 0:
                user_id_map = ID_MAP["user_profile"]
                for mention in user_mentions:
                    if not mention.has_attr("data-user-id"):
                        # Legacy mentions don't have a data-user-id
                        # field; we should just import them
                        # unmodified.
                        continue
                    if mention["data-user-id"] == "*":
                        # No rewriting is required for wildcard mentions
                        continue
                    old_user_id = int(mention["data-user-id"])
                    if old_user_id in user_id_map:
                        mention["data-user-id"] = str(user_id_map[old_user_id])
                message["rendered_content"] = str(soup)

            stream_mentions = soup.findAll("a", {"class": "stream"})
            if len(stream_mentions) != 0:
                stream_id_map = ID_MAP["stream"]
                for mention in stream_mentions:
                    old_stream_id = int(mention["data-stream-id"])
                    if old_stream_id in stream_id_map:
                        mention["data-stream-id"] = str(stream_id_map[old_stream_id])
                message["rendered_content"] = str(soup)

            user_group_mentions = soup.findAll("span", {"class": "user-group-mention"})
            if len(user_group_mentions) != 0:
                user_group_id_map = ID_MAP["usergroup"]
                for mention in user_group_mentions:
                    old_user_group_id = int(mention["data-user-group-id"])
                    if old_user_group_id in user_group_id_map:
                        mention["data-user-group-id"] = str(user_group_id_map[old_user_group_id])
                message["rendered_content"] = str(soup)
            continue

        try:
            content = message["content"]

            sender_id = message["sender_id"]
            sender = sender_map[sender_id]
            sent_by_bot = sender["is_bot"]
            translate_emoticons = sender["translate_emoticons"]

            # We don't handle alert words on import from third-party
            # platforms, since they generally don't have an "alert
            # words" type feature, and notifications aren't important anyway.
            realm_alert_words_automaton = None

            rendered_content = markdown_convert(
                content=content,
                realm_alert_words_automaton=realm_alert_words_automaton,
                message_realm=realm,
                sent_by_bot=sent_by_bot,
                translate_emoticons=translate_emoticons,
            ).rendered_content

            message["rendered_content"] = rendered_content
            message["rendered_content_version"] = markdown_version
        except Exception:
            # This generally happens with two possible causes:
            # * rendering Markdown throwing an uncaught exception
            # * rendering Markdown failing with the exception being
            #   caught in Markdown (which then returns None, causing the the
            #   rendered_content assert above to fire).
            logging.warning(
                "Error in Markdown rendering for message ID %s; continuing", message["id"]
            )


def current_table_ids(data: TableData, table: TableName) -> List[int]:
    """
    Returns the ids present in the current table
    """
    id_list = []
    for item in data[table]:
        id_list.append(item["id"])
    return id_list


def idseq(model_class: Any) -> str:
    if model_class == RealmDomain:
        return "zerver_realmalias_id_seq"
    elif model_class == BotStorageData:
        return "zerver_botuserstatedata_id_seq"
    elif model_class == BotConfigData:
        return "zerver_botuserconfigdata_id_seq"
    return f"{model_class._meta.db_table}_id_seq"


def allocate_ids(model_class: Any, count: int) -> List[int]:
    """
    Increases the sequence number for a given table by the amount of objects being
    imported into that table. Hence, this gives a reserved range of IDs to import the
    converted Slack objects into the tables.
    """
    conn = connection.cursor()
    sequence = idseq(model_class)
    conn.execute("select nextval(%s) from generate_series(1, %s)", [sequence, count])
    query = conn.fetchall()  # Each element in the result is a tuple like (5,)
    conn.close()
    # convert List[Tuple[int]] to List[int]
    return [item[0] for item in query]


def convert_to_id_fields(data: TableData, table: TableName, field_name: Field) -> None:
    """
    When Django gives us dict objects via model_to_dict, the foreign
    key fields are `foo`, but we want `foo_id` for the bulk insert.
    This function handles the simple case where we simply rename
    the fields.  For cases where we need to munge ids in the
    database, see re_map_foreign_keys.
    """
    for item in data[table]:
        item[field_name + "_id"] = item[field_name]
        del item[field_name]


def re_map_foreign_keys(
    data: TableData,
    table: TableName,
    field_name: Field,
    related_table: TableName,
    verbose: bool = False,
    id_field: bool = False,
    recipient_field: bool = False,
    reaction_field: bool = False,
) -> None:
    """
    This is a wrapper function for all the realm data tables
    and only avatar and attachment records need to be passed through the internal function
    because of the difference in data format (TableData corresponding to realm data tables
    and List[Record] corresponding to the avatar and attachment records)
    """

    # See comments in bulk_import_user_message_data.
    assert "usermessage" not in related_table

    re_map_foreign_keys_internal(
        data[table],
        table,
        field_name,
        related_table,
        verbose,
        id_field,
        recipient_field,
        reaction_field,
    )


def re_map_foreign_keys_internal(
    data_table: List[Record],
    table: TableName,
    field_name: Field,
    related_table: TableName,
    verbose: bool = False,
    id_field: bool = False,
    recipient_field: bool = False,
    reaction_field: bool = False,
) -> None:
    """
    We occasionally need to assign new ids to rows during the
    import/export process, to accommodate things like existing rows
    already being in tables.  See bulk_import_client for more context.

    The tricky part is making sure that foreign key references
    are in sync with the new ids, and this fixer function does
    the re-mapping.  (It also appends `_id` to the field.)
    """
    lookup_table = ID_MAP[related_table]
    for item in data_table:
        old_id = item[field_name]
        if recipient_field:
            if related_table == "stream" and item["type"] == 2:
                pass
            elif related_table == "user_profile" and item["type"] == 1:
                pass
            elif related_table == "huddle" and item["type"] == 3:
                # save the recipient id with the huddle id, so that we can extract
                # the user_profile ids involved in a huddle with the help of the
                # subscription object
                # check function 'get_huddles_from_subscription'
                ID_MAP["recipient_to_huddle_map"][item["id"]] = lookup_table[old_id]
            else:
                continue
        old_id = item[field_name]
        if reaction_field:
            if item["reaction_type"] == Reaction.REALM_EMOJI:
                old_id = int(old_id)
            else:
                continue
        if old_id in lookup_table:
            new_id = lookup_table[old_id]
            if verbose:
                logging.info(
                    "Remapping %s %s from %s to %s", table, field_name + "_id", old_id, new_id
                )
        else:
            new_id = old_id
        if not id_field:
            item[field_name + "_id"] = new_id
            del item[field_name]
        else:
            if reaction_field:
                item[field_name] = str(new_id)
            else:
                item[field_name] = new_id


def re_map_foreign_keys_many_to_many(
    data: TableData,
    table: TableName,
    field_name: Field,
    related_table: TableName,
    verbose: bool = False,
) -> None:
    """
    We need to assign new ids to rows during the import/export
    process.

    The tricky part is making sure that foreign key references
    are in sync with the new ids, and this wrapper function does
    the re-mapping only for ManyToMany fields.
    """
    for item in data[table]:
        old_id_list = item[field_name]
        new_id_list = re_map_foreign_keys_many_to_many_internal(
            table, field_name, related_table, old_id_list, verbose
        )
        item[field_name] = new_id_list
        del item[field_name]


def re_map_foreign_keys_many_to_many_internal(
    table: TableName,
    field_name: Field,
    related_table: TableName,
    old_id_list: List[int],
    verbose: bool = False,
) -> List[int]:
    """
    This is an internal function for tables with ManyToMany fields,
    which takes the old ID list of the ManyToMany relation and returns the
    new updated ID list.
    """
    lookup_table = ID_MAP[related_table]
    new_id_list = []
    for old_id in old_id_list:
        if old_id in lookup_table:
            new_id = lookup_table[old_id]
            if verbose:
                logging.info(
                    "Remapping %s %s from %s to %s", table, field_name + "_id", old_id, new_id
                )
        else:
            new_id = old_id
        new_id_list.append(new_id)
    return new_id_list


def fix_bitfield_keys(data: TableData, table: TableName, field_name: Field) -> None:
    for item in data[table]:
        item[field_name] = item[field_name + "_mask"]
        del item[field_name + "_mask"]


def fix_realm_authentication_bitfield(data: TableData, table: TableName, field_name: Field) -> None:
    """Used to fixup the authentication_methods bitfield to be a string"""
    for item in data[table]:
        values_as_bitstring = "".join("1" if field[1] else "0" for field in item[field_name])
        values_as_int = int(values_as_bitstring, 2)
        item[field_name] = values_as_int


def remove_denormalized_recipient_column_from_data(data: TableData) -> None:
    """
    The recipient column shouldn't be imported, we'll set the correct values
    when Recipient table gets imported.
    """
    for stream_dict in data["zerver_stream"]:
        if "recipient" in stream_dict:
            del stream_dict["recipient"]

    for user_profile_dict in data["zerver_userprofile"]:
        if "recipient" in user_profile_dict:
            del user_profile_dict["recipient"]

    for huddle_dict in data["zerver_huddle"]:
        if "recipient" in huddle_dict:
            del huddle_dict["recipient"]


def get_db_table(model_class: Any) -> str:
    """E.g. (RealmDomain -> 'zerver_realmdomain')"""
    return model_class._meta.db_table


def update_model_ids(model: Any, data: TableData, related_table: TableName) -> None:
    table = get_db_table(model)

    # Important: remapping usermessage rows is
    # not only unnessary, it's expensive and can cause
    # memory errors. We don't even use ids from ID_MAP.
    assert "usermessage" not in table

    old_id_list = current_table_ids(data, table)
    allocated_id_list = allocate_ids(model, len(data[table]))
    for item in range(len(data[table])):
        update_id_map(related_table, old_id_list[item], allocated_id_list[item])
    re_map_foreign_keys(data, table, "id", related_table=related_table, id_field=True)


def bulk_import_user_message_data(data: TableData, dump_file_id: int) -> None:
    model = UserMessage
    table = "zerver_usermessage"
    lst = data[table]

    # IMPORTANT NOTE: We do not use any primary id
    # data from either the import itself or ID_MAP.
    # We let the DB itself generate ids.  Note that
    # no tables use user_message.id as a foreign key,
    # so we can safely avoid all re-mapping complexity.

    def process_batch(items: List[Dict[str, Any]]) -> None:
        ums = [
            UserMessageLite(
                user_profile_id=item["user_profile_id"],
                message_id=item["message_id"],
                flags=item["flags"],
            )
            for item in items
        ]
        bulk_insert_ums(ums)

    chunk_size = 10000

    process_list_in_batches(
        lst=lst,
        chunk_size=chunk_size,
        process_batch=process_batch,
    )

    logging.info("Successfully imported %s from %s[%s].", model, table, dump_file_id)


def bulk_import_model(data: TableData, model: Any, dump_file_id: Optional[str] = None) -> None:
    table = get_db_table(model)
    # TODO, deprecate dump_file_id
    model.objects.bulk_create(model(**item) for item in data[table])
    if dump_file_id is None:
        logging.info("Successfully imported %s from %s.", model, table)
    else:
        logging.info("Successfully imported %s from %s[%s].", model, table, dump_file_id)


# Client is a table shared by multiple realms, so in order to
# correctly import multiple realms into the same server, we need to
# check if a Client object already exists, and so we need to support
# remap all Client IDs to the values in the new DB.
def bulk_import_client(data: TableData, model: Any, table: TableName) -> None:
    for item in data[table]:
        try:
            client = Client.objects.get(name=item["name"])
        except Client.DoesNotExist:
            client = Client.objects.create(name=item["name"])
        update_id_map(table="client", old_id=item["id"], new_id=client.id)


def fix_subscriptions_is_user_active_column(
    data: TableData, user_profiles: List[UserProfile]
) -> None:
    table = get_db_table(Subscription)
    user_id_to_active_status = {user.id: user.is_active for user in user_profiles}
    for sub in data[table]:
        sub["is_user_active"] = user_id_to_active_status[sub["user_profile_id"]]


def process_avatars(record: Dict[str, Any]) -> None:
    # We need to re-import upload_backend here, because in the
    # import-export unit tests, the Zulip settings are overridden for
    # specific tests to control the choice of upload backend, and this
    # reimport ensures that we use the right choice for the current
    # test. Outside the test suite, settings never change after the
    # server is started, so this import will have no effect in production.
    from zerver.lib.upload import upload_backend

    if record["s3_path"].endswith(".original"):
        user_profile = get_user_profile_by_id(record["user_profile_id"])
        if settings.LOCAL_UPLOADS_DIR is not None:
            avatar_path = user_avatar_path_from_ids(user_profile.id, record["realm_id"])
            medium_file_path = (
                os.path.join(settings.LOCAL_UPLOADS_DIR, "avatars", avatar_path) + "-medium.png"
            )
            if os.path.exists(medium_file_path):
                # We remove the image here primarily to deal with
                # issues when running the import script multiple
                # times in development (where one might reuse the
                # same realm ID from a previous iteration).
                os.remove(medium_file_path)
        try:
            upload_backend.ensure_avatar_image(user_profile=user_profile, is_medium=True)
            if record.get("importer_should_thumbnail"):
                upload_backend.ensure_avatar_image(user_profile=user_profile)
        except BadImageError:
            logging.warning(
                "Could not thumbnail avatar image for user %s; ignoring",
                user_profile.id,
            )
            # Delete the record of the avatar to avoid 404s.
            do_change_avatar_fields(
                user_profile, UserProfile.AVATAR_FROM_GRAVATAR, acting_user=None
            )


def import_uploads(
    realm: Realm,
    import_dir: Path,
    processes: int,
    processing_avatars: bool = False,
    processing_emojis: bool = False,
    processing_realm_icons: bool = False,
) -> None:
    if processing_avatars and processing_emojis:
        raise AssertionError("Cannot import avatars and emojis at the same time!")
    if processing_avatars:
        logging.info("Importing avatars")
    elif processing_emojis:
        logging.info("Importing emojis")
    elif processing_realm_icons:
        logging.info("Importing realm icons and logos")
    else:
        logging.info("Importing uploaded files")

    records_filename = os.path.join(import_dir, "records.json")
    with open(records_filename, "rb") as records_file:
        records: List[Dict[str, Any]] = orjson.loads(records_file.read())
    timestamp = datetime_to_timestamp(timezone_now())

    re_map_foreign_keys_internal(
        records, "records", "realm_id", related_table="realm", id_field=True
    )
    if not processing_emojis and not processing_realm_icons:
        re_map_foreign_keys_internal(
            records, "records", "user_profile_id", related_table="user_profile", id_field=True
        )

    s3_uploads = settings.LOCAL_UPLOADS_DIR is None

    if s3_uploads:
        if processing_avatars or processing_emojis or processing_realm_icons:
            bucket_name = settings.S3_AVATAR_BUCKET
        else:
            bucket_name = settings.S3_AUTH_UPLOADS_BUCKET
        bucket = get_bucket(bucket_name)

    count = 0
    for record in records:
        count += 1
        if count % 1000 == 0:
            logging.info("Processed %s/%s uploads", count, len(records))

        if processing_avatars:
            # For avatars, we need to rehash the user ID with the
            # new server's avatar salt
            relative_path = user_avatar_path_from_ids(record["user_profile_id"], record["realm_id"])
            if record["s3_path"].endswith(".original"):
                relative_path += ".original"
            else:
                # TODO: This really should be unconditional.  However,
                # until we fix the S3 upload backend to use the .png
                # path suffix for its normal avatar URLs, we need to
                # only do this for the LOCAL_UPLOADS_DIR backend.
                if not s3_uploads:
                    relative_path += ".png"
        elif processing_emojis:
            # For emojis we follow the function 'upload_emoji_image'
            relative_path = RealmEmoji.PATH_ID_TEMPLATE.format(
                realm_id=record["realm_id"], emoji_file_name=record["file_name"]
            )
            record["last_modified"] = timestamp
        elif processing_realm_icons:
            icon_name = os.path.basename(record["path"])
            relative_path = os.path.join(str(record["realm_id"]), "realm", icon_name)
            record["last_modified"] = timestamp
        else:
            # This relative_path is basically the new location of the file,
            # which will later be copied from its original location as
            # specified in record["s3_path"].
            relative_path = upload_backend.generate_message_upload_path(
                str(record["realm_id"]), sanitize_name(os.path.basename(record["path"]))
            )
            path_maps["attachment_path"][record["s3_path"]] = relative_path

        if s3_uploads:
            key = bucket.Object(relative_path)
            metadata = {}
            if processing_emojis and "user_profile_id" not in record:
                # Exported custom emoji from tools like Slack don't have
                # the data for what user uploaded them in `user_profile_id`.
                pass
            elif processing_realm_icons and "user_profile_id" not in record:
                # Exported realm icons and logos from local export don't have
                # the value of user_profile_id in the associated record.
                pass
            else:
                user_profile_id = int(record["user_profile_id"])
                # Support email gateway bot and other cross-realm messages
                if user_profile_id in ID_MAP["user_profile"]:
                    logging.info("Uploaded by ID mapped user: %s!", user_profile_id)
                    user_profile_id = ID_MAP["user_profile"][user_profile_id]
                user_profile = get_user_profile_by_id(user_profile_id)
                metadata["user_profile_id"] = str(user_profile.id)

            if "last_modified" in record:
                metadata["orig_last_modified"] = str(record["last_modified"])
            metadata["realm_id"] = str(record["realm_id"])

            # Zulip exports will always have a content-type, but third-party exports might not.
            content_type = record.get("content_type")
            if content_type is None:
                content_type = guess_type(record["s3_path"])[0]
                if content_type is None:
                    # This is the default for unknown data.  Note that
                    # for `.original` files, this is the value we'll
                    # set; that is OK, because those are never served
                    # directly anyway.
                    content_type = "application/octet-stream"

            key.upload_file(
                Filename=os.path.join(import_dir, record["path"]),
                ExtraArgs={"ContentType": content_type, "Metadata": metadata},
            )
        else:
            assert settings.LOCAL_UPLOADS_DIR is not None
            if processing_avatars or processing_emojis or processing_realm_icons:
                file_path = os.path.join(settings.LOCAL_UPLOADS_DIR, "avatars", relative_path)
            else:
                file_path = os.path.join(settings.LOCAL_UPLOADS_DIR, "files", relative_path)
            orig_file_path = os.path.join(import_dir, record["path"])
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            shutil.copy(orig_file_path, file_path)

    if processing_avatars:
        # Ensure that we have medium-size avatar images for every
        # avatar.  TODO: This implementation is hacky, both in that it
        # does get_user_profile_by_id for each user, and in that it
        # might be better to require the export to just have these.

        if processes == 1:
            for record in records:
                process_avatars(record)
        else:
            connection.close()
            cache._cache.disconnect_all()
            with multiprocessing.Pool(processes) as p:
                for out in p.imap_unordered(process_avatars, records):
                    pass


# Importing data suffers from a difficult ordering problem because of
# models that reference each other circularly.  Here is a correct order.
#
# * Client [no deps]
# * Realm [-notifications_stream]
# * Stream [only depends on realm]
# * Realm's notifications_stream
# * Now can do all realm_tables
# * UserProfile, in order by ID to avoid bot loop issues
# * Huddle
# * Recipient
# * Subscription
# * Message
# * UserMessage
#
# Because the Python object => JSON conversion process is not fully
# faithful, we have to use a set of fixers (e.g. on DateTime objects
# and foreign keys) to do the import correctly.
def do_import_realm(import_dir: Path, subdomain: str, processes: int = 1) -> Realm:
    logging.info("Importing realm dump %s", import_dir)
    if not os.path.exists(import_dir):
        raise Exception("Missing import directory!")

    realm_data_filename = os.path.join(import_dir, "realm.json")
    if not os.path.exists(realm_data_filename):
        raise Exception("Missing realm.json file!")

    if not server_initialized():
        create_internal_realm()

    logging.info("Importing realm data from %s", realm_data_filename)
    with open(realm_data_filename, "rb") as f:
        data = orjson.loads(f.read())
    remove_denormalized_recipient_column_from_data(data)

    sort_by_date = data.get("sort_by_date", False)

    bulk_import_client(data, Client, "zerver_client")

    # We don't import the Stream model yet, since it depends on Realm,
    # which isn't imported yet.  But we need the Stream model IDs for
    # notifications_stream.
    update_model_ids(Stream, data, "stream")
    re_map_foreign_keys(data, "zerver_realm", "notifications_stream", related_table="stream")
    re_map_foreign_keys(data, "zerver_realm", "signup_notifications_stream", related_table="stream")

    fix_datetime_fields(data, "zerver_realm")
    # Fix realm subdomain information
    data["zerver_realm"][0]["string_id"] = subdomain
    data["zerver_realm"][0]["name"] = subdomain
    fix_realm_authentication_bitfield(data, "zerver_realm", "authentication_methods")
    update_model_ids(Realm, data, "realm")

    realm = Realm(**data["zerver_realm"][0])

    if realm.notifications_stream_id is not None:
        notifications_stream_id: Optional[int] = int(realm.notifications_stream_id)
    else:
        notifications_stream_id = None
    realm.notifications_stream_id = None
    if realm.signup_notifications_stream_id is not None:
        signup_notifications_stream_id: Optional[int] = int(realm.signup_notifications_stream_id)
    else:
        signup_notifications_stream_id = None
    realm.signup_notifications_stream_id = None
    realm.save()

    # Email tokens will automatically be randomly generated when the
    # Stream objects are created by Django.
    fix_datetime_fields(data, "zerver_stream")
    re_map_foreign_keys(data, "zerver_stream", "realm", related_table="realm")
    # Handle rendering of stream descriptions for import from non-Zulip
    for stream in data["zerver_stream"]:
        stream["rendered_description"] = render_stream_description(stream["description"])
    bulk_import_model(data, Stream)

    realm.notifications_stream_id = notifications_stream_id
    realm.signup_notifications_stream_id = signup_notifications_stream_id
    realm.save()

    # Remap the user IDs for notification_bot and friends to their
    # appropriate IDs on this server
    internal_realm = get_realm(settings.SYSTEM_BOT_REALM)
    for item in data["zerver_userprofile_crossrealm"]:
        logging.info(
            "Adding to ID map: %s %s",
            item["id"],
            get_system_bot(item["email"], internal_realm.id).id,
        )
        new_user_id = get_system_bot(item["email"], internal_realm.id).id
        update_id_map(table="user_profile", old_id=item["id"], new_id=new_user_id)
        new_recipient_id = Recipient.objects.get(type=Recipient.PERSONAL, type_id=new_user_id).id
        update_id_map(table="recipient", old_id=item["recipient_id"], new_id=new_recipient_id)

    # Merge in zerver_userprofile_mirrordummy
    data["zerver_userprofile"] = data["zerver_userprofile"] + data["zerver_userprofile_mirrordummy"]
    del data["zerver_userprofile_mirrordummy"]
    data["zerver_userprofile"].sort(key=lambda r: r["id"])

    # To remap foreign key for UserProfile.last_active_message_id
    update_message_foreign_keys(import_dir=import_dir, sort_by_date=sort_by_date)

    fix_datetime_fields(data, "zerver_userprofile")
    update_model_ids(UserProfile, data, "user_profile")
    re_map_foreign_keys(data, "zerver_userprofile", "realm", related_table="realm")
    re_map_foreign_keys(data, "zerver_userprofile", "bot_owner", related_table="user_profile")
    re_map_foreign_keys(
        data, "zerver_userprofile", "default_sending_stream", related_table="stream"
    )
    re_map_foreign_keys(
        data, "zerver_userprofile", "default_events_register_stream", related_table="stream"
    )
    re_map_foreign_keys(
        data, "zerver_userprofile", "last_active_message_id", related_table="message", id_field=True
    )
    for user_profile_dict in data["zerver_userprofile"]:
        user_profile_dict["password"] = None
        user_profile_dict["api_key"] = generate_api_key()
        # Since Zulip doesn't use these permissions, drop them
        del user_profile_dict["user_permissions"]
        del user_profile_dict["groups"]
        # The short_name field is obsolete in Zulip, but it's
        # convenient for third party exports to populate it.
        if "short_name" in user_profile_dict:
            del user_profile_dict["short_name"]

    user_profiles = [UserProfile(**item) for item in data["zerver_userprofile"]]
    for user_profile in user_profiles:
        user_profile.set_unusable_password()
    UserProfile.objects.bulk_create(user_profiles)

    re_map_foreign_keys(data, "zerver_defaultstream", "stream", related_table="stream")
    re_map_foreign_keys(data, "zerver_realmemoji", "author", related_table="user_profile")
    for (table, model, related_table) in realm_tables:
        re_map_foreign_keys(data, table, "realm", related_table="realm")
        update_model_ids(model, data, related_table)
        bulk_import_model(data, model)

    if "zerver_huddle" in data:
        update_model_ids(Huddle, data, "huddle")
        # We don't import Huddle yet, since we don't have the data to
        # compute huddle hashes until we've imported some of the
        # tables below.
        # TODO: double-check this.

    re_map_foreign_keys(
        data,
        "zerver_recipient",
        "type_id",
        related_table="stream",
        recipient_field=True,
        id_field=True,
    )
    re_map_foreign_keys(
        data,
        "zerver_recipient",
        "type_id",
        related_table="user_profile",
        recipient_field=True,
        id_field=True,
    )
    re_map_foreign_keys(
        data,
        "zerver_recipient",
        "type_id",
        related_table="huddle",
        recipient_field=True,
        id_field=True,
    )
    update_model_ids(Recipient, data, "recipient")
    bulk_import_model(data, Recipient)
    bulk_set_users_or_streams_recipient_fields(Stream, Stream.objects.filter(realm=realm))
    bulk_set_users_or_streams_recipient_fields(UserProfile, UserProfile.objects.filter(realm=realm))

    re_map_foreign_keys(data, "zerver_subscription", "user_profile", related_table="user_profile")
    get_huddles_from_subscription(data, "zerver_subscription")
    re_map_foreign_keys(data, "zerver_subscription", "recipient", related_table="recipient")
    update_model_ids(Subscription, data, "subscription")
    fix_subscriptions_is_user_active_column(data, user_profiles)
    bulk_import_model(data, Subscription)

    if "zerver_realmauditlog" in data:
        fix_datetime_fields(data, "zerver_realmauditlog")
        re_map_foreign_keys(data, "zerver_realmauditlog", "realm", related_table="realm")
        re_map_foreign_keys(
            data, "zerver_realmauditlog", "modified_user", related_table="user_profile"
        )
        re_map_foreign_keys(
            data, "zerver_realmauditlog", "acting_user", related_table="user_profile"
        )
        re_map_foreign_keys(data, "zerver_realmauditlog", "modified_stream", related_table="stream")
        update_model_ids(RealmAuditLog, data, related_table="realmauditlog")
        bulk_import_model(data, RealmAuditLog)
    else:
        logging.info("about to call create_subscription_events")
        create_subscription_events(
            data=data,
            realm_id=realm.id,
        )
        logging.info("done with create_subscription_events")

    # Ensure the invariant that there's always a realm-creation audit
    # log event, even if the export was generated by an export tool
    # that does not create RealmAuditLog events.
    if not RealmAuditLog.objects.filter(
        realm=realm, event_type=RealmAuditLog.REALM_CREATED
    ).exists():
        RealmAuditLog.objects.create(
            realm=realm,
            event_type=RealmAuditLog.REALM_CREATED,
            event_time=realm.date_created,
            # Mark these as backfilled, since they weren't created
            # when the realm was actually created, and thus do not
            # have the creating user associated with them.
            backfilled=True,
        )

    if "zerver_huddle" in data:
        process_huddle_hash(data, "zerver_huddle")
        bulk_import_model(data, Huddle)
        for huddle in Huddle.objects.filter(recipient_id=None):
            recipient = Recipient.objects.get(type=Recipient.HUDDLE, type_id=huddle.id)
            huddle.recipient = recipient
            huddle.save(update_fields=["recipient"])

    if "zerver_alertword" in data:
        re_map_foreign_keys(data, "zerver_alertword", "user_profile", related_table="user_profile")
        re_map_foreign_keys(data, "zerver_alertword", "realm", related_table="realm")
        update_model_ids(AlertWord, data, "alertword")
        bulk_import_model(data, AlertWord)

    if "zerver_userhotspot" in data:
        fix_datetime_fields(data, "zerver_userhotspot")
        re_map_foreign_keys(data, "zerver_userhotspot", "user", related_table="user_profile")
        update_model_ids(UserHotspot, data, "userhotspot")
        bulk_import_model(data, UserHotspot)

    if "zerver_mutedtopic" in data:
        fix_datetime_fields(data, "zerver_mutedtopic")
        re_map_foreign_keys(data, "zerver_mutedtopic", "user_profile", related_table="user_profile")
        re_map_foreign_keys(data, "zerver_mutedtopic", "stream", related_table="stream")
        re_map_foreign_keys(data, "zerver_mutedtopic", "recipient", related_table="recipient")
        update_model_ids(UserTopic, data, "mutedtopic")
        bulk_import_model(data, UserTopic)

    if "zerver_muteduser" in data:
        fix_datetime_fields(data, "zerver_muteduser")
        re_map_foreign_keys(data, "zerver_muteduser", "user_profile", related_table="user_profile")
        re_map_foreign_keys(data, "zerver_muteduser", "muted_user", related_table="user_profile")
        update_model_ids(MutedUser, data, "muteduser")
        bulk_import_model(data, MutedUser)

    if "zerver_service" in data:
        re_map_foreign_keys(data, "zerver_service", "user_profile", related_table="user_profile")
        fix_service_tokens(data, "zerver_service")
        update_model_ids(Service, data, "service")
        bulk_import_model(data, Service)

    if "zerver_usergroup" in data:
        re_map_foreign_keys(data, "zerver_usergroup", "realm", related_table="realm")
        re_map_foreign_keys_many_to_many(
            data, "zerver_usergroup", "members", related_table="user_profile"
        )
        update_model_ids(UserGroup, data, "usergroup")
        bulk_import_model(data, UserGroup)

        re_map_foreign_keys(
            data, "zerver_usergroupmembership", "user_group", related_table="usergroup"
        )
        re_map_foreign_keys(
            data, "zerver_usergroupmembership", "user_profile", related_table="user_profile"
        )
        update_model_ids(UserGroupMembership, data, "usergroupmembership")
        bulk_import_model(data, UserGroupMembership)

    if "zerver_botstoragedata" in data:
        re_map_foreign_keys(
            data, "zerver_botstoragedata", "bot_profile", related_table="user_profile"
        )
        update_model_ids(BotStorageData, data, "botstoragedata")
        bulk_import_model(data, BotStorageData)

    if "zerver_botconfigdata" in data:
        re_map_foreign_keys(
            data, "zerver_botconfigdata", "bot_profile", related_table="user_profile"
        )
        update_model_ids(BotConfigData, data, "botconfigdata")
        bulk_import_model(data, BotConfigData)

    if "zerver_realmuserdefault" in data:
        re_map_foreign_keys(data, "zerver_realmuserdefault", "realm", related_table="realm")
        update_model_ids(RealmUserDefault, data, "realmuserdefault")
        bulk_import_model(data, RealmUserDefault)

    # Create RealmUserDefault table with default values if not created
    # already from the import data; this can happen when importing
    # data from another product.
    if not RealmUserDefault.objects.filter(realm=realm).exists():
        RealmUserDefault.objects.create(realm=realm)

    fix_datetime_fields(data, "zerver_userpresence")
    re_map_foreign_keys(data, "zerver_userpresence", "user_profile", related_table="user_profile")
    re_map_foreign_keys(data, "zerver_userpresence", "client", related_table="client")
    re_map_foreign_keys(data, "zerver_userpresence", "realm", related_table="realm")
    update_model_ids(UserPresence, data, "user_presence")
    bulk_import_model(data, UserPresence)

    fix_datetime_fields(data, "zerver_useractivity")
    re_map_foreign_keys(data, "zerver_useractivity", "user_profile", related_table="user_profile")
    re_map_foreign_keys(data, "zerver_useractivity", "client", related_table="client")
    update_model_ids(UserActivity, data, "useractivity")
    bulk_import_model(data, UserActivity)

    fix_datetime_fields(data, "zerver_useractivityinterval")
    re_map_foreign_keys(
        data, "zerver_useractivityinterval", "user_profile", related_table="user_profile"
    )
    update_model_ids(UserActivityInterval, data, "useractivityinterval")
    bulk_import_model(data, UserActivityInterval)

    re_map_foreign_keys(data, "zerver_customprofilefield", "realm", related_table="realm")
    update_model_ids(CustomProfileField, data, related_table="customprofilefield")
    bulk_import_model(data, CustomProfileField)

    re_map_foreign_keys(
        data, "zerver_customprofilefieldvalue", "user_profile", related_table="user_profile"
    )
    re_map_foreign_keys(
        data, "zerver_customprofilefieldvalue", "field", related_table="customprofilefield"
    )
    fix_customprofilefield(data)
    update_model_ids(CustomProfileFieldValue, data, related_table="customprofilefieldvalue")
    bulk_import_model(data, CustomProfileFieldValue)

    # Import uploaded files and avatars
    import_uploads(realm, os.path.join(import_dir, "avatars"), processes, processing_avatars=True)
    import_uploads(realm, os.path.join(import_dir, "uploads"), processes)

    # We need to have this check as the emoji files are only present in the data
    # importer from Slack
    # For Zulip export, this doesn't exist
    if os.path.exists(os.path.join(import_dir, "emoji")):
        import_uploads(realm, os.path.join(import_dir, "emoji"), processes, processing_emojis=True)

    if os.path.exists(os.path.join(import_dir, "realm_icons")):
        import_uploads(
            realm, os.path.join(import_dir, "realm_icons"), processes, processing_realm_icons=True
        )

    sender_map = {user["id"]: user for user in data["zerver_userprofile"]}

    # Import zerver_message and zerver_usermessage
    import_message_data(realm=realm, sender_map=sender_map, import_dir=import_dir)

    re_map_foreign_keys(data, "zerver_reaction", "message", related_table="message")
    re_map_foreign_keys(data, "zerver_reaction", "user_profile", related_table="user_profile")
    re_map_foreign_keys(
        data,
        "zerver_reaction",
        "emoji_code",
        related_table="realmemoji",
        id_field=True,
        reaction_field=True,
    )
    update_model_ids(Reaction, data, "reaction")
    bulk_import_model(data, Reaction)

    # Similarly, we need to recalculate the first_message_id for stream objects.
    for stream in Stream.objects.filter(realm=realm):
        recipient = Recipient.objects.get(type=Recipient.STREAM, type_id=stream.id)
        first_message = Message.objects.filter(recipient=recipient).first()
        if first_message is None:
            stream.first_message_id = None
        else:
            stream.first_message_id = first_message.id
        stream.save(update_fields=["first_message_id"])

    # Do attachments AFTER message data is loaded.
    # TODO: de-dup how we read these json files.
    fn = os.path.join(import_dir, "attachment.json")
    if not os.path.exists(fn):
        raise Exception("Missing attachment.json file!")

    logging.info("Importing attachment data from %s", fn)
    with open(fn, "rb") as f:
        data = orjson.loads(f.read())

    import_attachments(data)

    # Import the analytics file.
    import_analytics_data(realm=realm, import_dir=import_dir)

    if settings.BILLING_ENABLED:
        do_change_plan_type(realm, Realm.LIMITED, acting_user=None)
    else:
        do_change_plan_type(realm, Realm.SELF_HOSTED, acting_user=None)
    return realm


# create_users and do_import_system_bots differ from their equivalent
# in zerver/lib/server_initialization.py because here we check if the
# bots don't already exist and only then create a user for these bots.
def do_import_system_bots(realm: Any) -> None:
    internal_bots = [
        (bot["name"], bot["email_template"] % (settings.INTERNAL_BOT_DOMAIN,))
        for bot in settings.INTERNAL_BOTS
    ]
    create_users(realm, internal_bots, bot_type=UserProfile.DEFAULT_BOT)
    print("Finished importing system bots.")


def create_users(
    realm: Realm, name_list: Iterable[Tuple[str, str]], bot_type: Optional[int] = None
) -> None:
    user_set = set()
    for full_name, email in name_list:
        if not UserProfile.objects.filter(email=email):
            user_set.add((email, full_name, True))
    bulk_create_users(realm, user_set, bot_type)


def update_message_foreign_keys(import_dir: Path, sort_by_date: bool) -> None:
    old_id_list = get_incoming_message_ids(
        import_dir=import_dir,
        sort_by_date=sort_by_date,
    )

    count = len(old_id_list)

    new_id_list = allocate_ids(model_class=Message, count=count)

    for old_id, new_id in zip(old_id_list, new_id_list):
        update_id_map(
            table="message",
            old_id=old_id,
            new_id=new_id,
        )

    # We don't touch user_message keys here; that happens later when
    # we're actually read the files a second time to get actual data.


def get_incoming_message_ids(import_dir: Path, sort_by_date: bool) -> List[int]:
    """
    This function reads in our entire collection of message
    ids, which can be millions of integers for some installations.
    And then we sort the list.  This is necessary to ensure
    that the sort order of incoming ids matches the sort order
    of date_sent, which isn't always guaranteed by our
    utilities that convert third party chat data.  We also
    need to move our ids to a new range if we're dealing
    with a server that has data for other realms.
    """

    if sort_by_date:
        tups: List[Tuple[int, int]] = []
    else:
        message_ids: List[int] = []

    dump_file_id = 1
    while True:
        message_filename = os.path.join(import_dir, f"messages-{dump_file_id:06}.json")
        if not os.path.exists(message_filename):
            break

        with open(message_filename, "rb") as f:
            data = orjson.loads(f.read())

        # Aggressively free up memory.
        del data["zerver_usermessage"]

        for row in data["zerver_message"]:
            # We truncate date_sent to int to theoretically
            # save memory and speed up the sort.  For
            # Zulip-to-Zulip imports, the
            # message_id will generally be a good tiebreaker.
            # If we occasionally mis-order the ids for two
            # messages from the same second, it's not the
            # end of the world, as it's likely those messages
            # arrived to the original server in somewhat
            # arbitrary order.

            message_id = row["id"]

            if sort_by_date:
                date_sent = int(row["date_sent"])
                tup = (date_sent, message_id)
                tups.append(tup)
            else:
                message_ids.append(message_id)

        dump_file_id += 1

    if sort_by_date:
        tups.sort()
        message_ids = [tup[1] for tup in tups]

    return message_ids


def import_message_data(realm: Realm, sender_map: Dict[int, Record], import_dir: Path) -> None:
    dump_file_id = 1
    while True:
        message_filename = os.path.join(import_dir, f"messages-{dump_file_id:06}.json")
        if not os.path.exists(message_filename):
            break

        with open(message_filename, "rb") as f:
            data = orjson.loads(f.read())

        logging.info("Importing message dump %s", message_filename)
        re_map_foreign_keys(data, "zerver_message", "sender", related_table="user_profile")
        re_map_foreign_keys(data, "zerver_message", "recipient", related_table="recipient")
        re_map_foreign_keys(data, "zerver_message", "sending_client", related_table="client")
        fix_datetime_fields(data, "zerver_message")
        # Parser to update message content with the updated attachment URLs
        fix_upload_links(data, "zerver_message")

        # We already create mappings for zerver_message ids
        # in update_message_foreign_keys(), so here we simply
        # apply them.
        message_id_map = ID_MAP["message"]
        for row in data["zerver_message"]:
            row["id"] = message_id_map[row["id"]]

        for row in data["zerver_usermessage"]:
            assert row["message"] in message_id_map

        fix_message_rendered_content(
            realm=realm,
            sender_map=sender_map,
            messages=data["zerver_message"],
        )
        logging.info("Successfully rendered Markdown for message batch")

        # A LOT HAPPENS HERE.
        # This is where we actually import the message data.
        bulk_import_model(data, Message)

        # Due to the structure of these message chunks, we're
        # guaranteed to have already imported all the Message objects
        # for this batch of UserMessage objects.
        re_map_foreign_keys(data, "zerver_usermessage", "message", related_table="message")
        re_map_foreign_keys(
            data, "zerver_usermessage", "user_profile", related_table="user_profile"
        )
        fix_bitfield_keys(data, "zerver_usermessage", "flags")

        bulk_import_user_message_data(data, dump_file_id)
        dump_file_id += 1


def import_attachments(data: TableData) -> None:

    # Clean up the data in zerver_attachment that is not
    # relevant to our many-to-many import.
    fix_datetime_fields(data, "zerver_attachment")
    re_map_foreign_keys(data, "zerver_attachment", "owner", related_table="user_profile")
    re_map_foreign_keys(data, "zerver_attachment", "realm", related_table="realm")

    # Configure ourselves.  Django models many-to-many (m2m)
    # relations asymmetrically. The parent here refers to the
    # Model that has the ManyToManyField.  It is assumed here
    # the child models have been loaded, but we are in turn
    # responsible for loading the parents and the m2m rows.
    parent_model = Attachment
    parent_db_table_name = "zerver_attachment"
    parent_singular = "attachment"
    child_singular = "message"
    child_plural = "messages"
    m2m_table_name = "zerver_attachment_messages"
    parent_id = "attachment_id"
    child_id = "message_id"

    update_model_ids(parent_model, data, "attachment")
    # We don't bulk_import_model yet, because we need to first compute
    # the many-to-many for this table.

    # First, build our list of many-to-many (m2m) rows.
    # We do this in a slightly convoluted way to anticipate
    # a future where we may need to call re_map_foreign_keys.

    m2m_rows: List[Record] = []
    for parent_row in data[parent_db_table_name]:
        for fk_id in parent_row[child_plural]:
            m2m_row: Record = {}
            m2m_row[parent_singular] = parent_row["id"]
            m2m_row[child_singular] = ID_MAP["message"][fk_id]
            m2m_rows.append(m2m_row)

    # Create our table data for insert.
    m2m_data: TableData = {m2m_table_name: m2m_rows}
    convert_to_id_fields(m2m_data, m2m_table_name, parent_singular)
    convert_to_id_fields(m2m_data, m2m_table_name, child_singular)
    m2m_rows = m2m_data[m2m_table_name]

    # Next, delete out our child data from the parent rows.
    for parent_row in data[parent_db_table_name]:
        del parent_row[child_plural]

    # Update 'path_id' for the attachments
    for attachment in data[parent_db_table_name]:
        attachment["path_id"] = path_maps["attachment_path"][attachment["path_id"]]

    # Next, load the parent rows.
    bulk_import_model(data, parent_model)

    # Now, go back to our m2m rows.
    # TODO: Do this the kosher Django way.  We may find a
    # better way to do this in Django 1.9 particularly.
    with connection.cursor() as cursor:
        sql_template = SQL(
            """
            INSERT INTO {m2m_table_name} ({parent_id}, {child_id}) VALUES %s
        """
        ).format(
            m2m_table_name=Identifier(m2m_table_name),
            parent_id=Identifier(parent_id),
            child_id=Identifier(child_id),
        )
        tups = [(row[parent_id], row[child_id]) for row in m2m_rows]
        execute_values(cursor.cursor, sql_template, tups)

    logging.info("Successfully imported M2M table %s", m2m_table_name)


def import_analytics_data(realm: Realm, import_dir: Path) -> None:
    analytics_filename = os.path.join(import_dir, "analytics.json")
    if not os.path.exists(analytics_filename):
        return

    logging.info("Importing analytics data from %s", analytics_filename)
    with open(analytics_filename, "rb") as f:
        data = orjson.loads(f.read())

    # Process the data through the fixer functions.
    fix_datetime_fields(data, "analytics_realmcount")
    re_map_foreign_keys(data, "analytics_realmcount", "realm", related_table="realm")
    update_model_ids(RealmCount, data, "analytics_realmcount")
    bulk_import_model(data, RealmCount)

    fix_datetime_fields(data, "analytics_usercount")
    re_map_foreign_keys(data, "analytics_usercount", "realm", related_table="realm")
    re_map_foreign_keys(data, "analytics_usercount", "user", related_table="user_profile")
    update_model_ids(UserCount, data, "analytics_usercount")
    bulk_import_model(data, UserCount)

    fix_datetime_fields(data, "analytics_streamcount")
    re_map_foreign_keys(data, "analytics_streamcount", "realm", related_table="realm")
    re_map_foreign_keys(data, "analytics_streamcount", "stream", related_table="stream")
    update_model_ids(StreamCount, data, "analytics_streamcount")
    bulk_import_model(data, StreamCount)

# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import sys
sys.path.append('../')

from auto_scan_test import AutoScanTest, IgnoreReasons
from program_config import TensorConfig, ProgramConfig, OpConfig, CxxConfig, TargetType, PrecisionType, DataLayoutType, Place
import unittest

import hypothesis
from hypothesis import given, settings, seed, example, assume
import hypothesis.strategies as st
import argparse
import numpy as np
from functools import partial
import copy


class TestInverseOp(AutoScanTest):
    def __init__(self, *args, **kwargs):
        AutoScanTest.__init__(self, *args, **kwargs)
        self.enable_testing_on_place(
            TargetType.Host,
            PrecisionType.FP32,
            DataLayoutType.NCHW,
            thread=[1, 2])

    def is_program_valid(self,
                         program_config: ProgramConfig,
                         predictor_config: CxxConfig) -> bool:
        return True

    def sample_program_configs(self, draw):
        in_shape = draw(
            st.lists(
                st.integers(
                    min_value=1, max_value=32), min_size=1, max_size=2))

        def generate_input(*args, **kwargs):
            last_dim = np.random.randint(
                low=1, high=16, size=[1]).astype(np.int32)
            input_dim = copy.deepcopy(in_shape)
            input_dim.append(last_dim[0])  #last 2 dim must be equal
            input_dim.append(last_dim[0])
            return np.random.random(input_dim).astype(np.float32)

        build_ops = OpConfig(
            type="inverse",
            inputs={"Input": ["input_data"], },
            outputs={"Output": ["output_data"], },
            attrs={})

        program_config = ProgramConfig(
            ops=[build_ops],
            weights={},
            inputs={
                "input_data": TensorConfig(data_gen=partial(generate_input)),
            },
            outputs=["output_data"])
        return program_config

    def sample_predictor_configs(self):
        return self.get_predictor_configs(), ["inverse"], (5e-5, 5e-5)

    def add_ignore_pass_case(self):
        pass

    def test(self, *args, **kwargs):
        self.run_and_statis(quant=False, max_examples=25)


if __name__ == "__main__":
    unittest.main(argv=[''])

# -*- coding: utf-8 -*-
'''
Return data to a mongodb server

Required python modules: pymongo


This returner will send data from the minions to a MongoDB server. To
configure the settings for your MongoDB server, add the following lines
to the minion config files::

    mongo.db: <database name>
    mongo.host: <server ip address>
    mongo.user: <MongoDB username>
    mongo.password: <MongoDB user password>
    mongo.port: 27017

Alternative configuration values can be used by prefacing the configuration.
Any values not found in the alternative configuration will be pulled from
the default location::

    alternative.mongo.db: <database name>
    alternative.mongo.host: <server ip address>
    alternative.mongo.user: <MongoDB username>
    alternative.mongo.password: <MongoDB user password>
    alternative.mongo.port: 27017

  To use the mongo returner, append '--return mongo' to the salt command. ex:

    salt '*' test.ping --return mongo_return

  To use the alternative configuration, append '--return_config alternative' to the salt command. ex:

    salt '*' test.ping --return mongo_return --return_config alternative
'''
from __future__ import absolute_import

# Import python libs
import logging

# import Salt libs
import salt.utils
import salt.returners
import six

# Import third party libs
try:
    import pymongo
    HAS_PYMONGO = True
except ImportError:
    HAS_PYMONGO = False


log = logging.getLogger(__name__)

# Define the module's virtual name
# currently only used iby _get_options
__virtualname__ = 'mongo'


def __virtual__():
    if not HAS_PYMONGO:
        return False
    return 'mongo_return'


def _remove_dots(src):
    '''
    Remove dots from the given data structure
    '''
    output = {}
    for key, val in six.iteritems(src):
        if isinstance(val, dict):
            val = _remove_dots(val)
        output[key.replace('.', '-')] = val
    return output


def _get_options(ret):
    '''
    Get the monogo_return options from salt.
    '''
    attrs = {'host': 'host',
             'port': 'port',
             'db': 'db',
             'username': 'username',
             'password': 'password'}

    _options = salt.returners.get_returner_options(__virtualname__,
                                                   ret,
                                                   attrs,
                                                   __salt__=__salt__,
                                                   __opts__=__opts__)
    return _options


def _get_conn(ret):
    '''
    Return a mongodb connection object
    '''
    _options = _get_options(ret)

    host = _options.get('host')
    port = _options.get('port')
    db_ = _options.get('db')
    user = _options.get('user')
    password = _options.get('password')

    conn = pymongo.Connection(host, port)
    mdb = conn[db_]

    if user and password:
        mdb.authenticate(user, password)
    return conn, mdb


def returner(ret):
    '''
    Return data to a mongodb server
    '''
    conn, mdb = _get_conn(ret)
    col = mdb[ret['id']]

    if isinstance(ret['return'], dict):
        back = _remove_dots(ret['return'])
    else:
        back = ret['return']

    log.debug(back)
    sdata = {ret['jid']: back, 'fun': ret['fun']}
    if 'out' in ret:
        sdata['out'] = ret['out']
    col.insert(sdata)


def get_jid(jid):
    '''
    Return the return information associated with a jid
    '''
    conn, mdb = _get_conn(ret=None)
    ret = {}
    for collection in mdb.collection_names():
        rdata = mdb[collection].find_one({jid: {'$exists': 'true'}})
        if rdata:
            ret[collection] = rdata
    return ret


def get_fun(fun):
    '''
    Return the most recent jobs that have executed the named function
    '''
    conn, mdb = _get_conn(ret=None)
    ret = {}
    for collection in mdb.collection_names():
        rdata = mdb[collection].find_one({'fun': fun})
        if rdata:
            ret[collection] = rdata
    return ret


def prep_jid(nocache, passed_jid=None):  # pylint: disable=unused-argument
    '''
    Do any work necessary to prepare a JID, including sending a custom id
    '''
    return passed_jid if passed_jid is not None else salt.utils.gen_jid()

from PIL import Image
import numpy as np
# from streamlit.logger import update_formatter
import torch
from matplotlib import cm



def min_max_norm(array):
    lim = [array.min(), array.max()]
    array = array - lim[0] 
    array.mul_(1 / (1.e-10+ (lim[1] - lim[0])))
    # array = torch.clamp(array, min=0, max=1)
    return array

def torch_to_rgba(img):
    img = min_max_norm(img)
    rgba_im = img.permute(1, 2, 0).cpu()
    if rgba_im.shape[2] == 3:
        rgba_im = torch.cat((rgba_im, torch.ones(*rgba_im.shape[:2], 1)), dim=2)
    assert rgba_im.shape[2] == 4
    return rgba_im


def numpy_to_image(img, size):
    """
    takes a [0..1] normalized rgba input and returns resized image as [0...255] rgba image
    """
    resized = Image.fromarray((img*255.).astype(np.uint8)).resize((size, size))
    return resized

def upscale_pytorch(img:np.array, size):
    torch_img = torch.from_numpy(img).unsqueeze(0).permute(0,3,1,2)
    print(torch_img)
    upsampler = torch.nn.Upsample(size=size)    
    return upsampler(torch_img)[0].permute(1,2,0).cpu().numpy()


def heatmap_helper(image:torch.Tensor, heatmap: torch.Tensor, size=None, alpha=.6):
    if not size:
        size = image.shape[1]

    img = numpy_to_image(min_max_norm(heatmap).numpy(), size)
    return np.asarray(img)

def heatmap(image:torch.Tensor, heatmap: torch.Tensor, size=None, alpha=.6):
    if not size:
        size = image.shape[1]
    # print(heatmap)
    # print(min_max_norm(heatmap))

    img = torch_to_rgba(image).numpy() # [0...1] rgba numpy "image"
    hm = cm.jet(min_max_norm(heatmap).numpy()) # [0...1] rgba numpy "image"

    img = np.array(numpy_to_image(img,size))
    hm = np.array(numpy_to_image(hm, size))
    # hm = upscale_pytorch(hm, size)
    # print (hm) 

    return Image.fromarray((alpha * hm + (1-alpha)*img).astype(np.uint8))
    # return Image.fromarray(hm)
# -*- coding: utf-8 -*-
"""
This file is part of pyCMBS.
(c) 2012- Alexander Loew
For COPYING and LICENSE details, please refer to the LICENSE file
"""


class Koeppen(object):
    """
    KOEPPEN CLASS
    class to generate koeppen plot
    """

    def __init__(self, temp=None, precip=None, lsm=None):
        """
        Koeppen class
        This class implements the functionality to generate koeppen plots.

        Parameters
        ----------
        temp : Data
            data objekt of temperature
        precip : Data
            data objekt of precipitation
        lsm : Data
            data objekt of land-sea-mask (0.0 to 1.0)

        EXAMPLES
        ========

        """

        # check consistency
        if temp is None:
            raise ValueError('No temperature given')
        if precip is None:
            raise ValueError('No precipitation given')
        if lsm is None:
            raise ValueError('No land-sea-mask given')

        # set values of class
        self.temp = temp
        self.precip = precip
        self.lsm = lsm

        if not self._check_resolution():
            raise ValueError('ERROR:The three array differe in the resolution')
        if not self._check_units():
            raise ValueError('ERROR:The units of one value is wrong')

        # Create new koeppen Color map
        self.koeppen_cmap()
        self.cmap = cm.get_cmap('koeppen')
        # convert from [kg m-2 s-1] to [kg m-2 day-1] (= [mm day-1])
         # ??? Unklar warum nicht 'precip.mulc(60. * 60. * 24. * 365.)'
        self.precip = precip.mulc(60. * 60. * 24. * 365. / 12., copy=True)
        self.temp = temp.subc(273.15, copy=True)  # ??? Unklar warum nicht 'temp.subc(273.15)'

        Psum = self.precip.timsum(return_object=True)            # Berechnet die Summe der Jahresniederschlag

        nt, ny, nx = self.temp.shape
        nlat = ny
        nlon = nx

        Pmin = self.precip.data.min(axis=0)
        Pmax = self.precip.data.max(axis=0)

        precipHS = self.precip.copy()
        precipHS.data[(0, 1, 2, 3, 4, 5), 0:(nlat / 2 - 1), :] \
            = self.precip.data[(3, 4, 5, 6, 7, 8), 0:(nlat / 2 - 1), :]
        precipHS.data[(6, 7, 8, 9, 10, 11), 0:(nlat / 2 - 1), :] \
            = self.precip.data[(3, 4, 5, 6, 7, 8), 0:(nlat / 2 - 1), :]
        precipHS.data[(0, 1, 2, 3, 4, 5), (nlat / 2):(nlat - 1), :] \
            = self.precip.data[(0, 1, 2, 9, 10, 11), (nlat / 2):(nlat - 1), :]
        precipHS.data[(6, 7, 8, 9, 10, 11), (nlat / 2):(nlat - 1), :] \
            = self.precip.data[(0, 1, 2, 9, 10, 11), (nlat / 2):(nlat - 1), :]

        precipHW = self.precip.copy()
        precipHW.data[(0, 1, 2, 3, 4, 5), 0:(nlat / 2 - 1), :] = self.precip.data[(0, 1, 2, 9, 10, 11), 0:(nlat / 2 - 1), :]
        precipHW.data[(6, 7, 8, 9, 10, 11), 0:(nlat / 2 - 1), :] = self.precip.data[(0, 1, 2, 9, 10, 11), 0:(nlat / 2 - 1), :]
        precipHW.data[(0, 1, 2, 3, 4, 5), (nlat / 2):(nlat - 1), :] = self.precip.data[(3, 4, 5, 6, 7, 8), (nlat / 2):(nlat - 1), :]
        precipHW.data[(6, 7, 8, 9, 10, 11), (nlat / 2):(nlat - 1), :] = self.precip.data[(3, 4, 5, 6, 7, 8), (nlat / 2):(nlat - 1), :]

        PminHS = precipHS.data.min(axis=0)   # Bestimmt den minimalen Monastniederschlag aus PmaxHS
        PmaxHS = precipHS.data.max(axis=0)   # Bestimmt den maximalen Monastniederschlag aus PmaxHS
        PminHW = precipHW.data.min(axis=0)   # Bestimmt den minimalen Monastniederschlag aus PminHW
        PmaxHW = precipHW.data.max(axis=0)   # Bestimmt den maximalen Monastniederschlag aus PminHW

        Tavg = self.temp.data.mean(axis=0)   # Bestimmt die mittlere Jahrestemperatur
        Tmin = self.temp.data.min(axis=0)     # Bestimmt die minimale Monatstemperatur
        Tmax = self.temp.data.max(axis=0)     # Bestimmt die maximale Jahrestemperatur

        self.Clim = self.precip.timmean(return_object=True)
        self.Clim.units = "climate type"

        for lat in range(0, nlat):
            for lon in range(0, nlon):
                psum = Psum.data.data[lat][lon]
                pmin = Pmin[lat][lon]
                pminhs = PminHS[lat][lon]
                pminhw = PminHW[lat][lon]
                pmaxhs = PmaxHS[lat][lon]
                pmaxhw = PmaxHW[lat][lon]
                tavg = Tavg[lat][lon]
                tmin = Tmin[lat][lon]
                tmax = Tmax[lat][lon]
                self.Clim.data.data[lat][lon] = self.set_clim(psum, pmin, pminhs, pminhw, pmaxhs, pmaxhw, tavg, tmin, tmax)

        self.Clim.data.mask[less(self.lsm.data, 0.5)] = True

    def koeppen_cmap(self):
        """
        Create a colormap with 14 discrete colors and register it
        """
        # define individual colors as hex values
        cpool = ['#7f0000', '#ff0000', '#ff4c4c', '#ff9999', '#ffa500',
                 '#ffff4c', '#009900', '#00ff00', '#99ff99', '#990099',
                 '#e500e5', '#ff66ff', '#0000ff', '#9999ff', '#000000']
        cmap3 = col.ListedColormap(cpool[0:14], 'koeppen')
#       plt.cm.register_cmap(cmap=cmap3,name='koeppen',lut=15)
        plt.cm.register_cmap(cmap=cmap3, name='koeppen')
        return cmap3

    def set_clim(self, psum, pmin, pminhs, pminhw, pmaxhs, pmaxhw, tavg, tmin, tmax):
        clim = -999

        if tmin > 18:
            if pmin > 60:                 # A(B)
                clim = 1                    # Af
            else:
                if pmin > (0.04 * (2500 - psum)):      # A(B)-msw
                    clim = 2                  # Am
                else:
                    if (pminhs < 40) and (pminhs < (pmaxhw / 3)):   # A(B)-sw
                        if (psum / 10) < (2 * tavg):          # A(B)-s
                            if (psum / 10) < (tavg):            # B
                                clim = 6                    # BW
                            else:
                                clim = 5                    # BS
                        else:
                            clim = 3                      # As
                    else:
                        if (psum / 10) < (2 * (tavg + 14)):       # A(B)-w
                            if (psum / 10) < (tavg + 14):       # B
                                clim = 6                    # BW
                            else:
                                clim = 5                    # BS
                        else:
                            clim = 4                      # Aw
        else:
            if (pminhs < 40) and (pminhs < (pmaxhw / 3)):   # CDE(B)
                if (psum / 10) < (2 * tavg):          # CDE(B)-s
                    if (psum / 10) < (tavg):            # B
                        clim = 6                    # BW
                    else:
                        clim = 5                    # BS
                else:
                    if tmax < 10:                # CDE-s
                        if tmax < 0:                # E
                            clim = 14                 # EF
                        else:
                            clim = 13                 # ET
                    else:
                        if (tmin > -3):             # CD-s
                            clim = 8                  # Cs
                        else:
                            clim = 11                 # Ds
            else:
                if pminhw < (pmaxhs / 10):            # CDE(B)-fw
                    if (psum / 10) < (2 * (tavg + 14)):     # CDE(B)-w
                        if (psum / 10) < (tavg + 14):         # B
                            clim = 6                  # BW
                        else:
                            clim = 5                  # BS
                    else:
                        if tmax < 10:               # CDE-w

                            if (tmax < 0):                # E
                                clim = 14               # EF
                            else:
                                clim = 13              # ET
                        else:
                            if (tmin > -3):               # CD-w
                                clim = 9                # Cw
                            else:
                                clim = 12               # Dw
                else:
                    if (psum / 10) < (2 * (tavg + 7)):      # CDE(B)-f
                        if (psum / 10) < (tavg + 7):          # B
                            clim = 6                  # BW
                        else:
                            clim = 5                  # BS
                    else:
                        if (tmax < 10):             # CDE-f
                            if (tmax < 0):                # E
                                clim = 14               # EF
                            else:
                                clim = 13              # ET
                        else:
                            if (tmin > -3):               # CD-f
                                clim = 7                # Cf
                            else:
                                clim = 10              # Df
        return clim

    def _check_resolution(self):
        """
        This routine just checks if all three array have a equal number of ny and nx values
        """
        nt_t, ny_t, nx_t = self.temp.shape
        nt_p, ny_p, nx_p = self.precip.shape
        ny_l, nx_l = self.lsm.shape

        if (ny_t != ny_p) or (ny_t != ny_l):
            sys.exit('ERROR: The resolution ot the three arrays differ in \
       Y-dimension: \n' + str(ny_t) + "(temp)  " + str(ny_p)
                     + "(precip) " + str(ny_l) + "(lsm) ")
            return False

        if (nx_t != nx_p) or (nx_t != nx_l):
            sys.exit('ERROR: The resolution ot the three arrays differ in \
       X-dimension: \n' + str(nx_t) + "(temp)  " + str(nx_p)
                     + "(precip) " + str(nx_l) + "(lsm) ")
            return False

        return True

    def _check_units(self):
        """
        This routine just checks if all three array have a equal number of ny and nx values
        """
        if self.precip.unit != "kg/m^2s":
            raise ValueError('ERROR: The unit of the precip is not [kg/m^2s] its set to [' + self.precip.unit + "]")

        if self.temp.unit != "K":
            raise ValueError('ERROR: The unit of the temperature is not [K] its set to [' + self.temp.unit + "]")

        if self.lsm.unit != "fractional":
            raise ValueError('ERROR: The unit of the temperature is not [fractional] its set to [' + self.temp.unit + "]")

        return True

    def copy(self):
        """
        Returns the Clim Data as an Data variable
        """
        return self.Clim

    def climfrac(self):
        """
        This routine calculats the fraction of each type in per centum.
        ToDo:
        Unclear id the print is OK or if the values should given back as an array.
        """
        climfrac = [0] * 14

        ny, nx = self.Clim.data.data.shape
        for ny in range(0, ny - 1):
            Aweight = cos(self.Clim.lat[ny][0] / 180 * 3.14159265359)
            for nx in range(0, nx - 1):
                clim = int(self.Clim.data.data[ny][nx])
                climfrac[clim - 1] = climfrac[clim - 1] + Aweight

        s = sum(climfrac)
        climfrac[:] = [x / s for x in climfrac]

        print "Af: " + str(climfrac[0])
        print "Am: " + str(climfrac[1])
        print "As: " + str(climfrac[2])
        print "Aw: " + str(climfrac[3])
        print "BS: " + str(climfrac[4])
        print "BW: " + str(climfrac[5])
        print "Cf: " + str(climfrac[6])
        print "Cs: " + str(climfrac[7])
        print "Cw: " + str(climfrac[8])
        print "Df: " + str(climfrac[9])
        print "Ds: " + str(climfrac[10])
        print "Dw: " + str(climfrac[11])
        print "ET: " + str(climfrac[12])
        print "EF: " + str(climfrac[13])

    def legend(self):
        """
        This routine prints a legend of the geiger-koeppen types.
        The description is taken from:
        MARKUS KOTTEK, JUERGEN GRIESER, CHRISTOPH BECK , BRUNO RUDOLF and FRANZ RUBEL
        World Map of the Koeppen-Geiger climate classification updated
        Meteorologische Zeitschrift, Vol. 15, No. 3, 259-263 (June 2006)

        """

        print "|================= Class legend =================|"
        print "| Af: Equatorial rainforest, fully humid         |"
        print "| Am: Equatorial climates                        |"
        print "| As: Equatorial monsoon                         |"
        print "| Aw: Equatorial savannah with dry winter        |"
        print "|------------------------------------------------|"
        print "| BS: Steppe climate                             |"
        print "| BW: Desert climate                             |"
        print "|------------------------------------------------|"
        print "| Cf: Warm temperate climate, fully humid        |"
        print "| Cs: Warm temperate climate with dry summer     |"
        print "| Cw: Warm temperate climate with dry winter     |"
        print "|------------------------------------------------|"
        print "| Df: Snow climate, fully humid                  |"
        print "| Ds: Snow climate with dry summer               |"
        print "| Dw: Snow climate with dry winter               |"
        print "|------------------------------------------------|"
        print "| ET: Tundra climate                             |"
        print "| EF: Frost climate                              |"
        print "|================================================|"

    def plot(self, **kwargs):
        """
        This routine plots the data of his own geiger-koeppen data by
        using the plot-routine map_plot.
        It use the own created color-map and sets the color-bar to a
        horizontal orientation.
        It set the range of values between 0.5 and 14.5. Which are the
        possible values of geiger-koeppen.
        ToDo:
        At the moment the label of the geiger-koeppen types are missing
        at the color-bar
        """
        map_plot(self.Clim, cmap_data=self.cmap, colorbar_orientation='horizontal', vmin=0.5, vmax=14.5,
                 cticks=[1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14.],
                 cticklabels=["Af", "Am", "As", "Aw", "BS", "BW", "Cf",
                              "Cs", "Cw", "Df", "Ds", "Dw", "ET", "EF"],
                 nclasses=15, **kwargs)

MIME = {
	".3dm": "x-world/x-3dmf",
	".3dmf": "x-world/x-3dmf",
	".a": "application/octet-stream",
	".aab": "application/x-authorware-bin",
	".aam": "application/x-authorware-map",
	".aas": "application/x-authorware-seg",
	".abc": "text/vnd.abc",
	".acgi": "text/html",
	".afl": "video/animaflex",
	".ai": "application/postscript",
	".aif": "audio/aiff",
	".aif": "audio/x-aiff",
	".aifc": "audio/aiff",
	".aifc": "audio/x-aiff",
	".aiff": "audio/aiff",
	".aiff": "audio/x-aiff",
	".aim": "application/x-aim",
	".aip": "text/x-audiosoft-intra",
	".ani": "application/x-navi-animation",
	".aos": "application/x-nokia-9000-communicator-add-on-software",
	".aps": "application/mime",
	".arc": "application/octet-stream",
	".arj": "application/arj",
	".arj": "application/octet-stream",
	".art": "image/x-jg",
	".asf": "video/x-ms-asf",
	".asm": "text/x-asm",
	".asp": "text/asp",
	".asx": "application/x-mplayer2",
	".asx": "video/x-ms-asf",
	".asx": "video/x-ms-asf-plugin",
	".au": "audio/basic",
	".au": "audio/x-au",
	".avi": "application/x-troff-msvideo",
	".avi": "video/avi",
	".avi": "video/msvideo",
	".avi": "video/x-msvideo",
	".avs": "video/avs-video",
	".bcpio": "application/x-bcpio",
	".bin": "application/mac-binary",
	".bin": "application/macbinary",
	".bin": "application/octet-stream",
	".bin": "application/x-binary",
	".bin": "application/x-macbinary",
	".bm": "image/bmp",
	".bmp": "image/bmp",
	".bmp": "image/x-windows-bmp",
	".boo": "application/book",
	".book": "application/book",
	".boz": "application/x-bzip2",
	".bsh": "application/x-bsh",
	".bz": "application/x-bzip",
	".bz2": "application/x-bzip2",
	".c": "text/plain",
	".c": "text/x-c",
	".c++": "text/plain",
	".cat": "application/vnd.ms-pki.seccat",
	".cc": "text/plain",
	".cc": "text/x-c",
	".ccad": "application/clariscad",
	".cco": "application/x-cocoa",
	".cdf": "application/cdf",
	".cdf": "application/x-cdf",
	".cdf": "application/x-netcdf",
	".cer": "application/pkix-cert",
	".cer": "application/x-x509-ca-cert",
	".cha": "application/x-chat",
	".chat": "application/x-chat",
	".class": "application/java",
	".class": "application/java-byte-code",
	".class": "application/x-java-class",
	".com": "application/octet-stream",
	".com": "text/plain",
	".conf": "text/plain",
	".cpio": "application/x-cpio",
	".cpp": "text/x-c",
	".cpt": "application/mac-compactpro",
	".cpt": "application/x-compactpro",
	".cpt": "application/x-cpt",
	".crl": "application/pkcs-crl",
	".crl": "application/pkix-crl",
	".crt": "application/pkix-cert",
	".crt": "application/x-x509-ca-cert",
	".crt": "application/x-x509-user-cert",
	".csh": "application/x-csh",
	".csh": "text/x-script.csh",
	".css": "application/x-pointplus",
	".css": "text/css",
	".cxx": "text/plain",
	".dcr": "application/x-director",
	".deepv": "application/x-deepv",
	".def": "text/plain",
	".der": "application/x-x509-ca-cert",
	".dif": "video/x-dv",
	".dir": "application/x-director",
	".dl": "video/dl",
	".dl": "video/x-dl",
	".doc": "application/msword",
	".dot": "application/msword",
	".dp": "application/commonground",
	".drw": "application/drafting",
	".dump": "application/octet-stream",
	".dv": "video/x-dv",
	".dvi": "application/x-dvi",
	".dwf": "drawing/x-dwf (old)",
	".dwf": "model/vnd.dwf",
	".dwg": "application/acad",
	".dwg": "image/vnd.dwg",
	".dwg": "image/x-dwg",
	".dxf": "application/dxf",
	".dxf": "image/vnd.dwg",
	".dxf": "image/x-dwg",
	".dxr": "application/x-director",
	".el": "text/x-script.elisp",
	".elc": "application/x-bytecode.elisp (compiled elisp)",
	".elc": "application/x-elc",
	".env": "application/x-envoy",
	".eps": "application/postscript",
	".es": "application/x-esrehber",
	".etx": "text/x-setext",
	".evy": "application/envoy",
	".evy": "application/x-envoy",
	".exe": "application/octet-stream",
	".f": "text/plain",
	".f": "text/x-fortran",
	".f77": "text/x-fortran",
	".f90": "text/plain",
	".f90": "text/x-fortran",
	".fdf": "application/vnd.fdf",
	".fif": "application/fractals",
	".fif": "image/fif",
	".fli": "video/fli",
	".fli": "video/x-fli",
	".flo": "image/florian",
	".flx": "text/vnd.fmi.flexstor",
	".fmf": "video/x-atomic3d-feature",
	".for": "text/plain",
	".for": "text/x-fortran",
	".fpx": "image/vnd.fpx",
	".fpx": "image/vnd.net-fpx",
	".frl": "application/freeloader",
	".funk": "audio/make",
	".g": "text/plain",
	".g3": "image/g3fax",
	".gif": "image/gif",
	".gl": "video/gl",
	".gl": "video/x-gl",
	".gsd": "audio/x-gsm",
	".gsm": "audio/x-gsm",
	".gsp": "application/x-gsp",
	".gss": "application/x-gss",
	".gtar": "application/x-gtar",
	".gz": "application/x-compressed",
	".gz": "application/x-gzip",
	".gzip": "application/x-gzip",
	".gzip": "multipart/x-gzip",
	".h": "text/plain",
	".h": "text/x-h",
	".hdf": "application/x-hdf",
	".help": "application/x-helpfile",
	".hgl": "application/vnd.hp-hpgl",
	".hh": "text/plain",
	".hh": "text/x-h",
	".hlb": "text/x-script",
	".hlp": "application/hlp",
	".hlp": "application/x-helpfile",
	".hlp": "application/x-winhelp",
	".hpg": "application/vnd.hp-hpgl",
	".hpgl": "application/vnd.hp-hpgl",
	".hqx": "application/binhex",
	".hqx": "application/binhex4",
	".hqx": "application/mac-binhex",
	".hqx": "application/mac-binhex40",
	".hqx": "application/x-binhex40",
	".hqx": "application/x-mac-binhex40",
	".hta": "application/hta",
	".htc": "text/x-component",
	".htm": "text/html",
	".html": "text/html",
	".htmls": "text/html",
	".htt": "text/webviewhtml",
	".htx": "text/html",
	".ice": "x-conference/x-cooltalk",
	".ico": "image/x-icon",
	".idc": "text/plain",
	".ief": "image/ief",
	".iefs": "image/ief",
	".iges": "application/iges",
	".iges": "model/iges",
	".igs": "application/iges",
	".igs": "model/iges",
	".ima": "application/x-ima",
	".imap": "application/x-httpd-imap",
	".inf": "application/inf",
	".ins": "application/x-internett-signup",
	".ip": "application/x-ip2",
	".isu": "video/x-isvideo",
	".it": "audio/it",
	".iv": "application/x-inventor",
	".ivr": "i-world/i-vrml",
	".ivy": "application/x-livescreen",
	".jam": "audio/x-jam",
	".jav": "text/plain",
	".jav": "text/x-java-source",
	".java": "text/plain",
	".java": "text/x-java-source",
	".jcm": "application/x-java-commerce",
	".jfif": "image/jpeg",
	".jfif": "image/pjpeg",
	".jfif-tbnl": "image/jpeg",
	".jpe": "image/jpeg",
	".jpe": "image/pjpeg",
	".jpeg": "image/jpeg",
	".jpeg": "image/pjpeg",
	".jpg": "image/jpeg",
	".jpg": "image/pjpeg",
	".jps": "image/x-jps",
	".js": "application/x-javascript",
	".js": "application/javascript",
	".js": "application/ecmascript",
	".js": "text/javascript",
	".js": "text/ecmascript",
	".jut": "image/jutvision",
	".kar": "audio/midi",
	".kar": "music/x-karaoke",
	".ksh": "application/x-ksh",
	".ksh": "text/x-script.ksh",
	".la": "audio/nspaudio",
	".la": "audio/x-nspaudio",
	".lam": "audio/x-liveaudio",
	".latex": "application/x-latex",
	".lha": "application/lha",
	".lha": "application/octet-stream",
	".lha": "application/x-lha",
	".lhx": "application/octet-stream",
	".list": "text/plain",
	".lma": "audio/nspaudio",
	".lma": "audio/x-nspaudio",
	".log": "text/plain",
	".lsp": "application/x-lisp",
	".lsp": "text/x-script.lisp",
	".lst": "text/plain",
	".lsx": "text/x-la-asf",
	".ltx": "application/x-latex",
	".lzh": "application/octet-stream",
	".lzh": "application/x-lzh",
	".lzx": "application/lzx",
	".lzx": "application/octet-stream",
	".lzx": "application/x-lzx",
	".m": "text/plain",
	".m": "text/x-m",
	".m1v": "video/mpeg",
	".m2a": "audio/mpeg",
	".m2v": "video/mpeg",
	".m3u": "audio/x-mpequrl",
	".man": "application/x-troff-man",
	".map": "application/x-navimap",
	".mar": "text/plain",
	".mbd": "application/mbedlet",
	".mc$": "application/x-magic-cap-package-1.0",
	".mcd": "application/mcad",
	".mcd": "application/x-mathcad",
	".mcf": "image/vasa",
	".mcf": "text/mcf",
	".mcp": "application/netmc",
	".me": "application/x-troff-me",
	".mht": "message/rfc822",
	".mhtml": "message/rfc822",
	".mid": "application/x-midi",
	".mid": "audio/midi",
	".mid": "audio/x-mid",
	".mid": "audio/x-midi",
	".mid": "music/crescendo",
	".mid": "x-music/x-midi",
	".midi": "application/x-midi",
	".midi": "audio/midi",
	".midi": "audio/x-mid",
	".midi": "audio/x-midi",
	".midi": "music/crescendo",
	".midi": "x-music/x-midi",
	".mif": "application/x-frame",
	".mif": "application/x-mif",
	".mime": "message/rfc822",
	".mime": "www/mime",
	".mjf": "audio/x-vnd.audioexplosion.mjuicemediafile",
	".mjpg": "video/x-motion-jpeg",
	".mm": "application/base64",
	".mm": "application/x-meme",
	".mme": "application/base64",
	".mod": "audio/mod",
	".mod": "audio/x-mod",
	".moov": "video/quicktime",
	".mov": "video/quicktime",
	".movie": "video/x-sgi-movie",
	".mp2": "audio/mpeg",
	".mp2": "audio/x-mpeg",
	".mp2": "video/mpeg",
	".mp2": "video/x-mpeg",
	".mp2": "video/x-mpeq2a",
	".mp3": "audio/mpeg3",
	".mp3": "audio/x-mpeg-3",
	".mp3": "video/mpeg",
	".mp3": "video/x-mpeg",
	".mpa": "audio/mpeg",
	".mpa": "video/mpeg",
	".mpc": "application/x-project",
	".mpe": "video/mpeg",
	".mpeg": "video/mpeg",
	".mpg": "audio/mpeg",
	".mpg": "video/mpeg",
	".mpga": "audio/mpeg",
	".mpp": "application/vnd.ms-project",
	".mpt": "application/x-project",
	".mpv": "application/x-project",
	".mpx": "application/x-project",
	".mrc": "application/marc",
	".ms": "application/x-troff-ms",
	".mv": "video/x-sgi-movie",
	".my": "audio/make",
	".mzz": "application/x-vnd.audioexplosion.mzz",
	".nap": "image/naplps",
	".naplps": "image/naplps",
	".nc": "application/x-netcdf",
	".ncm": "application/vnd.nokia.configuration-message",
	".nif": "image/x-niff",
	".niff": "image/x-niff",
	".nix": "application/x-mix-transfer",
	".nsc": "application/x-conference",
	".nvd": "application/x-navidoc",
	".o": "application/octet-stream",
	".oda": "application/oda",
	".omc": "application/x-omc",
	".omcd": "application/x-omcdatamaker",
	".omcr": "application/x-omcregerator",
	".p": "text/x-pascal",
	".p10": "application/pkcs10",
	".p10": "application/x-pkcs10",
	".p12": "application/pkcs-12",
	".p12": "application/x-pkcs12",
	".p7a": "application/x-pkcs7-signature",
	".p7c": "application/pkcs7-mime",
	".p7c": "application/x-pkcs7-mime",
	".p7m": "application/pkcs7-mime",
	".p7m": "application/x-pkcs7-mime",
	".p7r": "application/x-pkcs7-certreqresp",
	".p7s": "application/pkcs7-signature",
	".part": "application/pro_eng",
	".pas": "text/pascal",
	".pbm": "image/x-portable-bitmap",
	".pcl": "application/vnd.hp-pcl",
	".pcl": "application/x-pcl",
	".pct": "image/x-pict",
	".pcx": "image/x-pcx",
	".pdb": "chemical/x-pdb",
	".pdf": "application/pdf",
	".pfunk": "audio/make",
	".pfunk": "audio/make.my.funk",
	".pgm": "image/x-portable-graymap",
	".pgm": "image/x-portable-greymap",
	".pic": "image/pict",
	".pict": "image/pict",
	".pkg": "application/x-newton-compatible-pkg",
	".pko": "application/vnd.ms-pki.pko",
	".pl": "text/plain",
	".pl": "text/x-script.perl",
	".plx": "application/x-pixclscript",
	".pm": "image/x-xpixmap",
	".pm": "text/x-script.perl-module",
	".pm4": "application/x-pagemaker",
	".pm5": "application/x-pagemaker",
	".png": "image/png",
	".pnm": "application/x-portable-anymap",
	".pnm": "image/x-portable-anymap",
	".pot": "application/mspowerpoint",
	".pot": "application/vnd.ms-powerpoint",
	".pov": "model/x-pov",
	".ppa": "application/vnd.ms-powerpoint",
	".ppm": "image/x-portable-pixmap",
	".pps": "application/mspowerpoint",
	".pps": "application/vnd.ms-powerpoint",
	".ppt": "application/mspowerpoint",
	".ppt": "application/powerpoint",
	".ppt": "application/vnd.ms-powerpoint",
	".ppt": "application/x-mspowerpoint",
	".ppz": "application/mspowerpoint",
	".pre": "application/x-freelance",
	".prt": "application/pro_eng",
	".ps": "application/postscript",
	".psd": "application/octet-stream",
	".pvu": "paleovu/x-pv",
	".pwz": "application/vnd.ms-powerpoint",
	".py": "text/x-script.phyton",
	".pyc": "application/x-bytecode.python",
	".qcp": "audio/vnd.qcelp",
	".qd3": "x-world/x-3dmf",
	".qd3d": "x-world/x-3dmf",
	".qif": "image/x-quicktime",
	".qt": "video/quicktime",
	".qtc": "video/x-qtc",
	".qti": "image/x-quicktime",
	".qtif": "image/x-quicktime",
	".ra": "audio/x-pn-realaudio",
	".ra": "audio/x-pn-realaudio-plugin",
	".ra": "audio/x-realaudio",
	".ram": "audio/x-pn-realaudio",
	".ras": "application/x-cmu-raster",
	".ras": "image/cmu-raster",
	".ras": "image/x-cmu-raster",
	".rast": "image/cmu-raster",
	".rexx": "text/x-script.rexx",
	".rf": "image/vnd.rn-realflash",
	".rgb": "image/x-rgb",
	".rm": "application/vnd.rn-realmedia",
	".rm": "audio/x-pn-realaudio",
	".rmi": "audio/mid",
	".rmm": "audio/x-pn-realaudio",
	".rmp": "audio/x-pn-realaudio",
	".rmp": "audio/x-pn-realaudio-plugin",
	".rng": "application/ringing-tones",
	".rng": "application/vnd.nokia.ringing-tone",
	".rnx": "application/vnd.rn-realplayer",
	".roff": "application/x-troff",
	".rp": "image/vnd.rn-realpix",
	".rpm": "audio/x-pn-realaudio-plugin",
	".rt": "text/richtext",
	".rt": "text/vnd.rn-realtext",
	".rtf": "application/rtf",
	".rtf": "application/x-rtf",
	".rtf": "text/richtext",
	".rtx": "application/rtf",
	".rtx": "text/richtext",
	".rv": "video/vnd.rn-realvideo",
	".s": "text/x-asm",
	".s3m": "audio/s3m",
	".saveme": "application/octet-stream",
	".sbk": "application/x-tbook",
	".scm": "application/x-lotusscreencam",
	".scm": "text/x-script.guile",
	".scm": "text/x-script.scheme",
	".scm": "video/x-scm",
	".sdml": "text/plain",
	".sdp": "application/sdp",
	".sdp": "application/x-sdp",
	".sdr": "application/sounder",
	".sea": "application/sea",
	".sea": "application/x-sea",
	".set": "application/set",
	".sgm": "text/sgml",
	".sgm": "text/x-sgml",
	".sgml": "text/sgml",
	".sgml": "text/x-sgml",
	".sh": "application/x-bsh",
	".sh": "application/x-sh",
	".sh": "application/x-shar",
	".sh": "text/x-script.sh",
	".shar": "application/x-bsh",
	".shar": "application/x-shar",
	".shtml": "text/html",
	".shtml": "text/x-server-parsed-html",
	".sid": "audio/x-psid",
	".sit": "application/x-sit",
	".sit": "application/x-stuffit",
	".skd": "application/x-koan",
	".skm": "application/x-koan",
	".skp": "application/x-koan",
	".skt": "application/x-koan",
	".sl": "application/x-seelogo",
	".smi": "application/smil",
	".smil": "application/smil",
	".snd": "audio/basic",
	".snd": "audio/x-adpcm",
	".sol": "application/solids",
	".spc": "application/x-pkcs7-certificates",
	".spc": "text/x-speech",
	".spl": "application/futuresplash",
	".spr": "application/x-sprite",
	".sprite": "application/x-sprite",
	".src": "application/x-wais-source",
	".ssi": "text/x-server-parsed-html",
	".ssm": "application/streamingmedia",
	".sst": "application/vnd.ms-pki.certstore",
	".step": "application/step",
	".stl": "application/sla",
	".stl": "application/vnd.ms-pki.stl",
	".stl": "application/x-navistyle",
	".stp": "application/step",
	".sv4cpio": "application/x-sv4cpio",
	".sv4crc": "application/x-sv4crc",
	".svf": "image/vnd.dwg",
	".svf": "image/x-dwg",
	".svr": "application/x-world",
	".svr": "x-world/x-svr",
	".swf": "application/x-shockwave-flash",
	".t": "application/x-troff",
	".talk": "text/x-speech",
	".tar": "application/x-tar",
	".tbk": "application/toolbook",
	".tbk": "application/x-tbook",
	".tcl": "application/x-tcl",
	".tcl": "text/x-script.tcl",
	".tcsh": "text/x-script.tcsh",
	".tex": "application/x-tex",
	".texi": "application/x-texinfo",
	".texinfo": "application/x-texinfo",
	".text": "application/plain",
	".text": "text/plain",
	".tgz": "application/gnutar",
	".tgz": "application/x-compressed",
	".tif": "image/tiff",
	".tif": "image/x-tiff",
	".tiff": "image/tiff",
	".tiff": "image/x-tiff",
	".tr": "application/x-troff",
	".tsi": "audio/tsp-audio",
	".tsp": "application/dsptype",
	".tsp": "audio/tsplayer",
	".tsv": "text/tab-separated-values",
	".turbot": "image/florian",
	".txt": "text/plain",
	".uil": "text/x-uil",
	".uni": "text/uri-list",
	".unis": "text/uri-list",
	".unv": "application/i-deas",
	".uri": "text/uri-list",
	".uris": "text/uri-list",
	".ustar": "application/x-ustar",
	".ustar": "multipart/x-ustar",
	".uu": "application/octet-stream",
	".uu": "text/x-uuencode",
	".uue": "text/x-uuencode",
	".vcd": "application/x-cdlink",
	".vcs": "text/x-vcalendar",
	".vda": "application/vda",
	".vdo": "video/vdo",
	".vew": "application/groupwise",
	".viv": "video/vivo",
	".viv": "video/vnd.vivo",
	".vivo": "video/vivo",
	".vivo": "video/vnd.vivo",
	".vmd": "application/vocaltec-media-desc",
	".vmf": "application/vocaltec-media-file",
	".voc": "audio/voc",
	".voc": "audio/x-voc",
	".vos": "video/vosaic",
	".vox": "audio/voxware",
	".vqe": "audio/x-twinvq-plugin",
	".vqf": "audio/x-twinvq",
	".vql": "audio/x-twinvq-plugin",
	".vrml": "application/x-vrml",
	".vrml": "model/vrml",
	".vrml": "x-world/x-vrml",
	".vrt": "x-world/x-vrt",
	".vsd": "application/x-visio",
	".vst": "application/x-visio",
	".vsw": "application/x-visio",
	".w60": "application/wordperfect6.0",
	".w61": "application/wordperfect6.1",
	".w6w": "application/msword",
	".wav": "audio/wav",
	".wav": "audio/x-wav",
	".wb1": "application/x-qpro",
	".wbmp": "image/vnd.wap.wbmp",
	".web": "application/vnd.xara",
	".wiz": "application/msword",
	".wk1": "application/x-123",
	".wmf": "windows/metafile",
	".wml": "text/vnd.wap.wml",
	".wmlc": "application/vnd.wap.wmlc",
	".wmls": "text/vnd.wap.wmlscript",
	".wmlsc": "application/vnd.wap.wmlscriptc",
	".word": "application/msword",
	".wp": "application/wordperfect",
	".wp5": "application/wordperfect",
	".wp5": "application/wordperfect6.0",
	".wp6": "application/wordperfect",
	".wpd": "application/wordperfect",
	".wpd": "application/x-wpwin",
	".wq1": "application/x-lotus",
	".wri": "application/mswrite",
	".wri": "application/x-wri",
	".wrl": "application/x-world",
	".wrl": "model/vrml",
	".wrl": "x-world/x-vrml",
	".wrz": "model/vrml",
	".wrz": "x-world/x-vrml",
	".wsc": "text/scriplet",
	".wsrc": "application/x-wais-source",
	".wtk": "application/x-wintalk",
	".xbm": "image/x-xbitmap",
	".xbm": "image/x-xbm",
	".xbm": "image/xbm",
	".xdr": "video/x-amt-demorun",
	".xgz": "xgl/drawing",
	".xif": "image/vnd.xiff",
	".xl": "application/excel",
	".xla": "application/excel",
	".xla": "application/x-excel",
	".xla": "application/x-msexcel",
	".xlb": "application/excel",
	".xlb": "application/vnd.ms-excel",
	".xlb": "application/x-excel",
	".xlc": "application/excel",
	".xlc": "application/vnd.ms-excel",
	".xlc": "application/x-excel",
	".xld": "application/excel",
	".xld": "application/x-excel",
	".xlk": "application/excel",
	".xlk": "application/x-excel",
	".xll": "application/excel",
	".xll": "application/vnd.ms-excel",
	".xll": "application/x-excel",
	".xlm": "application/excel",
	".xlm": "application/vnd.ms-excel",
	".xlm": "application/x-excel",
	".xls": "application/excel",
	".xls": "application/vnd.ms-excel",
	".xls": "application/x-excel",
	".xls": "application/x-msexcel",
	".xlt": "application/excel",
	".xlt": "application/x-excel",
	".xlv": "application/excel",
	".xlv": "application/x-excel",
	".xlw": "application/excel",
	".xlw": "application/vnd.ms-excel",
	".xlw": "application/x-excel",
	".xlw": "application/x-msexcel",
	".xm": "audio/xm",
	".xml": "application/xml",
	".xml": "text/xml",
	".xmz": "xgl/movie",
	".xpix": "application/x-vnd.ls-xpix",
	".xpm": "image/x-xpixmap",
	".xpm": "image/xpm",
	".x-png": "image/png",
	".xsr": "video/x-amt-showrun",
	".xwd": "image/x-xwd",
	".xwd": "image/x-xwindowdump",
	".xyz": "chemical/x-pdb",
	".z": "application/x-compress",
	".z": "application/x-compressed",
	".zip": "application/x-compressed",
	".zip": "application/x-zip-compressed",
	".zip": "application/zip",
	".zip": "multipart/x-zip",
	".zoo": "application/octet-stream",
	".zsh": "text/x-script.zsh",
	"unknown": "application/octet-stream",
}
"""The :mod:`sklearn.kernel_regressor` module implements the Kernel Regressor.
"""
# Author: Jan Hendrik Metzen <janmetzen@mailbox.de>
#
# License: BSD 3 clause

import numpy as np

from sklearn.metrics.pairwise import pairwise_kernels
from sklearn.base import BaseEstimator, RegressorMixin
import gc

class KernelRegression(BaseEstimator, RegressorMixin):
    """Nadaraya-Watson kernel regression with automatic bandwidth selection.

    This implements Nadaraya-Watson kernel regression with (optional) automatic
    bandwith selection of the kernel via leave-one-out cross-validation. Kernel
    regression is a simple non-parametric kernelized technique for learning
    a non-linear relationship between input variable(s) and a target variable.

    Parameters
    ----------
    kernel : string or callable, default="rbf"
        Kernel map to be approximated. A callable should accept two arguments
        and the keyword arguments passed to this object as kernel_params, and
        should return a floating point number.

    gamma : float, default=None
        Gamma parameter for the RBF ("bandwidth"), polynomial,
        exponential chi2 and sigmoid kernels. Interpretation of the default
        value is left to the kernel; see the documentation for
        sklearn.metrics.pairwise. Ignored by other kernels. If a sequence of
        values is given, one of these values is selected which minimizes
        the mean-squared-error of leave-one-out cross-validation.

    See also
    --------

    sklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.
    """

    def __init__(self, kernel="rbf", gamma=None):
        self.kernel = kernel
        self.gamma = gamma

    def fit(self, X, y):
        """Fit the model

        Parameters
        ----------
        X : array-like of shape = [n_samples, n_features]
            The training input samples.

        y : array-like, shape = [n_samples]
            The target values

        Returns
        -------
        self : object
            Returns self.
        """
        self.X = X
        self.y = y

        if hasattr(self.gamma, "__iter__"):
            self.gamma = self._optimize_gamma(self.gamma)

        return self

    def predict(self, X):
        """Predict target values for X.

        Parameters
        ----------
        X : array-like of shape = [n_samples, n_features]
            The input samples.

        Returns
        -------
        y : array of shape = [n_samples]
            The predicted target value.
        """
        K = pairwise_kernels(self.X, X, metric=self.kernel, gamma=self.gamma)
        try:
            ret = (K * self.y[:, None]).sum(axis=0) / K.sum(axis=0)
        except MemoryError:
            gc.collect()  # gc and retry
            ret = (K * self.y[:, None]).sum(axis=0) / K.sum(axis=0)

        return ret

    def _optimize_gamma(self, gamma_values):
        # Select specific value of gamma from the range of given gamma_values
        # by minimizing mean-squared error in leave-one-out cross validation
        mse = np.empty_like(gamma_values, dtype=np.float)
        for i, gamma in enumerate(gamma_values):
            K = pairwise_kernels(self.X, self.X, metric=self.kernel,
                                 gamma=gamma)
            np.fill_diagonal(K, 0)  # leave-one-out
            Ky = K * self.y[:, np.newaxis]
            y_pred = Ky.sum(axis=0) / K.sum(axis=0)
            mse[i] = ((y_pred - self.y) ** 2).mean()

        return gamma_values[np.nanargmin(mse)]

#!/usr/bin/env python
"""Django's command-line utility for administrative tasks."""
import os
import sys


def main():
    """Run administrative tasks."""
    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'finstan_project.settings')
    try:
        from django.core.management import execute_from_command_line
    except ImportError as exc:
        raise ImportError(
            "Couldn't import Django. Are you sure it's installed and "
            "available on your PYTHONPATH environment variable? Did you "
            "forget to activate a virtual environment?"
        ) from exc
    execute_from_command_line(sys.argv)


if __name__ == '__main__':
    main()

